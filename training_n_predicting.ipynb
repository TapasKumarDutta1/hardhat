{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_n_predicting.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/hardhat/blob/main/training_n_predicting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22ac868d-a25c-4a80-8544-dc74e566f457"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKqMehweWrGS",
        "outputId": "61356d67-5c4f-4c90-e877-436ed74311ac"
      },
      "source": [
        "import glob\n",
        "len(glob.glob('/content/gdrive/MyDrive/hardhat/*'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9035"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ5bhg7BMrMZ"
      },
      "source": [
        "def convert_annot(size , box):\n",
        "    x1 = int(box[0])\n",
        "    y1 = int(box[1])\n",
        "    x2 = int(box[2])\n",
        "    y2 = int(box[3])\n",
        "\n",
        "    dw = np.float32(1. / int(size[0]))\n",
        "    dh = np.float32(1. / int(size[1]))\n",
        "\n",
        "    w = x2 - x1\n",
        "    h = y2 - y1\n",
        "    x = x1 + (w / 2)\n",
        "    y = y1 + (h / 2)\n",
        "\n",
        "    x = x * dw\n",
        "    w = w * dw\n",
        "    y = y * dh\n",
        "    h = h * dh\n",
        "    return [x, y, w, h]\n",
        "def save_txt_file(img_jpg_file_name, size, img_box):\n",
        "    save_file_name = '/content/Dataset/labels/' +  img_jpg_file_name + '.txt'\n",
        "    #file_path = open(save_file_name, \"a+\")\n",
        "    with open(save_file_name ,'a+') as file_path:\n",
        "        for box in img_box:\n",
        "\n",
        "            cls_num = classes.index(box[0])\n",
        "\n",
        "            new_box = convert_annot(size, box[1:])\n",
        "\n",
        "            file_path.write(f\"{cls_num} {new_box[0]} {new_box[1]} {new_box[2]} {new_box[3]}\\n\")\n",
        "\n",
        "        file_path.flush()\n",
        "        file_path.close()\n",
        "def get_xml_data(file_path, img_xml_file):\n",
        "    img_path = file_path + '/' + img_xml_file + '.xml'\n",
        "    #print(img_path)\n",
        "\n",
        "    dom = parse(img_path)\n",
        "    root = dom.documentElement\n",
        "    img_name = root.getElementsByTagName(\"filename\")[0].childNodes[0].data\n",
        "    img_size = root.getElementsByTagName(\"size\")[0]\n",
        "    objects = root.getElementsByTagName(\"object\")\n",
        "    img_w = img_size.getElementsByTagName(\"width\")[0].childNodes[0].data\n",
        "    img_h = img_size.getElementsByTagName(\"height\")[0].childNodes[0].data\n",
        "    img_c = img_size.getElementsByTagName(\"depth\")[0].childNodes[0].data\n",
        "   \n",
        "    img_box = []\n",
        "    for box in objects:\n",
        "        cls_name = box.getElementsByTagName(\"name\")[0].childNodes[0].data\n",
        "        if cls_name=='helmet':\n",
        "            x1 = int(box.getElementsByTagName(\"xmin\")[0].childNodes[0].data)\n",
        "            y1 = int(box.getElementsByTagName(\"ymin\")[0].childNodes[0].data)\n",
        "            x2 = int(box.getElementsByTagName(\"xmax\")[0].childNodes[0].data)\n",
        "            y2 = int(box.getElementsByTagName(\"ymax\")[0].childNodes[0].data)\n",
        "\n",
        "            img_jpg_file_name = img_xml_file + '.jpg'\n",
        "            img_box.append([cls_name, x1, y1, x2, y2])\n",
        "\n",
        "    # test_dataset_box_feature(img_jpg_file_name, img_box)\n",
        "    save_txt_file(img_xml_file, [img_w, img_h], img_box)\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "from pathlib import Path\n",
        "from xml.dom.minidom import parse\n",
        "from shutil import copyfile\n",
        "import os\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj_Qu0XRdUq6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c17bc29-cf75-41d4-e874-d925550b9323"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 8670, done.\u001b[K\n",
            "remote: Counting objects: 100% (384/384), done.\u001b[K\n",
            "remote: Compressing objects: 100% (253/253), done.\u001b[K\n",
            "remote: Total 8670 (delta 234), reused 247 (delta 131), pack-reused 8286\u001b[K\n",
            "Receiving objects: 100% (8670/8670), 9.63 MiB | 22.31 MiB/s, done.\n",
            "Resolving deltas: 100% (5976/5976), done.\n",
            "/content/yolov5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c9YjY_IZpii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa786407-e9da-4700-f6f2-3c48447b26ff"
      },
      "source": [
        "!pip install -r requirements.txt "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.19.5)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (7.1.2)\n",
            "Collecting PyYAML>=5.3.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (0.10.0+cu102)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (4.41.1)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (2.5.0)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (0.11.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 20)) (1.1.5)\n",
            "Collecting thop\n",
            "  Downloading thop-0.0.31.post2005241907-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->-r requirements.txt (line 10)) (3.7.4.3)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.17.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.32.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.36.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (2.23.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.34.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (57.2.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 20)) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 15)) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 15)) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 15)) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.1->-r requirements.txt (line 15)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.1->-r requirements.txt (line 15)) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.5.0)\n",
            "Installing collected packages: thop, PyYAML\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1 thop-0.0.31.post2005241907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grXXPqLkgNPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffdad85e-633a-41a6-cdbc-3fad3c7a9756"
      },
      "source": [
        "%%writefile datasets.py\n",
        "\n",
        "import glob\n",
        "import hashlib\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "from itertools import repeat\n",
        "from multiprocessing.pool import ThreadPool, Pool\n",
        "from pathlib import Path\n",
        "from threading import Thread\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import yaml\n",
        "from PIL import Image, ExifTags\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "from utils.augmentations import Albumentations, augment_hsv, copy_paste, letterbox, mixup, random_perspective\n",
        "from utils.general import check_requirements, check_file, check_dataset, xywh2xyxy, xywhn2xyxy, xyxy2xywhn, \\\n",
        "    xyn2xy, segments2boxes, clean_str\n",
        "from utils.torch_utils import torch_distributed_zero_first\n",
        "\n",
        "# Parameters\n",
        "HELP_URL = 'https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data'\n",
        "IMG_FORMATS = ['bmp', 'jpg', 'jpeg', 'png', 'tif', 'tiff', 'dng', 'webp', 'mpo']  # acceptable image suffixes\n",
        "VID_FORMATS = ['mov', 'avi', 'mp4', 'mpg', 'mpeg', 'm4v', 'wmv', 'mkv']  # acceptable video suffixes\n",
        "NUM_THREADS = min(8, os.cpu_count())  # number of multiprocessing threads\n",
        "\n",
        "# Get orientation exif tag\n",
        "for orientation in ExifTags.TAGS.keys():\n",
        "    if ExifTags.TAGS[orientation] == 'Orientation':\n",
        "        break\n",
        "\n",
        "\n",
        "def get_hash(paths):\n",
        "    # Returns a single hash value of a list of paths (files or dirs)\n",
        "    size = sum(os.path.getsize(p) for p in paths if os.path.exists(p))  # sizes\n",
        "    h = hashlib.md5(str(size).encode())  # hash sizes\n",
        "    h.update(''.join(paths).encode())  # hash paths\n",
        "    return h.hexdigest()  # return hash\n",
        "\n",
        "\n",
        "def exif_size(img):\n",
        "    # Returns exif-corrected PIL size\n",
        "    s = img.size  # (width, height)\n",
        "    try:\n",
        "        rotation = dict(img._getexif().items())[orientation]\n",
        "        if rotation == 6:  # rotation 270\n",
        "            s = (s[1], s[0])\n",
        "        elif rotation == 8:  # rotation 90\n",
        "            s = (s[1], s[0])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "def exif_transpose(image):\n",
        "    \"\"\"\n",
        "    Transpose a PIL image accordingly if it has an EXIF Orientation tag.\n",
        "    From https://github.com/python-pillow/Pillow/blob/master/src/PIL/ImageOps.py\n",
        "\n",
        "    :param image: The image to transpose.\n",
        "    :return: An image.\n",
        "    \"\"\"\n",
        "    exif = image.getexif()\n",
        "    orientation = exif.get(0x0112, 1)  # default 1\n",
        "    if orientation > 1:\n",
        "        method = {2: Image.FLIP_LEFT_RIGHT,\n",
        "                  3: Image.ROTATE_180,\n",
        "                  4: Image.FLIP_TOP_BOTTOM,\n",
        "                  5: Image.TRANSPOSE,\n",
        "                  6: Image.ROTATE_270,\n",
        "                  7: Image.TRANSVERSE,\n",
        "                  8: Image.ROTATE_90,\n",
        "                  }.get(orientation)\n",
        "        if method is not None:\n",
        "            image = image.transpose(method)\n",
        "            del exif[0x0112]\n",
        "            image.info[\"exif\"] = exif.tobytes()\n",
        "    return image\n",
        "\n",
        "\n",
        "def create_dataloader(path, imgsz, batch_size, stride, single_cls=False, hyp=None, augment=False, cache=False, pad=0.0,\n",
        "                      rect=False, rank=-1, workers=8, image_weights=False, quad=False, prefix=''):\n",
        "    # Make sure only the first process in DDP process the dataset first, and the following others can use the cache\n",
        "    with torch_distributed_zero_first(rank):\n",
        "        dataset = LoadImagesAndLabels(path, imgsz, batch_size,\n",
        "                                      augment=augment,  # augment images\n",
        "                                      hyp=hyp,  # augmentation hyperparameters\n",
        "                                      rect=rect,  # rectangular training\n",
        "                                      cache_images=cache,\n",
        "                                      single_cls=single_cls,\n",
        "                                      stride=int(stride),\n",
        "                                      pad=pad,\n",
        "                                      image_weights=image_weights,\n",
        "                                      prefix=prefix)\n",
        "\n",
        "    batch_size = min(batch_size, len(dataset))\n",
        "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, workers])  # number of workers\n",
        "    sampler = torch.utils.data.distributed.DistributedSampler(dataset) if rank != -1 else None\n",
        "    loader = torch.utils.data.DataLoader if image_weights else InfiniteDataLoader\n",
        "    # Use torch.utils.data.DataLoader() if dataset.properties will update during training else InfiniteDataLoader()\n",
        "    dataloader = loader(dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        num_workers=nw,\n",
        "                        sampler=sampler,\n",
        "                        pin_memory=True,\n",
        "                        collate_fn=LoadImagesAndLabels.collate_fn4 if quad else LoadImagesAndLabels.collate_fn)\n",
        "    return dataloader, dataset\n",
        "\n",
        "\n",
        "class InfiniteDataLoader(torch.utils.data.dataloader.DataLoader):\n",
        "    \"\"\" Dataloader that reuses workers\n",
        "\n",
        "    Uses same syntax as vanilla DataLoader\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        object.__setattr__(self, 'batch_sampler', _RepeatSampler(self.batch_sampler))\n",
        "        self.iterator = super().__iter__()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batch_sampler.sampler)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(len(self)):\n",
        "            yield next(self.iterator)\n",
        "\n",
        "\n",
        "class _RepeatSampler(object):\n",
        "    \"\"\" Sampler that repeats forever\n",
        "\n",
        "    Args:\n",
        "        sampler (Sampler)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sampler):\n",
        "        self.sampler = sampler\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            yield from iter(self.sampler)\n",
        "\n",
        "\n",
        "class LoadImages:  # for inference\n",
        "    def __init__(self, path, img_size=416, stride=32):\n",
        "        p = str(Path(path).absolute())  # os-agnostic absolute path\n",
        "        if '*' in p:\n",
        "            files = sorted(glob.glob(p, recursive=True))  # glob\n",
        "        elif os.path.isdir(p):\n",
        "            files = sorted(glob.glob(os.path.join(p, '*.*')))  # dir\n",
        "        elif os.path.isfile(p):\n",
        "            files = [p]  # files\n",
        "        else:\n",
        "            raise Exception(f'ERROR: {p} does not exist')\n",
        "\n",
        "        images = [x for x in files if x.split('.')[-1].lower() in IMG_FORMATS]\n",
        "        videos = [x for x in files if x.split('.')[-1].lower() in VID_FORMATS]\n",
        "        ni, nv = len(images), len(videos)\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.stride = stride\n",
        "        self.files = images + videos\n",
        "        self.nf = ni + nv  # number of files\n",
        "        self.video_flag = [False] * ni + [True] * nv\n",
        "        self.mode = 'image'\n",
        "        if any(videos):\n",
        "            self.new_video(videos[0])  # new video\n",
        "        else:\n",
        "            self.cap = None\n",
        "        assert self.nf > 0, f'No images or videos found in {p}. ' \\\n",
        "                            f'Supported formats are:\\nimages: {IMG_FORMATS}\\nvideos: {VID_FORMATS}'\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.count == self.nf:\n",
        "            raise StopIteration\n",
        "        path = self.files[self.count]\n",
        "\n",
        "        if self.video_flag[self.count]:\n",
        "            # Read video\n",
        "            self.mode = 'video'\n",
        "            ret_val, img0 = self.cap.read()\n",
        "            if not ret_val:\n",
        "                self.count += 1\n",
        "                self.cap.release()\n",
        "                if self.count == self.nf:  # last video\n",
        "                    raise StopIteration\n",
        "                else:\n",
        "                    path = self.files[self.count]\n",
        "                    self.new_video(path)\n",
        "                    ret_val, img0 = self.cap.read()\n",
        "\n",
        "            self.frame += 1\n",
        "            print(f'video {self.count + 1}/{self.nf} ({self.frame}/{self.frames}) {path}: ', end='')\n",
        "\n",
        "        else:\n",
        "            # Read image\n",
        "            self.count += 1\n",
        "            img0 = cv2.imread(path)  # BGR\n",
        "            assert img0 is not None, 'Image Not Found ' + path\n",
        "            print(f'image {self.count}/{self.nf} {path}: ', end='')\n",
        "\n",
        "        # Padded resize\n",
        "#         img = letterbox(img0, self.img_size, stride=self.stride)[0]\n",
        "\n",
        "        # Convert\n",
        "        img = img0.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "        img = np.ascontiguousarray(img)\n",
        "\n",
        "        return path, img, img0, self.cap\n",
        "\n",
        "    def new_video(self, path):\n",
        "        self.frame = 0\n",
        "        self.cap = cv2.VideoCapture(path)\n",
        "        self.frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nf  # number of files\n",
        "\n",
        "\n",
        "class LoadWebcam:  # for inference\n",
        "    def __init__(self, pipe='0', img_size=640, stride=32):\n",
        "        self.img_size = img_size\n",
        "        self.stride = stride\n",
        "        self.pipe = eval(pipe) if pipe.isnumeric() else pipe\n",
        "        self.cap = cv2.VideoCapture(self.pipe)  # video capture object\n",
        "        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 3)  # set buffer size\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = -1\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        self.count += 1\n",
        "        if cv2.waitKey(1) == ord('q'):  # q to quit\n",
        "            self.cap.release()\n",
        "            cv2.destroyAllWindows()\n",
        "            raise StopIteration\n",
        "\n",
        "        # Read frame\n",
        "        ret_val, img0 = self.cap.read()\n",
        "        img0 = cv2.flip(img0, 1)  # flip left-right\n",
        "\n",
        "        # Print\n",
        "        assert ret_val, f'Camera Error {self.pipe}'\n",
        "        img_path = 'webcam.jpg'\n",
        "        print(f'webcam {self.count}: ', end='')\n",
        "\n",
        "        # Padded resize\n",
        "        img = letterbox(img0, self.img_size, stride=self.stride)[0]\n",
        "\n",
        "        # Convert\n",
        "        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "        img = np.ascontiguousarray(img)\n",
        "\n",
        "        return img_path, img, img0, None\n",
        "\n",
        "    def __len__(self):\n",
        "        return 0\n",
        "\n",
        "\n",
        "class LoadStreams:  # multiple IP or RTSP cameras\n",
        "    def __init__(self, sources='streams.txt', img_size=640, stride=32):\n",
        "        self.mode = 'stream'\n",
        "        self.img_size = img_size\n",
        "        self.stride = stride\n",
        "\n",
        "        if os.path.isfile(sources):\n",
        "            with open(sources, 'r') as f:\n",
        "                sources = [x.strip() for x in f.read().strip().splitlines() if len(x.strip())]\n",
        "        else:\n",
        "            sources = [sources]\n",
        "\n",
        "        n = len(sources)\n",
        "        self.imgs, self.fps, self.frames, self.threads = [None] * n, [0] * n, [0] * n, [None] * n\n",
        "        self.sources = [clean_str(x) for x in sources]  # clean source names for later\n",
        "        for i, s in enumerate(sources):  # index, source\n",
        "            # Start thread to read frames from video stream\n",
        "            print(f'{i + 1}/{n}: {s}... ', end='')\n",
        "            if 'youtube.com/' in s or 'youtu.be/' in s:  # if source is YouTube video\n",
        "                check_requirements(('pafy', 'youtube_dl'))\n",
        "                import pafy\n",
        "                s = pafy.new(s).getbest(preftype=\"mp4\").url  # YouTube URL\n",
        "            s = eval(s) if s.isnumeric() else s  # i.e. s = '0' local webcam\n",
        "            cap = cv2.VideoCapture(s)\n",
        "            assert cap.isOpened(), f'Failed to open {s}'\n",
        "            w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "            h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "            self.fps[i] = max(cap.get(cv2.CAP_PROP_FPS) % 100, 0) or 30.0  # 30 FPS fallback\n",
        "            self.frames[i] = max(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), 0) or float('inf')  # infinite stream fallback\n",
        "\n",
        "            _, self.imgs[i] = cap.read()  # guarantee first frame\n",
        "            self.threads[i] = Thread(target=self.update, args=([i, cap]), daemon=True)\n",
        "            print(f\" success ({self.frames[i]} frames {w}x{h} at {self.fps[i]:.2f} FPS)\")\n",
        "            self.threads[i].start()\n",
        "        print('')  # newline\n",
        "\n",
        "        # check for common shapes\n",
        "        s = np.stack([letterbox(x, self.img_size, stride=self.stride)[0].shape for x in self.imgs], 0)  # shapes\n",
        "        self.rect = np.unique(s, axis=0).shape[0] == 1  # rect inference if all shapes equal\n",
        "        if not self.rect:\n",
        "            print('WARNING: Different stream shapes detected. For optimal performance supply similarly-shaped streams.')\n",
        "\n",
        "    def update(self, i, cap):\n",
        "        # Read stream `i` frames in daemon thread\n",
        "        n, f, read = 0, self.frames[i], 1  # frame number, frame array, inference every 'read' frame\n",
        "        while cap.isOpened() and n < f:\n",
        "            n += 1\n",
        "            # _, self.imgs[index] = cap.read()\n",
        "            cap.grab()\n",
        "            if n % read == 0:\n",
        "                success, im = cap.retrieve()\n",
        "                self.imgs[i] = im if success else self.imgs[i] * 0\n",
        "            time.sleep(1 / self.fps[i])  # wait time\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = -1\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        self.count += 1\n",
        "        if not all(x.is_alive() for x in self.threads) or cv2.waitKey(1) == ord('q'):  # q to quit\n",
        "            cv2.destroyAllWindows()\n",
        "            raise StopIteration\n",
        "\n",
        "        # Letterbox\n",
        "        img0 = self.imgs.copy()\n",
        "        img = [letterbox(x, self.img_size, auto=self.rect, stride=self.stride)[0] for x in img0]\n",
        "\n",
        "        # Stack\n",
        "        img = np.stack(img, 0)\n",
        "\n",
        "        # Convert\n",
        "        img = img[..., ::-1].transpose((0, 3, 1, 2))  # BGR to RGB, BHWC to BCHW\n",
        "        img = np.ascontiguousarray(img)\n",
        "\n",
        "        return self.sources, img, img0, None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sources)  # 1E12 frames = 32 streams at 30 FPS for 30 years\n",
        "\n",
        "\n",
        "def img2label_paths(img_paths):\n",
        "    # Define label paths as a function of image paths\n",
        "    sa, sb = os.sep + 'images' + os.sep, os.sep + 'labels' + os.sep  # /images/, /labels/ substrings\n",
        "    return [sb.join(x.rsplit(sa, 1)).rsplit('.', 1)[0] + '.txt' for x in img_paths]\n",
        "\n",
        "\n",
        "class LoadImagesAndLabels(Dataset):  # for training/testing\n",
        "    def __init__(self, path, img_size=640, batch_size=16, augment=False, hyp=None, rect=False, image_weights=False,\n",
        "                 cache_images=False, single_cls=False, stride=32, pad=0.0, prefix=''):\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "        self.hyp = hyp\n",
        "        self.image_weights = image_weights\n",
        "        self.rect = False if image_weights else rect\n",
        "        self.mosaic = self.augment and not self.rect  # load 4 images at a time into a mosaic (only during training)\n",
        "        self.mosaic_border = [-img_size // 2, -img_size // 2]\n",
        "        self.stride = stride\n",
        "        self.path = path\n",
        "        self.albumentations = Albumentations() if augment else None\n",
        "\n",
        "        try:\n",
        "            f = []  # image files\n",
        "            for p in path if isinstance(path, list) else [path]:\n",
        "                p = Path(p)  # os-agnostic\n",
        "                if p.is_dir():  # dir\n",
        "                    f += glob.glob(str(p / '**' / '*.*'), recursive=True)\n",
        "                    # f = list(p.rglob('**/*.*'))  # pathlib\n",
        "                elif p.is_file():  # file\n",
        "                    with open(p, 'r') as t:\n",
        "                        t = t.read().strip().splitlines()\n",
        "                        parent = str(p.parent) + os.sep\n",
        "                        f += [x.replace('./', parent) if x.startswith('./') else x for x in t]  # local to global path\n",
        "                        # f += [p.parent / x.lstrip(os.sep) for x in t]  # local to global path (pathlib)\n",
        "                else:\n",
        "                    raise Exception(f'{prefix}{p} does not exist')\n",
        "            self.img_files = sorted([x.replace('/', os.sep) for x in f if x.split('.')[-1].lower() in IMG_FORMATS])\n",
        "            # self.img_files = sorted([x for x in f if x.suffix[1:].lower() in img_formats])  # pathlib\n",
        "            assert self.img_files, f'{prefix}No images found'\n",
        "        except Exception as e:\n",
        "            raise Exception(f'{prefix}Error loading data from {path}: {e}\\nSee {HELP_URL}')\n",
        "\n",
        "        # Check cache\n",
        "        self.label_files = img2label_paths(self.img_files)  # labels\n",
        "        cache_path = (p if p.is_file() else Path(self.label_files[0]).parent).with_suffix('.cache')\n",
        "        try:\n",
        "            cache, exists = np.load(cache_path, allow_pickle=True).item(), True  # load dict\n",
        "            assert cache['version'] == 0.4 and cache['hash'] == get_hash(self.label_files + self.img_files)\n",
        "        except:\n",
        "            cache, exists = self.cache_labels(cache_path, prefix), False  # cache\n",
        "\n",
        "        # Display cache\n",
        "        nf, nm, ne, nc, n = cache.pop('results')  # found, missing, empty, corrupted, total\n",
        "        if exists:\n",
        "            d = f\"Scanning '{cache_path}' images and labels... {nf} found, {nm} missing, {ne} empty, {nc} corrupted\"\n",
        "            tqdm(None, desc=prefix + d, total=n, initial=n)  # display cache results\n",
        "            if cache['msgs']:\n",
        "                logging.info('\\n'.join(cache['msgs']))  # display warnings\n",
        "        assert nf > 0 or not augment, f'{prefix}No labels in {cache_path}. Can not train without labels. See {HELP_URL}'\n",
        "\n",
        "        # Read cache\n",
        "        [cache.pop(k) for k in ('hash', 'version', 'msgs')]  # remove items\n",
        "        labels, shapes, self.segments = zip(*cache.values())\n",
        "        self.labels = list(labels)\n",
        "        self.shapes = np.array(shapes, dtype=np.float64)\n",
        "        self.img_files = list(cache.keys())  # update\n",
        "        self.label_files = img2label_paths(cache.keys())  # update\n",
        "        if single_cls:\n",
        "            for x in self.labels:\n",
        "                x[:, 0] = 0\n",
        "\n",
        "        n = len(shapes)  # number of images\n",
        "        bi = np.floor(np.arange(n) / batch_size).astype(np.int)  # batch index\n",
        "        nb = bi[-1] + 1  # number of batches\n",
        "        self.batch = bi  # batch index of image\n",
        "        self.n = n\n",
        "        self.indices = range(n)\n",
        "\n",
        "        # Rectangular Training\n",
        "        if self.rect:\n",
        "            # Sort by aspect ratio\n",
        "            s = self.shapes  # wh\n",
        "            ar = s[:, 1] / s[:, 0]  # aspect ratio\n",
        "            irect = ar.argsort()\n",
        "            self.img_files = [self.img_files[i] for i in irect]\n",
        "            self.label_files = [self.label_files[i] for i in irect]\n",
        "            self.labels = [self.labels[i] for i in irect]\n",
        "            self.shapes = s[irect]  # wh\n",
        "            ar = ar[irect]\n",
        "\n",
        "            # Set training image shapes\n",
        "            shapes = [[1, 1]] * nb\n",
        "            for i in range(nb):\n",
        "                ari = ar[bi == i]\n",
        "                mini, maxi = ari.min(), ari.max()\n",
        "                if maxi < 1:\n",
        "                    shapes[i] = [maxi, 1]\n",
        "                elif mini > 1:\n",
        "                    shapes[i] = [1, 1 / mini]\n",
        "\n",
        "            self.batch_shapes = np.ceil(np.array(shapes) * img_size / stride + pad).astype(np.int) * stride\n",
        "\n",
        "        # Cache images into memory for faster training (WARNING: large datasets may exceed system RAM)\n",
        "        self.imgs = [None] * n\n",
        "        if cache_images:\n",
        "            gb = 0  # Gigabytes of cached images\n",
        "            self.img_hw0, self.img_hw = [None] * n, [None] * n\n",
        "            results = ThreadPool(NUM_THREADS).imap(lambda x: load_image(*x), zip(repeat(self), range(n)))\n",
        "            pbar = tqdm(enumerate(results), total=n)\n",
        "            for i, x in pbar:\n",
        "                self.imgs[i], self.img_hw0[i], self.img_hw[i] = x  # img, hw_original, hw_resized = load_image(self, i)\n",
        "                gb += self.imgs[i].nbytes\n",
        "                pbar.desc = f'{prefix}Caching images ({gb / 1E9:.1f}GB)'\n",
        "            pbar.close()\n",
        "\n",
        "    def cache_labels(self, path=Path('./labels.cache'), prefix=''):\n",
        "        # Cache dataset labels, check images and read shapes\n",
        "        x = {}  # dict\n",
        "        nm, nf, ne, nc, msgs = 0, 0, 0, 0, []  # number missing, found, empty, corrupt, messages\n",
        "        desc = f\"{prefix}Scanning '{path.parent / path.stem}' images and labels...\"\n",
        "        with Pool(NUM_THREADS) as pool:\n",
        "            pbar = tqdm(pool.imap_unordered(verify_image_label, zip(self.img_files, self.label_files, repeat(prefix))),\n",
        "                        desc=desc, total=len(self.img_files))\n",
        "            for im_file, l, shape, segments, nm_f, nf_f, ne_f, nc_f, msg in pbar:\n",
        "                nm += nm_f\n",
        "                nf += nf_f\n",
        "                ne += ne_f\n",
        "                nc += nc_f\n",
        "                if im_file:\n",
        "                    x[im_file] = [l, shape, segments]\n",
        "                if msg:\n",
        "                    msgs.append(msg)\n",
        "                pbar.desc = f\"{desc}{nf} found, {nm} missing, {ne} empty, {nc} corrupted\"\n",
        "\n",
        "        pbar.close()\n",
        "        if msgs:\n",
        "            logging.info('\\n'.join(msgs))\n",
        "        if nf == 0:\n",
        "            logging.info(f'{prefix}WARNING: No labels found in {path}. See {HELP_URL}')\n",
        "        x['hash'] = get_hash(self.label_files + self.img_files)\n",
        "        x['results'] = nf, nm, ne, nc, len(self.img_files)\n",
        "        x['msgs'] = msgs  # warnings\n",
        "        x['version'] = 0.4  # cache version\n",
        "        try:\n",
        "            np.save(path, x)  # save cache for next time\n",
        "            path.with_suffix('.cache.npy').rename(path)  # remove .npy suffix\n",
        "            logging.info(f'{prefix}New cache created: {path}')\n",
        "        except Exception as e:\n",
        "            logging.info(f'{prefix}WARNING: Cache directory {path.parent} is not writeable: {e}')  # path not writeable\n",
        "        return x\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    # def __iter__(self):\n",
        "    #     self.count = -1\n",
        "    #     print('ran dataset iter')\n",
        "    #     #self.shuffled_vector = np.random.permutation(self.nF) if self.augment else np.arange(self.nF)\n",
        "    #     return self\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = self.indices[index]  # linear, shuffled, or image_weights\n",
        "\n",
        "        hyp = self.hyp\n",
        "        mosaic = self.mosaic and random.random() < hyp['mosaic']\n",
        "        if mosaic:\n",
        "            # Load mosaic\n",
        "            img, labels = load_mosaic(self, index)\n",
        "            shapes = None\n",
        "\n",
        "            # MixUp augmentation\n",
        "            if random.random() < hyp['mixup']:\n",
        "                img, labels = mixup(img, labels, *load_mosaic(self, random.randint(0, self.n - 1)))\n",
        "\n",
        "        else:\n",
        "            # Load image\n",
        "            img, (h0, w0), (h, w) = load_image(self, index)\n",
        "\n",
        "            # Letterbox\n",
        "            shape = self.batch_shapes[self.batch[index]] if self.rect else self.img_size  # final letterboxed shape\n",
        "            img, ratio, pad = letterbox(img, shape, auto=False, scaleup=self.augment)\n",
        "            shapes = (h0, w0), ((h / h0, w / w0), pad)  # for COCO mAP rescaling\n",
        "\n",
        "            labels = self.labels[index].copy()\n",
        "            if labels.size:  # normalized xywh to pixel xyxy format\n",
        "                labels[:, 1:] = xywhn2xyxy(labels[:, 1:], ratio[0] * w, ratio[1] * h, padw=pad[0], padh=pad[1])\n",
        "\n",
        "            if self.augment:\n",
        "                img, labels = random_perspective(img, labels,\n",
        "                                                 degrees=hyp['degrees'],\n",
        "                                                 translate=hyp['translate'],\n",
        "                                                 scale=hyp['scale'],\n",
        "                                                 shear=hyp['shear'],\n",
        "                                                 perspective=hyp['perspective'])\n",
        "\n",
        "        nl = len(labels)  # number of labels\n",
        "        if nl:\n",
        "            labels[:, 1:5] = xyxy2xywhn(labels[:, 1:5], w=img.shape[1], h=img.shape[0], clip=True, eps=1E-3)\n",
        "\n",
        "        if self.augment:\n",
        "            # Albumentations\n",
        "            img, labels = self.albumentations(img, labels)\n",
        "\n",
        "            # HSV color-space\n",
        "            augment_hsv(img, hgain=hyp['hsv_h'], sgain=hyp['hsv_s'], vgain=hyp['hsv_v'])\n",
        "\n",
        "            # Flip up-down\n",
        "            if random.random() < hyp['flipud']:\n",
        "                img = np.flipud(img)\n",
        "                if nl:\n",
        "                    labels[:, 2] = 1 - labels[:, 2]\n",
        "\n",
        "            # Flip left-right\n",
        "            if random.random() < hyp['fliplr']:\n",
        "                img = np.fliplr(img)\n",
        "                if nl:\n",
        "                    labels[:, 1] = 1 - labels[:, 1]\n",
        "\n",
        "            # Cutouts\n",
        "            # labels = cutout(img, labels, p=0.5)\n",
        "\n",
        "        labels_out = torch.zeros((nl, 6))\n",
        "        if nl:\n",
        "            labels_out[:, 1:] = torch.from_numpy(labels)\n",
        "\n",
        "        # Convert\n",
        "        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "        img = np.ascontiguousarray(img)\n",
        "\n",
        "        return torch.from_numpy(img), labels_out, self.img_files[index], shapes\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        img, label, path, shapes = zip(*batch)  # transposed\n",
        "        for i, l in enumerate(label):\n",
        "            l[:, 0] = i  # add target image index for build_targets()\n",
        "        return torch.stack(img, 0), torch.cat(label, 0), path, shapes\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn4(batch):\n",
        "        img, label, path, shapes = zip(*batch)  # transposed\n",
        "        n = len(shapes) // 4\n",
        "        img4, label4, path4, shapes4 = [], [], path[:n], shapes[:n]\n",
        "\n",
        "        ho = torch.tensor([[0., 0, 0, 1, 0, 0]])\n",
        "        wo = torch.tensor([[0., 0, 1, 0, 0, 0]])\n",
        "        s = torch.tensor([[1, 1, .5, .5, .5, .5]])  # scale\n",
        "        for i in range(n):  # zidane torch.zeros(16,3,720,1280)  # BCHW\n",
        "            i *= 4\n",
        "            if random.random() < 0.5:\n",
        "                im = F.interpolate(img[i].unsqueeze(0).float(), scale_factor=2., mode='bilinear', align_corners=False)[\n",
        "                    0].type(img[i].type())\n",
        "                l = label[i]\n",
        "            else:\n",
        "                im = torch.cat((torch.cat((img[i], img[i + 1]), 1), torch.cat((img[i + 2], img[i + 3]), 1)), 2)\n",
        "                l = torch.cat((label[i], label[i + 1] + ho, label[i + 2] + wo, label[i + 3] + ho + wo), 0) * s\n",
        "            img4.append(im)\n",
        "            label4.append(l)\n",
        "\n",
        "        for i, l in enumerate(label4):\n",
        "            l[:, 0] = i  # add target image index for build_targets()\n",
        "\n",
        "        return torch.stack(img4, 0), torch.cat(label4, 0), path4, shapes4\n",
        "\n",
        "\n",
        "# Ancillary functions --------------------------------------------------------------------------------------------------\n",
        "def load_image(self, index):\n",
        "    # loads 1 image from dataset, returns img, original hw, resized hw\n",
        "    img = self.imgs[index]\n",
        "    if img is None:  # not cached\n",
        "        path = self.img_files[index]\n",
        "        img = cv2.imread(path)  # BGR\n",
        "        assert img is not None, 'Image Not Found ' + path\n",
        "        h0, w0 = img.shape[:2]  # orig hw\n",
        "        r = self.img_size / max(h0, w0)  # ratio\n",
        "        if r != 1:  # if sizes are not equal\n",
        "            img = cv2.resize(img, (int(w0 * r), int(h0 * r)),\n",
        "                             interpolation=cv2.INTER_AREA if r < 1 and not self.augment else cv2.INTER_LINEAR)\n",
        "        return img, (h0, w0), img.shape[:2]  # img, hw_original, hw_resized\n",
        "    else:\n",
        "        return self.imgs[index], self.img_hw0[index], self.img_hw[index]  # img, hw_original, hw_resized\n",
        "\n",
        "\n",
        "def load_mosaic(self, index):\n",
        "    # loads images in a 4-mosaic\n",
        "\n",
        "    labels4, segments4 = [], []\n",
        "    s = self.img_size\n",
        "    yc, xc = [int(random.uniform(-x, 2 * s + x)) for x in self.mosaic_border]  # mosaic center x, y\n",
        "    indices = [index] + random.choices(self.indices, k=3)  # 3 additional image indices\n",
        "    for i, index in enumerate(indices):\n",
        "        # Load image\n",
        "        img, _, (h, w) = load_image(self, index)\n",
        "\n",
        "        # place img in img4\n",
        "        if i == 0:  # top left\n",
        "            img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\n",
        "            x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n",
        "            x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n",
        "        elif i == 1:  # top right\n",
        "            x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n",
        "            x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n",
        "        elif i == 2:  # bottom left\n",
        "            x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n",
        "            x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h)\n",
        "        elif i == 3:  # bottom right\n",
        "            x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n",
        "            x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n",
        "\n",
        "        img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]\n",
        "        padw = x1a - x1b\n",
        "        padh = y1a - y1b\n",
        "\n",
        "        # Labels\n",
        "        labels, segments = self.labels[index].copy(), self.segments[index].copy()\n",
        "        if labels.size:\n",
        "            labels[:, 1:] = xywhn2xyxy(labels[:, 1:], w, h, padw, padh)  # normalized xywh to pixel xyxy format\n",
        "            segments = [xyn2xy(x, w, h, padw, padh) for x in segments]\n",
        "        labels4.append(labels)\n",
        "        segments4.extend(segments)\n",
        "\n",
        "    # Concat/clip labels\n",
        "    labels4 = np.concatenate(labels4, 0)\n",
        "    for x in (labels4[:, 1:], *segments4):\n",
        "        np.clip(x, 0, 2 * s, out=x)  # clip when using random_perspective()\n",
        "    # img4, labels4 = replicate(img4, labels4)  # replicate\n",
        "\n",
        "    # Augment\n",
        "    img4, labels4, segments4 = copy_paste(img4, labels4, segments4, p=self.hyp['copy_paste'])\n",
        "    img4, labels4 = random_perspective(img4, labels4, segments4,\n",
        "                                       degrees=self.hyp['degrees'],\n",
        "                                       translate=self.hyp['translate'],\n",
        "                                       scale=self.hyp['scale'],\n",
        "                                       shear=self.hyp['shear'],\n",
        "                                       perspective=self.hyp['perspective'],\n",
        "                                       border=self.mosaic_border)  # border to remove\n",
        "\n",
        "    return img4, labels4\n",
        "\n",
        "\n",
        "def load_mosaic9(self, index):\n",
        "    # loads images in a 9-mosaic\n",
        "\n",
        "    labels9, segments9 = [], []\n",
        "    s = self.img_size\n",
        "    indices = [index] + random.choices(self.indices, k=8)  # 8 additional image indices\n",
        "    for i, index in enumerate(indices):\n",
        "        # Load image\n",
        "        img, _, (h, w) = load_image(self, index)\n",
        "\n",
        "        # place img in img9\n",
        "        if i == 0:  # center\n",
        "            img9 = np.full((s * 3, s * 3, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\n",
        "            h0, w0 = h, w\n",
        "            c = s, s, s + w, s + h  # xmin, ymin, xmax, ymax (base) coordinates\n",
        "        elif i == 1:  # top\n",
        "            c = s, s - h, s + w, s\n",
        "        elif i == 2:  # top right\n",
        "            c = s + wp, s - h, s + wp + w, s\n",
        "        elif i == 3:  # right\n",
        "            c = s + w0, s, s + w0 + w, s + h\n",
        "        elif i == 4:  # bottom right\n",
        "            c = s + w0, s + hp, s + w0 + w, s + hp + h\n",
        "        elif i == 5:  # bottom\n",
        "            c = s + w0 - w, s + h0, s + w0, s + h0 + h\n",
        "        elif i == 6:  # bottom left\n",
        "            c = s + w0 - wp - w, s + h0, s + w0 - wp, s + h0 + h\n",
        "        elif i == 7:  # left\n",
        "            c = s - w, s + h0 - h, s, s + h0\n",
        "        elif i == 8:  # top left\n",
        "            c = s - w, s + h0 - hp - h, s, s + h0 - hp\n",
        "\n",
        "        padx, pady = c[:2]\n",
        "        x1, y1, x2, y2 = [max(x, 0) for x in c]  # allocate coords\n",
        "\n",
        "        # Labels\n",
        "        labels, segments = self.labels[index].copy(), self.segments[index].copy()\n",
        "        if labels.size:\n",
        "            labels[:, 1:] = xywhn2xyxy(labels[:, 1:], w, h, padx, pady)  # normalized xywh to pixel xyxy format\n",
        "            segments = [xyn2xy(x, w, h, padx, pady) for x in segments]\n",
        "        labels9.append(labels)\n",
        "        segments9.extend(segments)\n",
        "\n",
        "        # Image\n",
        "        img9[y1:y2, x1:x2] = img[y1 - pady:, x1 - padx:]  # img9[ymin:ymax, xmin:xmax]\n",
        "        hp, wp = h, w  # height, width previous\n",
        "\n",
        "    # Offset\n",
        "    yc, xc = [int(random.uniform(0, s)) for _ in self.mosaic_border]  # mosaic center x, y\n",
        "    img9 = img9[yc:yc + 2 * s, xc:xc + 2 * s]\n",
        "\n",
        "    # Concat/clip labels\n",
        "    labels9 = np.concatenate(labels9, 0)\n",
        "    labels9[:, [1, 3]] -= xc\n",
        "    labels9[:, [2, 4]] -= yc\n",
        "    c = np.array([xc, yc])  # centers\n",
        "    segments9 = [x - c for x in segments9]\n",
        "\n",
        "    for x in (labels9[:, 1:], *segments9):\n",
        "        np.clip(x, 0, 2 * s, out=x)  # clip when using random_perspective()\n",
        "    # img9, labels9 = replicate(img9, labels9)  # replicate\n",
        "\n",
        "    # Augment\n",
        "    img9, labels9 = random_perspective(img9, labels9, segments9,\n",
        "                                       degrees=self.hyp['degrees'],\n",
        "                                       translate=self.hyp['translate'],\n",
        "                                       scale=self.hyp['scale'],\n",
        "                                       shear=self.hyp['shear'],\n",
        "                                       perspective=self.hyp['perspective'],\n",
        "                                       border=self.mosaic_border)  # border to remove\n",
        "\n",
        "    return img9, labels9\n",
        "\n",
        "\n",
        "def create_folder(path='./new'):\n",
        "    # Create folder\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path)  # delete output folder\n",
        "    os.makedirs(path)  # make new output folder\n",
        "\n",
        "\n",
        "def flatten_recursive(path='../datasets/coco128'):\n",
        "    # Flatten a recursive directory by bringing all files to top level\n",
        "    new_path = Path(path + '_flat')\n",
        "    create_folder(new_path)\n",
        "    for file in tqdm(glob.glob(str(Path(path)) + '/**/*.*', recursive=True)):\n",
        "        shutil.copyfile(file, new_path / Path(file).name)\n",
        "\n",
        "\n",
        "def extract_boxes(path='../datasets/coco128'):  # from utils.datasets import *; extract_boxes()\n",
        "    # Convert detection dataset into classification dataset, with one directory per class\n",
        "    path = Path(path)  # images dir\n",
        "    shutil.rmtree(path / 'classifier') if (path / 'classifier').is_dir() else None  # remove existing\n",
        "    files = list(path.rglob('*.*'))\n",
        "    n = len(files)  # number of files\n",
        "    for im_file in tqdm(files, total=n):\n",
        "        if im_file.suffix[1:] in IMG_FORMATS:\n",
        "            # image\n",
        "            im = cv2.imread(str(im_file))[..., ::-1]  # BGR to RGB\n",
        "            h, w = im.shape[:2]\n",
        "\n",
        "            # labels\n",
        "            lb_file = Path(img2label_paths([str(im_file)])[0])\n",
        "            if Path(lb_file).exists():\n",
        "                with open(lb_file, 'r') as f:\n",
        "                    lb = np.array([x.split() for x in f.read().strip().splitlines()], dtype=np.float32)  # labels\n",
        "\n",
        "                for j, x in enumerate(lb):\n",
        "                    c = int(x[0])  # class\n",
        "                    f = (path / 'classifier') / f'{c}' / f'{path.stem}_{im_file.stem}_{j}.jpg'  # new filename\n",
        "                    if not f.parent.is_dir():\n",
        "                        f.parent.mkdir(parents=True)\n",
        "\n",
        "                    b = x[1:] * [w, h, w, h]  # box\n",
        "                    # b[2:] = b[2:].max()  # rectangle to square\n",
        "                    b[2:] = b[2:] * 1.2 + 3  # pad\n",
        "                    b = xywh2xyxy(b.reshape(-1, 4)).ravel().astype(np.int)\n",
        "\n",
        "                    b[[0, 2]] = np.clip(b[[0, 2]], 0, w)  # clip boxes outside of image\n",
        "                    b[[1, 3]] = np.clip(b[[1, 3]], 0, h)\n",
        "                    assert cv2.imwrite(str(f), im[b[1]:b[3], b[0]:b[2]]), f'box failure in {f}'\n",
        "\n",
        "\n",
        "def autosplit(path='../datasets/coco128/images', weights=(0.9, 0.1, 0.0), annotated_only=False):\n",
        "    \"\"\" Autosplit a dataset into train/val/test splits and save path/autosplit_*.txt files\n",
        "    Usage: from utils.datasets import *; autosplit()\n",
        "    Arguments\n",
        "        path:            Path to images directory\n",
        "        weights:         Train, val, test weights (list, tuple)\n",
        "        annotated_only:  Only use images with an annotated txt file\n",
        "    \"\"\"\n",
        "    path = Path(path)  # images dir\n",
        "    files = sum([list(path.rglob(f\"*.{img_ext}\")) for img_ext in IMG_FORMATS], [])  # image files only\n",
        "    n = len(files)  # number of files\n",
        "    random.seed(0)  # for reproducibility\n",
        "    indices = random.choices([0, 1, 2], weights=weights, k=n)  # assign each image to a split\n",
        "\n",
        "    txt = ['autosplit_train.txt', 'autosplit_val.txt', 'autosplit_test.txt']  # 3 txt files\n",
        "    [(path.parent / x).unlink(missing_ok=True) for x in txt]  # remove existing\n",
        "\n",
        "    print(f'Autosplitting images from {path}' + ', using *.txt labeled images only' * annotated_only)\n",
        "    for i, img in tqdm(zip(indices, files), total=n):\n",
        "        if not annotated_only or Path(img2label_paths([str(img)])[0]).exists():  # check label\n",
        "            with open(path.parent / txt[i], 'a') as f:\n",
        "                f.write('./' + img.relative_to(path.parent).as_posix() + '\\n')  # add image to txt file\n",
        "\n",
        "\n",
        "def verify_image_label(args):\n",
        "    # Verify one image-label pair\n",
        "    im_file, lb_file, prefix = args\n",
        "    nm, nf, ne, nc = 0, 0, 0, 0  # number missing, found, empty, corrupt\n",
        "    try:\n",
        "        # verify images\n",
        "        im = Image.open(im_file)\n",
        "        im.verify()  # PIL verify\n",
        "        shape = exif_size(im)  # image size\n",
        "        assert (shape[0] > 9) & (shape[1] > 9), f'image size {shape} <10 pixels'\n",
        "        assert im.format.lower() in IMG_FORMATS, f'invalid image format {im.format}'\n",
        "        if im.format.lower() in ('jpg', 'jpeg'):\n",
        "            with open(im_file, 'rb') as f:\n",
        "                f.seek(-2, 2)\n",
        "                assert f.read() == b'\\xff\\xd9', 'corrupted JPEG'\n",
        "\n",
        "        # verify labels\n",
        "        segments = []  # instance segments\n",
        "        if os.path.isfile(lb_file):\n",
        "            nf = 1  # label found\n",
        "            with open(lb_file, 'r') as f:\n",
        "                l = [x.split() for x in f.read().strip().splitlines() if len(x)]\n",
        "                if any([len(x) > 8 for x in l]):  # is segment\n",
        "                    classes = np.array([x[0] for x in l], dtype=np.float32)\n",
        "                    segments = [np.array(x[1:], dtype=np.float32).reshape(-1, 2) for x in l]  # (cls, xy1...)\n",
        "                    l = np.concatenate((classes.reshape(-1, 1), segments2boxes(segments)), 1)  # (cls, xywh)\n",
        "                l = np.array(l, dtype=np.float32)\n",
        "            if len(l):\n",
        "                assert l.shape[1] == 5, 'labels require 5 columns each'\n",
        "                assert (l >= 0).all(), 'negative labels'\n",
        "                assert (l[:, 1:] <= 1).all(), 'non-normalized or out of bounds coordinate labels'\n",
        "                assert np.unique(l, axis=0).shape[0] == l.shape[0], 'duplicate labels'\n",
        "            else:\n",
        "                ne = 1  # label empty\n",
        "                l = np.zeros((0, 5), dtype=np.float32)\n",
        "        else:\n",
        "            nm = 1  # label missing\n",
        "            l = np.zeros((0, 5), dtype=np.float32)\n",
        "        return im_file, l, shape, segments, nm, nf, ne, nc, ''\n",
        "    except Exception as e:\n",
        "        nc = 1\n",
        "        msg = f'{prefix}WARNING: Ignoring corrupted image and/or label {im_file}: {e}'\n",
        "        return [None, None, None, None, nm, nf, ne, nc, msg]\n",
        "\n",
        "\n",
        "def dataset_stats(path='coco128.yaml', autodownload=False, verbose=False):\n",
        "    \"\"\" Return dataset statistics dictionary with images and instances counts per split per class\n",
        "    Usage1: from utils.datasets import *; dataset_stats('coco128.yaml', verbose=True)\n",
        "    Usage2: from utils.datasets import *; dataset_stats('../datasets/coco128.zip', verbose=True)\n",
        "    \n",
        "    Arguments\n",
        "        path:           Path to data.yaml or data.zip (with data.yaml inside data.zip)\n",
        "        autodownload:   Attempt to download dataset if not found locally\n",
        "        verbose:        Print stats dictionary\n",
        "    \"\"\"\n",
        "\n",
        "    def round_labels(labels):\n",
        "        # Update labels to integer class and 6 decimal place floats\n",
        "        return [[int(c), *[round(x, 6) for x in points]] for c, *points in labels]\n",
        "\n",
        "    def unzip(path):\n",
        "        # Unzip data.zip TODO: CONSTRAINT: path/to/abc.zip MUST unzip to 'path/to/abc/'\n",
        "        if str(path).endswith('.zip'):  # path is data.zip\n",
        "            assert os.system(f'unzip -q {path} -d {path.parent}') == 0, f'Error unzipping {path}'\n",
        "            data_dir = path.with_suffix('')  # dataset directory\n",
        "            return True, data_dir, list(data_dir.rglob('*.yaml'))[0]  # zipped, data_dir, yaml_path\n",
        "        else:  # path is data.yaml\n",
        "            return False, None, path\n",
        "\n",
        "    zipped, data_dir, yaml_path = unzip(Path(path))\n",
        "    with open(check_file(yaml_path)) as f:\n",
        "        data = yaml.safe_load(f)  # data dict\n",
        "        if zipped:\n",
        "            data['path'] = data_dir  # TODO: should this be dir.resolve()?\n",
        "    check_dataset(data, autodownload)  # download dataset if missing\n",
        "    nc = data['nc']  # number of classes\n",
        "    stats = {'nc': nc, 'names': data['names']}  # statistics dictionary\n",
        "    for split in 'train', 'val', 'test':\n",
        "        if data.get(split) is None:\n",
        "            stats[split] = None  # i.e. no test set\n",
        "            continue\n",
        "        x = []\n",
        "        dataset = LoadImagesAndLabels(data[split], augment=False, rect=True)  # load dataset\n",
        "        if split == 'train':\n",
        "            cache_path = Path(dataset.label_files[0]).parent.with_suffix('.cache')  # *.cache path\n",
        "        for label in tqdm(dataset.labels, total=dataset.n, desc='Statistics'):\n",
        "            x.append(np.bincount(label[:, 0].astype(int), minlength=nc))\n",
        "        x = np.array(x)  # shape(128x80)\n",
        "        stats[split] = {'instance_stats': {'total': int(x.sum()), 'per_class': x.sum(0).tolist()},\n",
        "                        'image_stats': {'total': dataset.n, 'unlabelled': int(np.all(x == 0, 1).sum()),\n",
        "                                        'per_class': (x > 0).sum(0).tolist()},\n",
        "                        'labels': [{str(Path(k).name): round_labels(v.tolist())} for k, v in\n",
        "                                   zip(dataset.img_files, dataset.labels)]}\n",
        "\n",
        "    # Save, print and return\n",
        "    with open(cache_path.with_suffix('.json'), 'w') as f:\n",
        "        json.dump(stats, f)  # save stats *.json\n",
        "    if verbose:\n",
        "        print(json.dumps(stats, indent=2, sort_keys=False))\n",
        "        # print(yaml.dump([stats], sort_keys=False, default_flow_style=False))\n",
        "    return stats\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing datasets.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0L42pdJM82D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72686f9b-e2b7-4d7c-fd16-a75ac4e2e170"
      },
      "source": [
        "%%writefile detect.py\n",
        "\"\"\"Run inference with a YOLOv5 model on images, videos, directories, streams\n",
        "\n",
        "Usage:\n",
        "    $ python path/to/detect.py --source path/to/img.jpg --weights yolov5s.pt --img 640\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "FILE = Path(__file__).absolute()\n",
        "sys.path.append(FILE.parents[0].as_posix())  # add yolov5/ to path\n",
        "from matplotlib import pyplot as plt\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import check_img_size, check_requirements, check_imshow, colorstr, non_max_suppression, \\\n",
        "    apply_classifier, scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path, save_one_box\n",
        "from utils.plots import colors, plot_one_box\n",
        "from utils.torch_utils import select_device, load_classifier, time_sync\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def run(weights='yolov5s.pt',  # model.pt path(s)\n",
        "        source='data/images',  # file/dir/URL/glob, 0 for webcam\n",
        "        imgsz=416,  # inference size (pixels)\n",
        "        conf_thres=0.25,  # confidence threshold\n",
        "        iou_thres=0.45,  # NMS IOU threshold\n",
        "        max_det=1000,  # maximum detections per image\n",
        "        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
        "        view_img=False,  # show results\n",
        "        save_txt=False,  # save results to *.txt\n",
        "        save_conf=False,  # save confidences in --save-txt labels\n",
        "        save_crop=False,  # save cropped prediction boxes\n",
        "        nosave=False,  # do not save images/videos\n",
        "        classes=None,  # filter by class: --class 0, or --class 0 2 3\n",
        "        agnostic_nms=False,  # class-agnostic NMS\n",
        "        augment=False,  # augmented inference\n",
        "        visualize=False,  # visualize features\n",
        "        update=False,  # update all models\n",
        "        project='runs/detect',  # save results to project/name\n",
        "        name='exp',  # save results to project/name\n",
        "        exist_ok=False,  # existing project/name ok, do not increment\n",
        "        line_thickness=3,  # bounding box thickness (pixels)\n",
        "        hide_labels=False,  # hide labels\n",
        "        hide_conf=False,  # hide confidences\n",
        "        half=False,  # use FP16 half-precision inference\n",
        "        ):\n",
        "    save_img = not nosave and not source.endswith('.txt')  # save inference images\n",
        "    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n",
        "        ('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
        "\n",
        "    # Directories\n",
        "    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n",
        "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
        "\n",
        "    # Initialize\n",
        "    set_logging()\n",
        "    device = select_device(device)\n",
        "    half &= device.type != 'cpu'  # half precision only supported on CUDA\n",
        "\n",
        "    # Load model\n",
        "    w = weights[0] if isinstance(weights, list) else weights\n",
        "    classify, pt, onnx = False, w.endswith('.pt'), w.endswith('.onnx')  # inference type\n",
        "    stride, names = 64, [f'class{i}' for i in range(1000)]  # assign defaults\n",
        "    if pt:\n",
        "        model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "        stride = int(model.stride.max())  # model stride\n",
        "        names = model.module.names if hasattr(model, 'module') else model.names  # get class names\n",
        "        if half:\n",
        "            model.half()  # to FP16\n",
        "        if classify:  # second-stage classifier\n",
        "            modelc = load_classifier(name='resnet50', n=2)  # initialize\n",
        "            modelc.load_state_dict(torch.load('resnet50.pt', map_location=device)['model']).to(device).eval()\n",
        "    elif onnx:\n",
        "        check_requirements(('onnx', 'onnxruntime'))\n",
        "        import onnxruntime\n",
        "        session = onnxruntime.InferenceSession(w, None)\n",
        "    imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
        "\n",
        "    # Dataloader\n",
        "    if webcam:\n",
        "        view_img = check_imshow()\n",
        "        cudnn.benchmark = True  # set True to speed up constant image size inference\n",
        "        dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n",
        "        bs = len(dataset)  # batch_size\n",
        "    else:\n",
        "        dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
        "        bs = 1  # batch_size\n",
        "    vid_path, vid_writer = [None] * bs, [None] * bs\n",
        "\n",
        "    # Run inference\n",
        "    if pt and device.type != 'cpu':\n",
        "        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
        "    t0 = time.time()\n",
        "    for en,(path, img, im0s, vid_cap) in enumerate(dataset):\n",
        "        all_total=[]\n",
        "#         img=cv2.resize(img,(416,416))\n",
        "        if pt:\n",
        "            img = torch.from_numpy(img).to(device)\n",
        "            img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "        elif onnx:\n",
        "            img = img.astype('float32')\n",
        "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "        if len(img.shape) == 3:\n",
        "            img = img[None]  # expand for batch dim\n",
        "\n",
        "        # Inference\n",
        "        t1 = time_sync()\n",
        "        if pt:\n",
        "            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
        "            pred = model(img, augment=augment, visualize=visualize)[0]\n",
        "        elif onnx:\n",
        "            pred = torch.tensor(session.run([session.get_outputs()[0].name], {session.get_inputs()[0].name: img}))\n",
        "\n",
        "        # NMS\n",
        "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "        t2 = time_sync()\n",
        "\n",
        "        # Second-stage classifier (optional)\n",
        "        if classify:\n",
        "            pred = apply_classifier(pred, modelc, img, im0s)\n",
        "\n",
        "        # Process predictions\n",
        "        for i, det in enumerate(pred):  # detections per image\n",
        "            if webcam:  # batch_size >= 1\n",
        "                p, s, im0, frame = path[i], f'{i}: ', im0s[i].copy(), dataset.count\n",
        "            else:\n",
        "                p, s, im0, frame = path, '', im0s.copy(), getattr(dataset, 'frame', 0)\n",
        "\n",
        "            p = Path(p)  # to Path\n",
        "            save_path = str(save_dir / p.name)  # img.jpg\n",
        "            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt\n",
        "            s += '%gx%g ' % img.shape[2:]  # print string\n",
        "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "            imc = im0.copy() if save_crop else im0  # for save_crop\n",
        "            if len(det):\n",
        "                # Rescale boxes from img_size to im0 size\n",
        "#                 det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
        "\n",
        "                # Print results\n",
        "                for c in det[:, -1].unique():\n",
        "                    n = (det[:, -1] == c).sum()  # detections per class\n",
        "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "\n",
        "                # Write results\n",
        "                for *xyxy, conf, cls in reversed(det):\n",
        "                    if save_txt:  # Write to file\n",
        "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
        "                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
        "                        with open(txt_path + '.txt', 'a') as f:\n",
        "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
        "\n",
        "                    if save_img or save_crop or view_img:  # Add bbox to image\n",
        "                        c = int(cls)  # integer class\n",
        "                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
        "                        plot_one_box(xyxy, im0, label=label, color=colors(c, True), line_thickness=line_thickness)\n",
        "                        if save_crop:\n",
        "                            save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n",
        "                    w=im0.shape[1]\n",
        "                    x1=int(xyxy[0].cpu().numpy().item()) \n",
        "                    y1=int(xyxy[1].cpu().numpy().item())\n",
        "                    x2=int(xyxy[2].cpu().numpy().item())\n",
        "                    y2=int(xyxy[3].cpu().numpy().item())\n",
        "                    total=[x1,y1,x2,y2]\n",
        "                    \n",
        "                    if label!=None:\n",
        "                            all_total.append({\n",
        "                            'boxes': total,\n",
        "                            'scores': label})\n",
        "\n",
        "        np.save('/content/gdrive/MyDrive/'+path.split('/')[-1].split('.')[0]+'_1'+weights[0].split('/')[-1].split('.')[0]+'.npy',all_total)\n",
        "    if save_txt or save_img:\n",
        "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
        "        print(f\"Results saved to {save_dir}{s}\")\n",
        "\n",
        "    if update:\n",
        "        strip_optimizer(weights)  # update model (to fix SourceChangeWarning)\n",
        "\n",
        "    print(f'Done. ({time.time() - t0:.3f}s)')\n",
        "    \n",
        "\n",
        "def parse_opt():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--weights', nargs='+', type=str, default='yolov5s.pt', help='model.pt path(s)')\n",
        "    parser.add_argument('--source', type=str, default='data/images', help='file/dir/URL/glob, 0 for webcam')\n",
        "    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=416, help='inference size (pixels)')\n",
        "    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')\n",
        "    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')\n",
        "    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')\n",
        "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--view-img', action='store_true', help='show results')\n",
        "    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
        "    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
        "    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')\n",
        "    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')\n",
        "    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')\n",
        "    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
        "    parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
        "    parser.add_argument('--visualize', action='store_true', help='visualize features')\n",
        "    parser.add_argument('--update', action='store_true', help='update all models')\n",
        "    parser.add_argument('--project', default='runs/detect', help='save results to project/name')\n",
        "    parser.add_argument('--name', default='exp', help='save results to project/name')\n",
        "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
        "    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')\n",
        "    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')\n",
        "    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')\n",
        "    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n",
        "    opt = parser.parse_args()\n",
        "    return opt\n",
        "\n",
        "\n",
        "def main(opt):\n",
        "    print(colorstr('detect: ') + ', '.join(f'{k}={v}' for k, v in vars(opt).items()))\n",
        "    check_requirements(exclude=('tensorboard', 'thop'))\n",
        "    run(**vars(opt))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    opt = parse_opt()\n",
        "    main(opt)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting detect.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etBaCGDfXjbh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0abd53f1-d1f9-47bc-86c1-e248b472b469"
      },
      "source": [
        "!python detect.py --source /content/gdrive/MyDrive/hardhat  --weights /content/gdrive/MyDrive/last_0.pt --conf 0.25 --imgsz 1280"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/gdrive/MyDrive/last_0.pt'], source=/content/gdrive/MyDrive/hardhat, imgsz=1280, conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False\n",
            "YOLOv5 🚀 v5.0-339-g53bfcbe torch 1.9.0+cu102 CUDA:0 (Tesla T4, 15109.75MB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 224 layers, 7053910 parameters, 0 gradients, 16.3 GFLOPs\n",
            "image 1/9035 /content/gdrive/MyDrive/hardhat/hardhat_0.jpg: image 2/9035 /content/gdrive/MyDrive/hardhat/hardhat_1.jpg: image 3/9035 /content/gdrive/MyDrive/hardhat/hardhat_10.jpg: image 4/9035 /content/gdrive/MyDrive/hardhat/hardhat_100.jpg: image 5/9035 /content/gdrive/MyDrive/hardhat/hardhat_1000.jpg: image 6/9035 /content/gdrive/MyDrive/hardhat/hardhat_1001.jpg: image 7/9035 /content/gdrive/MyDrive/hardhat/hardhat_1002.jpg: image 8/9035 /content/gdrive/MyDrive/hardhat/hardhat_1003.jpg: image 9/9035 /content/gdrive/MyDrive/hardhat/hardhat_1004.jpg: image 10/9035 /content/gdrive/MyDrive/hardhat/hardhat_1005.jpg: image 11/9035 /content/gdrive/MyDrive/hardhat/hardhat_1006.jpg: image 12/9035 /content/gdrive/MyDrive/hardhat/hardhat_1007.jpg: image 13/9035 /content/gdrive/MyDrive/hardhat/hardhat_1008.jpg: image 14/9035 /content/gdrive/MyDrive/hardhat/hardhat_1009.jpg: image 15/9035 /content/gdrive/MyDrive/hardhat/hardhat_101.jpg: image 16/9035 /content/gdrive/MyDrive/hardhat/hardhat_1010.jpg: image 17/9035 /content/gdrive/MyDrive/hardhat/hardhat_1011.jpg: image 18/9035 /content/gdrive/MyDrive/hardhat/hardhat_1012.jpg: image 19/9035 /content/gdrive/MyDrive/hardhat/hardhat_1013.jpg: image 20/9035 /content/gdrive/MyDrive/hardhat/hardhat_1014.jpg: image 21/9035 /content/gdrive/MyDrive/hardhat/hardhat_1015.jpg: image 22/9035 /content/gdrive/MyDrive/hardhat/hardhat_1016.jpg: image 23/9035 /content/gdrive/MyDrive/hardhat/hardhat_1017.jpg: image 24/9035 /content/gdrive/MyDrive/hardhat/hardhat_1018.jpg: image 25/9035 /content/gdrive/MyDrive/hardhat/hardhat_1019.jpg: image 26/9035 /content/gdrive/MyDrive/hardhat/hardhat_102.jpg: image 27/9035 /content/gdrive/MyDrive/hardhat/hardhat_1020.jpg: image 28/9035 /content/gdrive/MyDrive/hardhat/hardhat_1021.jpg: image 29/9035 /content/gdrive/MyDrive/hardhat/hardhat_1022.jpg: image 30/9035 /content/gdrive/MyDrive/hardhat/hardhat_1023.jpg: image 31/9035 /content/gdrive/MyDrive/hardhat/hardhat_1024.jpg: image 32/9035 /content/gdrive/MyDrive/hardhat/hardhat_1025.jpg: image 33/9035 /content/gdrive/MyDrive/hardhat/hardhat_1026.jpg: image 34/9035 /content/gdrive/MyDrive/hardhat/hardhat_1027.jpg: image 35/9035 /content/gdrive/MyDrive/hardhat/hardhat_1028.jpg: image 36/9035 /content/gdrive/MyDrive/hardhat/hardhat_1029.jpg: image 37/9035 /content/gdrive/MyDrive/hardhat/hardhat_103.jpg: image 38/9035 /content/gdrive/MyDrive/hardhat/hardhat_1030.jpg: image 39/9035 /content/gdrive/MyDrive/hardhat/hardhat_1031.jpg: image 40/9035 /content/gdrive/MyDrive/hardhat/hardhat_1032.jpg: image 41/9035 /content/gdrive/MyDrive/hardhat/hardhat_1033.jpg: image 42/9035 /content/gdrive/MyDrive/hardhat/hardhat_1034.jpg: image 43/9035 /content/gdrive/MyDrive/hardhat/hardhat_1035.jpg: image 44/9035 /content/gdrive/MyDrive/hardhat/hardhat_1036.jpg: image 45/9035 /content/gdrive/MyDrive/hardhat/hardhat_1037.jpg: image 46/9035 /content/gdrive/MyDrive/hardhat/hardhat_1038.jpg: image 47/9035 /content/gdrive/MyDrive/hardhat/hardhat_1039.jpg: image 48/9035 /content/gdrive/MyDrive/hardhat/hardhat_104.jpg: image 49/9035 /content/gdrive/MyDrive/hardhat/hardhat_1040.jpg: image 50/9035 /content/gdrive/MyDrive/hardhat/hardhat_1041.jpg: image 51/9035 /content/gdrive/MyDrive/hardhat/hardhat_1042.jpg: image 52/9035 /content/gdrive/MyDrive/hardhat/hardhat_1043.jpg: image 53/9035 /content/gdrive/MyDrive/hardhat/hardhat_1044.jpg: image 54/9035 /content/gdrive/MyDrive/hardhat/hardhat_1045.jpg: image 55/9035 /content/gdrive/MyDrive/hardhat/hardhat_1046.jpg: image 56/9035 /content/gdrive/MyDrive/hardhat/hardhat_1047.jpg: image 57/9035 /content/gdrive/MyDrive/hardhat/hardhat_1048.jpg: image 58/9035 /content/gdrive/MyDrive/hardhat/hardhat_1049.jpg: image 59/9035 /content/gdrive/MyDrive/hardhat/hardhat_105.jpg: image 60/9035 /content/gdrive/MyDrive/hardhat/hardhat_1050.jpg: image 61/9035 /content/gdrive/MyDrive/hardhat/hardhat_1051.jpg: image 62/9035 /content/gdrive/MyDrive/hardhat/hardhat_1052.jpg: image 63/9035 /content/gdrive/MyDrive/hardhat/hardhat_1053.jpg: image 64/9035 /content/gdrive/MyDrive/hardhat/hardhat_1054.jpg: image 65/9035 /content/gdrive/MyDrive/hardhat/hardhat_1055.jpg: image 66/9035 /content/gdrive/MyDrive/hardhat/hardhat_1056.jpg: image 67/9035 /content/gdrive/MyDrive/hardhat/hardhat_1057.jpg: image 68/9035 /content/gdrive/MyDrive/hardhat/hardhat_1058.jpg: image 69/9035 /content/gdrive/MyDrive/hardhat/hardhat_1059.jpg: image 70/9035 /content/gdrive/MyDrive/hardhat/hardhat_106.jpg: image 71/9035 /content/gdrive/MyDrive/hardhat/hardhat_1060.jpg: image 72/9035 /content/gdrive/MyDrive/hardhat/hardhat_1061.jpg: image 73/9035 /content/gdrive/MyDrive/hardhat/hardhat_1062.jpg: image 74/9035 /content/gdrive/MyDrive/hardhat/hardhat_1063.jpg: image 75/9035 /content/gdrive/MyDrive/hardhat/hardhat_1064.jpg: image 76/9035 /content/gdrive/MyDrive/hardhat/hardhat_1065.jpg: image 77/9035 /content/gdrive/MyDrive/hardhat/hardhat_1066.jpg: image 78/9035 /content/gdrive/MyDrive/hardhat/hardhat_1067.jpg: image 79/9035 /content/gdrive/MyDrive/hardhat/hardhat_1068.jpg: image 80/9035 /content/gdrive/MyDrive/hardhat/hardhat_1069.jpg: image 81/9035 /content/gdrive/MyDrive/hardhat/hardhat_107.jpg: image 82/9035 /content/gdrive/MyDrive/hardhat/hardhat_1070.jpg: image 83/9035 /content/gdrive/MyDrive/hardhat/hardhat_1071.jpg: image 84/9035 /content/gdrive/MyDrive/hardhat/hardhat_1072.jpg: image 85/9035 /content/gdrive/MyDrive/hardhat/hardhat_1073.jpg: image 86/9035 /content/gdrive/MyDrive/hardhat/hardhat_1074.jpg: image 87/9035 /content/gdrive/MyDrive/hardhat/hardhat_1075.jpg: image 88/9035 /content/gdrive/MyDrive/hardhat/hardhat_1076.jpg: image 89/9035 /content/gdrive/MyDrive/hardhat/hardhat_1077.jpg: image 90/9035 /content/gdrive/MyDrive/hardhat/hardhat_1078.jpg: image 91/9035 /content/gdrive/MyDrive/hardhat/hardhat_1079.jpg: image 92/9035 /content/gdrive/MyDrive/hardhat/hardhat_108.jpg: image 93/9035 /content/gdrive/MyDrive/hardhat/hardhat_1080.jpg: image 94/9035 /content/gdrive/MyDrive/hardhat/hardhat_1081.jpg: image 95/9035 /content/gdrive/MyDrive/hardhat/hardhat_1082.jpg: image 96/9035 /content/gdrive/MyDrive/hardhat/hardhat_1083.jpg: image 97/9035 /content/gdrive/MyDrive/hardhat/hardhat_1084.jpg: image 98/9035 /content/gdrive/MyDrive/hardhat/hardhat_1085.jpg: image 99/9035 /content/gdrive/MyDrive/hardhat/hardhat_1086.jpg: image 100/9035 /content/gdrive/MyDrive/hardhat/hardhat_1087.jpg: image 101/9035 /content/gdrive/MyDrive/hardhat/hardhat_1088.jpg: image 102/9035 /content/gdrive/MyDrive/hardhat/hardhat_1089.jpg: image 103/9035 /content/gdrive/MyDrive/hardhat/hardhat_109.jpg: image 104/9035 /content/gdrive/MyDrive/hardhat/hardhat_1090.jpg: image 105/9035 /content/gdrive/MyDrive/hardhat/hardhat_1091.jpg: image 106/9035 /content/gdrive/MyDrive/hardhat/hardhat_1092.jpg: image 107/9035 /content/gdrive/MyDrive/hardhat/hardhat_1093.jpg: image 108/9035 /content/gdrive/MyDrive/hardhat/hardhat_1094.jpg: image 109/9035 /content/gdrive/MyDrive/hardhat/hardhat_1095.jpg: image 110/9035 /content/gdrive/MyDrive/hardhat/hardhat_1096.jpg: image 111/9035 /content/gdrive/MyDrive/hardhat/hardhat_1097.jpg: image 112/9035 /content/gdrive/MyDrive/hardhat/hardhat_1098.jpg: image 113/9035 /content/gdrive/MyDrive/hardhat/hardhat_1099.jpg: image 114/9035 /content/gdrive/MyDrive/hardhat/hardhat_11.jpg: image 115/9035 /content/gdrive/MyDrive/hardhat/hardhat_110.jpg: image 116/9035 /content/gdrive/MyDrive/hardhat/hardhat_1100.jpg: image 117/9035 /content/gdrive/MyDrive/hardhat/hardhat_1101.jpg: image 118/9035 /content/gdrive/MyDrive/hardhat/hardhat_1102.jpg: image 119/9035 /content/gdrive/MyDrive/hardhat/hardhat_1103.jpg: image 120/9035 /content/gdrive/MyDrive/hardhat/hardhat_1104.jpg: image 121/9035 /content/gdrive/MyDrive/hardhat/hardhat_1105.jpg: image 122/9035 /content/gdrive/MyDrive/hardhat/hardhat_1106.jpg: image 123/9035 /content/gdrive/MyDrive/hardhat/hardhat_1107.jpg: image 124/9035 /content/gdrive/MyDrive/hardhat/hardhat_1108.jpg: image 125/9035 /content/gdrive/MyDrive/hardhat/hardhat_1109.jpg: image 126/9035 /content/gdrive/MyDrive/hardhat/hardhat_111.jpg: image 127/9035 /content/gdrive/MyDrive/hardhat/hardhat_1110.jpg: image 128/9035 /content/gdrive/MyDrive/hardhat/hardhat_1111.jpg: image 129/9035 /content/gdrive/MyDrive/hardhat/hardhat_1112.jpg: image 130/9035 /content/gdrive/MyDrive/hardhat/hardhat_1113.jpg: image 131/9035 /content/gdrive/MyDrive/hardhat/hardhat_1114.jpg: image 132/9035 /content/gdrive/MyDrive/hardhat/hardhat_1115.jpg: image 133/9035 /content/gdrive/MyDrive/hardhat/hardhat_1116.jpg: image 134/9035 /content/gdrive/MyDrive/hardhat/hardhat_1117.jpg: image 135/9035 /content/gdrive/MyDrive/hardhat/hardhat_1118.jpg: image 136/9035 /content/gdrive/MyDrive/hardhat/hardhat_1119.jpg: image 137/9035 /content/gdrive/MyDrive/hardhat/hardhat_112.jpg: image 138/9035 /content/gdrive/MyDrive/hardhat/hardhat_1120.jpg: image 139/9035 /content/gdrive/MyDrive/hardhat/hardhat_1121.jpg: image 140/9035 /content/gdrive/MyDrive/hardhat/hardhat_1122.jpg: image 141/9035 /content/gdrive/MyDrive/hardhat/hardhat_1123.jpg: image 142/9035 /content/gdrive/MyDrive/hardhat/hardhat_1124.jpg: image 143/9035 /content/gdrive/MyDrive/hardhat/hardhat_1125.jpg: image 144/9035 /content/gdrive/MyDrive/hardhat/hardhat_1126.jpg: image 145/9035 /content/gdrive/MyDrive/hardhat/hardhat_1127.jpg: image 146/9035 /content/gdrive/MyDrive/hardhat/hardhat_1128.jpg: image 147/9035 /content/gdrive/MyDrive/hardhat/hardhat_1129.jpg: image 148/9035 /content/gdrive/MyDrive/hardhat/hardhat_113.jpg: image 149/9035 /content/gdrive/MyDrive/hardhat/hardhat_1130.jpg: image 150/9035 /content/gdrive/MyDrive/hardhat/hardhat_1131.jpg: image 151/9035 /content/gdrive/MyDrive/hardhat/hardhat_1132.jpg: image 152/9035 /content/gdrive/MyDrive/hardhat/hardhat_1133.jpg: image 153/9035 /content/gdrive/MyDrive/hardhat/hardhat_1134.jpg: image 154/9035 /content/gdrive/MyDrive/hardhat/hardhat_1135.jpg: image 155/9035 /content/gdrive/MyDrive/hardhat/hardhat_1136.jpg: image 156/9035 /content/gdrive/MyDrive/hardhat/hardhat_1137.jpg: image 157/9035 /content/gdrive/MyDrive/hardhat/hardhat_1138.jpg: image 158/9035 /content/gdrive/MyDrive/hardhat/hardhat_1139.jpg: image 159/9035 /content/gdrive/MyDrive/hardhat/hardhat_114.jpg: image 160/9035 /content/gdrive/MyDrive/hardhat/hardhat_1140.jpg: image 161/9035 /content/gdrive/MyDrive/hardhat/hardhat_1141.jpg: image 162/9035 /content/gdrive/MyDrive/hardhat/hardhat_1142.jpg: image 163/9035 /content/gdrive/MyDrive/hardhat/hardhat_1143.jpg: image 164/9035 /content/gdrive/MyDrive/hardhat/hardhat_1144.jpg: image 165/9035 /content/gdrive/MyDrive/hardhat/hardhat_1145.jpg: image 166/9035 /content/gdrive/MyDrive/hardhat/hardhat_1146.jpg: image 167/9035 /content/gdrive/MyDrive/hardhat/hardhat_1147.jpg: image 168/9035 /content/gdrive/MyDrive/hardhat/hardhat_1148.jpg: image 169/9035 /content/gdrive/MyDrive/hardhat/hardhat_1149.jpg: image 170/9035 /content/gdrive/MyDrive/hardhat/hardhat_115.jpg: image 171/9035 /content/gdrive/MyDrive/hardhat/hardhat_1150.jpg: image 172/9035 /content/gdrive/MyDrive/hardhat/hardhat_1151.jpg: image 173/9035 /content/gdrive/MyDrive/hardhat/hardhat_1152.jpg: image 174/9035 /content/gdrive/MyDrive/hardhat/hardhat_1153.jpg: image 175/9035 /content/gdrive/MyDrive/hardhat/hardhat_1154.jpg: image 176/9035 /content/gdrive/MyDrive/hardhat/hardhat_1155.jpg: image 177/9035 /content/gdrive/MyDrive/hardhat/hardhat_1156.jpg: image 178/9035 /content/gdrive/MyDrive/hardhat/hardhat_1157.jpg: image 179/9035 /content/gdrive/MyDrive/hardhat/hardhat_1158.jpg: image 180/9035 /content/gdrive/MyDrive/hardhat/hardhat_1159.jpg: image 181/9035 /content/gdrive/MyDrive/hardhat/hardhat_116.jpg: image 182/9035 /content/gdrive/MyDrive/hardhat/hardhat_1160.jpg: image 183/9035 /content/gdrive/MyDrive/hardhat/hardhat_1161.jpg: image 184/9035 /content/gdrive/MyDrive/hardhat/hardhat_1162.jpg: image 185/9035 /content/gdrive/MyDrive/hardhat/hardhat_1163.jpg: image 186/9035 /content/gdrive/MyDrive/hardhat/hardhat_1164.jpg: image 187/9035 /content/gdrive/MyDrive/hardhat/hardhat_1165.jpg: image 188/9035 /content/gdrive/MyDrive/hardhat/hardhat_1166.jpg: image 189/9035 /content/gdrive/MyDrive/hardhat/hardhat_1167.jpg: image 190/9035 /content/gdrive/MyDrive/hardhat/hardhat_1168.jpg: image 191/9035 /content/gdrive/MyDrive/hardhat/hardhat_1169.jpg: image 192/9035 /content/gdrive/MyDrive/hardhat/hardhat_117.jpg: image 193/9035 /content/gdrive/MyDrive/hardhat/hardhat_1170.jpg: image 194/9035 /content/gdrive/MyDrive/hardhat/hardhat_1171.jpg: image 195/9035 /content/gdrive/MyDrive/hardhat/hardhat_1172.jpg: image 196/9035 /content/gdrive/MyDrive/hardhat/hardhat_1173.jpg: image 197/9035 /content/gdrive/MyDrive/hardhat/hardhat_1174.jpg: image 198/9035 /content/gdrive/MyDrive/hardhat/hardhat_1175.jpg: image 199/9035 /content/gdrive/MyDrive/hardhat/hardhat_1176.jpg: image 200/9035 /content/gdrive/MyDrive/hardhat/hardhat_1177.jpg: image 201/9035 /content/gdrive/MyDrive/hardhat/hardhat_1178.jpg: image 202/9035 /content/gdrive/MyDrive/hardhat/hardhat_1179.jpg: image 203/9035 /content/gdrive/MyDrive/hardhat/hardhat_118.jpg: image 204/9035 /content/gdrive/MyDrive/hardhat/hardhat_1180.jpg: image 205/9035 /content/gdrive/MyDrive/hardhat/hardhat_1181.jpg: image 206/9035 /content/gdrive/MyDrive/hardhat/hardhat_1182.jpg: image 207/9035 /content/gdrive/MyDrive/hardhat/hardhat_1183.jpg: image 208/9035 /content/gdrive/MyDrive/hardhat/hardhat_1184.jpg: image 209/9035 /content/gdrive/MyDrive/hardhat/hardhat_1185.jpg: image 210/9035 /content/gdrive/MyDrive/hardhat/hardhat_1186.jpg: image 211/9035 /content/gdrive/MyDrive/hardhat/hardhat_1187.jpg: image 212/9035 /content/gdrive/MyDrive/hardhat/hardhat_1188.jpg: image 213/9035 /content/gdrive/MyDrive/hardhat/hardhat_1189.jpg: image 214/9035 /content/gdrive/MyDrive/hardhat/hardhat_119.jpg: image 215/9035 /content/gdrive/MyDrive/hardhat/hardhat_1190.jpg: image 216/9035 /content/gdrive/MyDrive/hardhat/hardhat_1191.jpg: image 217/9035 /content/gdrive/MyDrive/hardhat/hardhat_1192.jpg: image 218/9035 /content/gdrive/MyDrive/hardhat/hardhat_1193.jpg: image 219/9035 /content/gdrive/MyDrive/hardhat/hardhat_1194.jpg: image 220/9035 /content/gdrive/MyDrive/hardhat/hardhat_1195.jpg: image 221/9035 /content/gdrive/MyDrive/hardhat/hardhat_1196.jpg: image 222/9035 /content/gdrive/MyDrive/hardhat/hardhat_1197.jpg: image 223/9035 /content/gdrive/MyDrive/hardhat/hardhat_1198.jpg: image 224/9035 /content/gdrive/MyDrive/hardhat/hardhat_1199.jpg: image 225/9035 /content/gdrive/MyDrive/hardhat/hardhat_12.jpg: image 226/9035 /content/gdrive/MyDrive/hardhat/hardhat_120.jpg: image 227/9035 /content/gdrive/MyDrive/hardhat/hardhat_1200.jpg: image 228/9035 /content/gdrive/MyDrive/hardhat/hardhat_1201.jpg: image 229/9035 /content/gdrive/MyDrive/hardhat/hardhat_1202.jpg: image 230/9035 /content/gdrive/MyDrive/hardhat/hardhat_1203.jpg: image 231/9035 /content/gdrive/MyDrive/hardhat/hardhat_1204.jpg: image 232/9035 /content/gdrive/MyDrive/hardhat/hardhat_1205.jpg: image 233/9035 /content/gdrive/MyDrive/hardhat/hardhat_1206.jpg: image 234/9035 /content/gdrive/MyDrive/hardhat/hardhat_1207.jpg: image 235/9035 /content/gdrive/MyDrive/hardhat/hardhat_1208.jpg: image 236/9035 /content/gdrive/MyDrive/hardhat/hardhat_1209.jpg: image 237/9035 /content/gdrive/MyDrive/hardhat/hardhat_121.jpg: image 238/9035 /content/gdrive/MyDrive/hardhat/hardhat_1210.jpg: image 239/9035 /content/gdrive/MyDrive/hardhat/hardhat_1211.jpg: image 240/9035 /content/gdrive/MyDrive/hardhat/hardhat_1212.jpg: image 241/9035 /content/gdrive/MyDrive/hardhat/hardhat_1213.jpg: image 242/9035 /content/gdrive/MyDrive/hardhat/hardhat_1214.jpg: image 243/9035 /content/gdrive/MyDrive/hardhat/hardhat_1215.jpg: image 244/9035 /content/gdrive/MyDrive/hardhat/hardhat_1216.jpg: image 245/9035 /content/gdrive/MyDrive/hardhat/hardhat_1217.jpg: image 246/9035 /content/gdrive/MyDrive/hardhat/hardhat_1218.jpg: image 247/9035 /content/gdrive/MyDrive/hardhat/hardhat_1219.jpg: image 248/9035 /content/gdrive/MyDrive/hardhat/hardhat_122.jpg: image 249/9035 /content/gdrive/MyDrive/hardhat/hardhat_1220.jpg: image 250/9035 /content/gdrive/MyDrive/hardhat/hardhat_1221.jpg: image 251/9035 /content/gdrive/MyDrive/hardhat/hardhat_1222.jpg: image 252/9035 /content/gdrive/MyDrive/hardhat/hardhat_1223.jpg: image 253/9035 /content/gdrive/MyDrive/hardhat/hardhat_1224.jpg: image 254/9035 /content/gdrive/MyDrive/hardhat/hardhat_1225.jpg: image 255/9035 /content/gdrive/MyDrive/hardhat/hardhat_1226.jpg: image 256/9035 /content/gdrive/MyDrive/hardhat/hardhat_1227.jpg: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMt111RuM4pz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d27aaa2-c956-44f3-bc0d-9e7a8af877bc"
      },
      "source": [
        "!python detect.py --source /content/gdrive/MyDrive/hardhat  --weights /content/gdrive/MyDrive/last_2.pt --conf 0.25 --imgsz 1280"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total = 4750\n",
            "train : 3800\n",
            "val   : 475\n",
            "test  : 475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKYTgleLNMxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df6c5b4-603c-4c4a-ec73-162078d0de2b"
      },
      "source": [
        "!python detect.py --source /content/gdrive/MyDrive/hardhat  --weights /content/gdrive/MyDrive/last_3.pt --conf 0.25 --imgsz 1280"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Path /content/Dataset/images/train does not exit\n",
            "Path /content/Dataset/labels/train does not exit\n",
            "Path /content/Dataset/images/val does not exit\n",
            "Path /content/Dataset/labels/val does not exit\n",
            "Path /content/Dataset/images/test does not exit\n",
            "Path /content/Dataset/labels/test does not exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3moy_plGNkcO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "707053d5-75c0-43f6-cef0-aee71b2f6025"
      },
      "source": [
        "!python detect.py --source /content/gdrive/MyDrive/hardhat  --weights /content/gdrive/MyDrive/last_4.pt --conf 0.25 --imgsz 1280"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 8149, done.\u001b[K\n",
            "remote: Counting objects: 100% (461/461), done.\u001b[K\n",
            "remote: Compressing objects: 100% (206/206), done.\u001b[K\n",
            "remote: Total 8149 (delta 317), reused 373 (delta 255), pack-reused 7688\u001b[K\n",
            "Receiving objects: 100% (8149/8149), 9.42 MiB | 20.30 MiB/s, done.\n",
            "Resolving deltas: 100% (5615/5615), done.\n",
            "/content/yolov5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdwB3kxLUhXw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}