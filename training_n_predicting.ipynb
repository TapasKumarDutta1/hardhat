{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_n_predicting.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/hardhat/blob/main/training_n_predicting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e7da4ac-09bb-4a66-df29-eca5481755fb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12WBdpyP-7K7"
      },
      "source": [
        "import cv2\n",
        "cap= cv2.VideoCapture('/content/gdrive/MyDrive/Top 10 Safety Vest For Construction For Men And Women.mp4')\n",
        "i=0\n",
        "while(cap.isOpened()):\n",
        "    ret, frame = cap.read()\n",
        "    if ret == False:\n",
        "        break\n",
        "    cv2.imwrite('/content/gdrive/MyDrive/hardhat_'+str(i)+'.jpg',frame)\n",
        "    i+=1\n",
        " \n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/gdrive/MyDrive/HardHat_Dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zOPOudIvXMKs",
        "outputId": "6fa573cc-0fe3-4302-e4c3-37f3cb7d7de1"
      },
      "source": [
        "import glob\n",
        "a=glob.glob('/content/annotations/*')\n",
        "a[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/annotations/hard_hat_workers4204.xml'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ5bhg7BMrMZ"
      },
      "source": [
        "!mkdir -p Dataset/labels\n",
        "!mkdir -p Dataset/images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOBXXTTdXr1W"
      },
      "source": [
        "classes = ['helmet']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj_Qu0XRdUq6"
      },
      "source": [
        "def convert_annot(size , box):\n",
        "    x1 = int(box[0])\n",
        "    y1 = int(box[1])\n",
        "    x2 = int(box[2])\n",
        "    y2 = int(box[3])\n",
        "\n",
        "    dw = np.float32(1. / int(size[0]))\n",
        "    dh = np.float32(1. / int(size[1]))\n",
        "\n",
        "    w = x2 - x1\n",
        "    h = y2 - y1\n",
        "    x = x1 + (w / 2)\n",
        "    y = y1 + (h / 2)\n",
        "\n",
        "    x = x * dw\n",
        "    w = w * dw\n",
        "    y = y * dh\n",
        "    h = h * dh\n",
        "    return [x, y, w, h]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c9YjY_IZpii"
      },
      "source": [
        "def save_txt_file(img_jpg_file_name, size, img_box):\n",
        "    save_file_name = '/content/Dataset/labels/' +  img_jpg_file_name + '.txt'\n",
        "    #file_path = open(save_file_name, \"a+\")\n",
        "    with open(save_file_name ,'a+') as file_path:\n",
        "        for box in img_box:\n",
        "\n",
        "            cls_num = classes.index(box[0])\n",
        "\n",
        "            new_box = convert_annot(size, box[1:])\n",
        "\n",
        "            file_path.write(f\"{cls_num} {new_box[0]} {new_box[1]} {new_box[2]} {new_box[3]}\\n\")\n",
        "\n",
        "        file_path.flush()\n",
        "        file_path.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grXXPqLkgNPr"
      },
      "source": [
        "def get_xml_data(file_path, img_xml_file):\n",
        "    img_path = file_path + '/' + img_xml_file + '.xml'\n",
        "    #print(img_path)\n",
        "\n",
        "    dom = parse(img_path)\n",
        "    root = dom.documentElement\n",
        "    img_name = root.getElementsByTagName(\"filename\")[0].childNodes[0].data\n",
        "    img_size = root.getElementsByTagName(\"size\")[0]\n",
        "    objects = root.getElementsByTagName(\"object\")\n",
        "    img_w = img_size.getElementsByTagName(\"width\")[0].childNodes[0].data\n",
        "    img_h = img_size.getElementsByTagName(\"height\")[0].childNodes[0].data\n",
        "    img_c = img_size.getElementsByTagName(\"depth\")[0].childNodes[0].data\n",
        "   \n",
        "    img_box = []\n",
        "    for box in objects:\n",
        "        cls_name = box.getElementsByTagName(\"name\")[0].childNodes[0].data\n",
        "        if cls_name=='helmet':\n",
        "            x1 = int(box.getElementsByTagName(\"xmin\")[0].childNodes[0].data)\n",
        "            y1 = int(box.getElementsByTagName(\"ymin\")[0].childNodes[0].data)\n",
        "            x2 = int(box.getElementsByTagName(\"xmax\")[0].childNodes[0].data)\n",
        "            y2 = int(box.getElementsByTagName(\"ymax\")[0].childNodes[0].data)\n",
        "\n",
        "            img_jpg_file_name = img_xml_file + '.jpg'\n",
        "            img_box.append([cls_name, x1, y1, x2, y2])\n",
        "\n",
        "    # test_dataset_box_feature(img_jpg_file_name, img_box)\n",
        "    save_txt_file(img_xml_file, [img_w, img_h], img_box)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0L42pdJM82D"
      },
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "from pathlib import Path\n",
        "from xml.dom.minidom import parse\n",
        "from shutil import copyfile\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etBaCGDfXjbh"
      },
      "source": [
        "import os\n",
        "files = os.listdir('/content/annotations')\n",
        "for file in files:\n",
        "    file_xml = file.split(\".\")\n",
        "    get_xml_data('/content/annotations', file_xml[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMt111RuM4pz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d27aaa2-c956-44f3-bc0d-9e7a8af877bc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "image_list = os.listdir('/content/images')\n",
        "train_list, test_list = train_test_split(image_list, test_size=0.2, random_state=42)\n",
        "val_list, test_list = train_test_split(test_list, test_size=0.5, random_state=42)\n",
        "print('total =',len(image_list))\n",
        "print('train :',len(train_list))\n",
        "print('val   :',len(val_list))\n",
        "print('test  :',len(test_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total = 4750\n",
            "train : 3800\n",
            "val   : 475\n",
            "test  : 475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKYTgleLNMxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df6c5b4-603c-4c4a-ec73-162078d0de2b"
      },
      "source": [
        "def copy_data(file_list, img_labels_root, imgs_source, mode):\n",
        "\n",
        "    root_file = Path( '/content/Dataset/images/'+  mode)\n",
        "    if not root_file.exists():\n",
        "        print(f\"Path {root_file} does not exit\")\n",
        "        os.makedirs(root_file)\n",
        "\n",
        "    root_file = Path('/content/Dataset/labels/' + mode)\n",
        "    if not root_file.exists():\n",
        "        print(f\"Path {root_file} does not exit\")\n",
        "        os.makedirs(root_file)\n",
        "\n",
        "    for file in file_list:               \n",
        "        img_name = file.replace('.png', '')        \n",
        "        img_src_file = imgs_source + '/' + img_name + '.png'        \n",
        "        label_src_file = img_labels_root + '/' + img_name + '.txt'\n",
        "\n",
        "        #print(img_sor_file)\n",
        "        #print(label_sor_file)\n",
        "        # im = Image.open(rf\"{img_sor_file}\")\n",
        "        # im.show()\n",
        "\n",
        "        # Copy image\n",
        "        DICT_DIR = '/content/Dataset/images/'  + mode\n",
        "        img_dict_file = DICT_DIR + '/' + img_name + '.png'\n",
        "\n",
        "        copyfile(img_src_file, img_dict_file)\n",
        "\n",
        "        # Copy label\n",
        "        DICT_DIR = '/content/Dataset/labels/' + mode\n",
        "        img_dict_file = DICT_DIR + '/' + img_name + '.txt'\n",
        "        copyfile(label_src_file, img_dict_file)\n",
        "copy_data(train_list, '/content/Dataset/labels', '/content/images', \"train\")\n",
        "copy_data(val_list,   '/content/Dataset/labels', '/content/images', \"val\")\n",
        "copy_data(test_list,  '/content/Dataset/labels', '/content/images', \"test\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Path /content/Dataset/images/train does not exit\n",
            "Path /content/Dataset/labels/train does not exit\n",
            "Path /content/Dataset/images/val does not exit\n",
            "Path /content/Dataset/labels/val does not exit\n",
            "Path /content/Dataset/images/test does not exit\n",
            "Path /content/Dataset/labels/test does not exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3moy_plGNkcO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "707053d5-75c0-43f6-cef0-aee71b2f6025"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 8149, done.\u001b[K\n",
            "remote: Counting objects: 100% (461/461), done.\u001b[K\n",
            "remote: Compressing objects: 100% (206/206), done.\u001b[K\n",
            "remote: Total 8149 (delta 317), reused 373 (delta 255), pack-reused 7688\u001b[K\n",
            "Receiving objects: 100% (8149/8149), 9.42 MiB | 20.30 MiB/s, done.\n",
            "Resolving deltas: 100% (5615/5615), done.\n",
            "/content/yolov5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_reTVRlPNqQj"
      },
      "source": [
        "import yaml\n",
        "\n",
        "dict_file = {'train':'/content/Dataset/images/train' ,\n",
        "            'val': '/content/Dataset/images/val',\n",
        "            'nc' : '1',\n",
        "            'names' : ['helmet']}\n",
        "\n",
        "with open('/content/yolov5/data/hard_head.yaml', 'w+') as file:\n",
        "    documents = yaml.dump(dict_file, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKfH8iIyOeBZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e7f7fc7-8343-4ef8-8701-3ee3470deddb"
      },
      "source": [
        "!pip install -r /content/yolov5/requirements.txt "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/yolov5/requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/yolov5/requirements.txt (line 5)) (1.19.5)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/yolov5/requirements.txt (line 6)) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from -r /content/yolov5/requirements.txt (line 7)) (7.1.2)\n",
            "Collecting PyYAML>=5.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/yolov5/requirements.txt (line 9)) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/yolov5/requirements.txt (line 10)) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/yolov5/requirements.txt (line 11)) (0.10.0+cu102)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/yolov5/requirements.txt (line 12)) (4.41.1)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/yolov5/requirements.txt (line 15)) (2.5.0)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/yolov5/requirements.txt (line 19)) (0.11.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r /content/yolov5/requirements.txt (line 20)) (1.1.5)\n",
            "Collecting thop\n",
            "  Downloading https://files.pythonhosted.org/packages/6c/8b/22ce44e1c71558161a8bd54471123cc796589c7ebbfc15a7e8932e522f83/thop-0.0.31.post2005241907-py3-none-any.whl\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r /content/yolov5/requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r /content/yolov5/requirements.txt (line 4)) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r /content/yolov5/requirements.txt (line 4)) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r /content/yolov5/requirements.txt (line 4)) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->-r /content/yolov5/requirements.txt (line 10)) (3.7.4.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (1.32.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (3.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (0.4.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (1.34.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (1.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (0.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (57.0.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (0.36.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r /content/yolov5/requirements.txt (line 20)) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.2->-r /content/yolov5/requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (0.2.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (4.6.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.4.1->-r /content/yolov5/requirements.txt (line 15)) (3.5.0)\n",
            "Installing collected packages: PyYAML, thop\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1 thop-0.0.31.post2005241907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0zR80zTS_pH",
        "outputId": "e1e49a13-1963-4d44-88f3-a2be82196f6a"
      },
      "source": [
        "%%writefile train.py\n",
        "\"\"\"Train a YOLOv5 model on a custom dataset\n",
        "\n",
        "Usage:\n",
        "    $ python path/to/train.py --data coco128.yaml --weights yolov5s.pt --img 640\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "from copy import deepcopy\n",
        "from pathlib import Path\n",
        "from threading import Thread\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import torch.utils.data\n",
        "import yaml\n",
        "from torch.cuda import amp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "FILE = Path(__file__).absolute()\n",
        "sys.path.append(FILE.parents[0].as_posix())  # add yolov5/ to path\n",
        "\n",
        "import val  # for end-of-epoch mAP\n",
        "from models.experimental import attempt_load\n",
        "from models.yolo import Model\n",
        "from utils.autoanchor import check_anchors\n",
        "from utils.datasets import create_dataloader\n",
        "from utils.general import labels_to_class_weights, increment_path, labels_to_image_weights, init_seeds, \\\n",
        "    strip_optimizer, get_latest_run, check_dataset, check_file, check_git_status, check_img_size, \\\n",
        "    check_requirements, print_mutation, set_logging, one_cycle, colorstr\n",
        "from utils.google_utils import attempt_download\n",
        "from utils.loss import ComputeLoss\n",
        "from utils.plots import plot_images, plot_labels, plot_results, plot_evolution\n",
        "from utils.torch_utils import ModelEMA, select_device, intersect_dicts, torch_distributed_zero_first, de_parallel\n",
        "from utils.wandb_logging.wandb_utils import WandbLogger, check_wandb_resume\n",
        "from utils.metrics import fitness\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html\n",
        "RANK = int(os.getenv('RANK', -1))\n",
        "WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))\n",
        "\n",
        "\n",
        "def train(hyp,  # path/to/hyp.yaml or hyp dictionary\n",
        "          opt,\n",
        "          device,\n",
        "          ):\n",
        "    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, = \\\n",
        "        opt.save_dir, opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \\\n",
        "        opt.resume, opt.noval, opt.nosave, opt.workers\n",
        "\n",
        "    # Directories\n",
        "    save_dir = Path(save_dir)\n",
        "    wdir = save_dir / 'weights'\n",
        "    wdir.mkdir(parents=True, exist_ok=True)  # make dir\n",
        "    last = wdir / 'last.pt'\n",
        "    best = wdir / 'best.pt'\n",
        "    results_file = save_dir / 'results.txt'\n",
        "\n",
        "    # Hyperparameters\n",
        "    if isinstance(hyp, str):\n",
        "        with open(hyp) as f:\n",
        "            hyp = yaml.safe_load(f)  # load hyps dict\n",
        "    logger.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))\n",
        "\n",
        "    # Save run settings\n",
        "    with open(save_dir / 'hyp.yaml', 'w') as f:\n",
        "        yaml.safe_dump(hyp, f, sort_keys=False)\n",
        "    with open(save_dir / 'opt.yaml', 'w') as f:\n",
        "        yaml.safe_dump(vars(opt), f, sort_keys=False)\n",
        "\n",
        "    # Configure\n",
        "    plots = not evolve  # create plots\n",
        "    cuda = device.type != 'cpu'\n",
        "    init_seeds(1 + RANK)\n",
        "    with open(data) as f:\n",
        "        data_dict = yaml.safe_load(f)  # data dict\n",
        "\n",
        "    # Loggers\n",
        "    loggers = {'wandb': None, 'tb': None}  # loggers dict\n",
        "    if RANK in [-1, 0]:\n",
        "        # TensorBoard\n",
        "        if not evolve:\n",
        "            prefix = colorstr('tensorboard: ')\n",
        "            logger.info(f\"{prefix}Start with 'tensorboard --logdir {opt.project}', view at http://localhost:6006/\")\n",
        "            loggers['tb'] = SummaryWriter(str(save_dir))\n",
        "\n",
        "        # W&B\n",
        "        opt.hyp = hyp  # add hyperparameters\n",
        "        run_id = torch.load(weights).get('wandb_id') if weights.endswith('.pt') and os.path.isfile(weights) else None\n",
        "        run_id = run_id if opt.resume else None  # start fresh run if transfer learning\n",
        "        wandb_logger = WandbLogger(opt, save_dir.stem, run_id, data_dict)\n",
        "        loggers['wandb'] = wandb_logger.wandb\n",
        "        if loggers['wandb']:\n",
        "            data_dict = wandb_logger.data_dict\n",
        "            weights, epochs, hyp = opt.weights, opt.epochs, opt.hyp  # may update weights, epochs if resuming\n",
        "\n",
        "    nc = 1 if single_cls else int(data_dict['nc'])  # number of classes\n",
        "    names = ['item'] if single_cls and len(data_dict['names']) != 1 else data_dict['names']  # class names\n",
        "    assert len(names) == nc, '%g names found for nc=%g dataset in %s' % (len(names), nc, data)  # check\n",
        "    is_coco = data.endswith('coco.yaml') and nc == 80  # COCO dataset\n",
        "\n",
        "    # Model\n",
        "    pretrained = weights.endswith('.pt')\n",
        "    if pretrained:\n",
        "        with torch_distributed_zero_first(RANK):\n",
        "            weights = attempt_download(weights)  # download if not found locally\n",
        "        ckpt = torch.load(weights, map_location=device)  # load checkpoint\n",
        "        model = Model(cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\n",
        "        exclude = ['anchor'] if (cfg or hyp.get('anchors')) and not resume else []  # exclude keys\n",
        "        state_dict = ckpt['model'].float().state_dict()  # to FP32\n",
        "        state_dict = intersect_dicts(state_dict, model.state_dict(), exclude=exclude)  # intersect\n",
        "        model.load_state_dict(state_dict, strict=False)  # load\n",
        "        logger.info('Transferred %g/%g items from %s' % (len(state_dict), len(model.state_dict()), weights))  # report\n",
        "    else:\n",
        "        model = Model(cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\n",
        "    with torch_distributed_zero_first(RANK):\n",
        "        check_dataset(data_dict)  # check\n",
        "    train_path = data_dict['train']\n",
        "    val_path = data_dict['val']\n",
        "\n",
        "    # Freeze\n",
        "    freeze = []  # parameter names to freeze (full or partial)\n",
        "    for k, v in model.named_parameters():\n",
        "        v.requires_grad = True  # train all layers\n",
        "        if any(x in k for x in freeze):\n",
        "            print('freezing %s' % k)\n",
        "            v.requires_grad = False\n",
        "\n",
        "    # Optimizer\n",
        "    nbs = 64  # nominal batch size\n",
        "    accumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizing\n",
        "    hyp['weight_decay'] *= batch_size * accumulate / nbs  # scale weight_decay\n",
        "    logger.info(f\"Scaled weight_decay = {hyp['weight_decay']}\")\n",
        "\n",
        "    pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n",
        "    for k, v in model.named_modules():\n",
        "        if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):\n",
        "            pg2.append(v.bias)  # biases\n",
        "        if isinstance(v, nn.BatchNorm2d):\n",
        "            pg0.append(v.weight)  # no decay\n",
        "        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):\n",
        "            pg1.append(v.weight)  # apply decay\n",
        "\n",
        "    if opt.adam:\n",
        "        optimizer = optim.Adam(pg0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum\n",
        "    else:\n",
        "        optimizer = optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n",
        "\n",
        "    optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay\n",
        "    optimizer.add_param_group({'params': pg2})  # add pg2 (biases)\n",
        "    logger.info('Optimizer groups: %g .bias, %g conv.weight, %g other' % (len(pg2), len(pg1), len(pg0)))\n",
        "    del pg0, pg1, pg2\n",
        "\n",
        "    # Scheduler https://arxiv.org/pdf/1812.01187.pdf\n",
        "    # https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#OneCycleLR\n",
        "    if opt.linear_lr:\n",
        "        lf = lambda x: (1 - x / (epochs - 1)) * (1.0 - hyp['lrf']) + hyp['lrf']  # linear\n",
        "    else:\n",
        "        lf = one_cycle(1, hyp['lrf'], epochs)  # cosine 1->hyp['lrf']\n",
        "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n",
        "    # plot_lr_scheduler(optimizer, scheduler, epochs)\n",
        "\n",
        "    # EMA\n",
        "    ema = ModelEMA(model) if RANK in [-1, 0] else None\n",
        "\n",
        "    # Resume\n",
        "    start_epoch, best_fitness = 0, 0.0\n",
        "    if pretrained:\n",
        "        # Optimizer\n",
        "        if ckpt['optimizer'] is not None:\n",
        "            optimizer.load_state_dict(ckpt['optimizer'])\n",
        "            best_fitness = ckpt['best_fitness']\n",
        "\n",
        "        # EMA\n",
        "        if ema and ckpt.get('ema'):\n",
        "            ema.ema.load_state_dict(ckpt['ema'].float().state_dict())\n",
        "            ema.updates = ckpt['updates']\n",
        "\n",
        "        # Results\n",
        "        if ckpt.get('training_results') is not None:\n",
        "            results_file.write_text(ckpt['training_results'])  # write results.txt\n",
        "\n",
        "        # Epochs\n",
        "        start_epoch = ckpt['epoch'] + 1\n",
        "        if resume:\n",
        "            assert start_epoch > 0, '%s training to %g epochs is finished, nothing to resume.' % (weights, epochs)\n",
        "        if epochs < start_epoch:\n",
        "            logger.info('%s has been trained for %g epochs. Fine-tuning for %g additional epochs.' %\n",
        "                        (weights, ckpt['epoch'], epochs))\n",
        "            epochs += ckpt['epoch']  # finetune additional epochs\n",
        "\n",
        "        del ckpt, state_dict\n",
        "\n",
        "    # Image sizes\n",
        "    gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
        "    nl = model.model[-1].nl  # number of detection layers (used for scaling hyp['obj'])\n",
        "    imgsz, imgsz_val = [check_img_size(x, gs) for x in opt.img_size]  # verify imgsz are gs-multiples\n",
        "\n",
        "    # DP mode\n",
        "    if cuda and RANK == -1 and torch.cuda.device_count() > 1:\n",
        "        logging.warning('DP not recommended, instead use torch.distributed.run for best DDP Multi-GPU results.\\n'\n",
        "                        'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # SyncBatchNorm\n",
        "    if opt.sync_bn and cuda and RANK != -1:\n",
        "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)\n",
        "        logger.info('Using SyncBatchNorm()')\n",
        "\n",
        "    # Trainloader\n",
        "    dataloader, dataset = create_dataloader(train_path, imgsz, batch_size // WORLD_SIZE, gs, single_cls,\n",
        "                                            hyp=hyp, augment=True, cache=opt.cache_images, rect=opt.rect, rank=RANK,\n",
        "                                            workers=workers,\n",
        "                                            image_weights=opt.image_weights, quad=opt.quad, prefix=colorstr('train: '))\n",
        "    mlc = np.concatenate(dataset.labels, 0)[:, 0].max()  # max label class\n",
        "    nb = len(dataloader)  # number of batches\n",
        "    assert mlc < nc, 'Label class %g exceeds nc=%g in %s. Possible class labels are 0-%g' % (mlc, nc, data, nc - 1)\n",
        "\n",
        "    # Process 0\n",
        "    if RANK in [-1, 0]:\n",
        "        valloader = create_dataloader(val_path, imgsz_val, batch_size // WORLD_SIZE * 2, gs, single_cls,\n",
        "                                       hyp=hyp, cache=opt.cache_images and not noval, rect=True, rank=-1,\n",
        "                                       workers=workers,\n",
        "                                       pad=0.5, prefix=colorstr('val: '))[0]\n",
        "\n",
        "        if not resume:\n",
        "            labels = np.concatenate(dataset.labels, 0)\n",
        "            c = torch.tensor(labels[:, 0])  # classes\n",
        "            # cf = torch.bincount(c.long(), minlength=nc) + 1.  # frequency\n",
        "            # model._initialize_biases(cf.to(device))\n",
        "            if plots:\n",
        "                plot_labels(labels, names, save_dir, loggers)\n",
        "                if loggers['tb']:\n",
        "                    loggers['tb'].add_histogram('classes', c, 0)  # TensorBoard\n",
        "\n",
        "            # Anchors\n",
        "            if not opt.noautoanchor:\n",
        "                check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\n",
        "            model.half().float()  # pre-reduce anchor precision\n",
        "\n",
        "    # DDP mode\n",
        "    if cuda and RANK != -1:\n",
        "        model = DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK)\n",
        "\n",
        "    # Model parameters\n",
        "    hyp['box'] *= 3. / nl  # scale to layers\n",
        "    hyp['cls'] *= nc / 80. * 3. / nl  # scale to classes and layers\n",
        "    hyp['obj'] *= (imgsz / 640) ** 2 * 3. / nl  # scale to image size and layers\n",
        "    hyp['label_smoothing'] = opt.label_smoothing\n",
        "    model.nc = nc  # attach number of classes to model\n",
        "    model.hyp = hyp  # attach hyperparameters to model\n",
        "    model.gr = 1.0  # iou loss ratio (obj_loss = 1.0 or iou)\n",
        "    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  # attach class weights\n",
        "    model.names = names\n",
        "\n",
        "    # Start training\n",
        "    t0 = time.time()\n",
        "    nw = max(round(hyp['warmup_epochs'] * nb), 1000)  # number of warmup iterations, max(3 epochs, 1k iterations)\n",
        "    # nw = min(nw, (epochs - start_epoch) / 2 * nb)  # limit warmup to < 1/2 of training\n",
        "    last_opt_step = -1\n",
        "    maps = np.zeros(nc)  # mAP per class\n",
        "    results = (0, 0, 0, 0, 0, 0, 0)  # P, R, mAP@.5, mAP@.5-.95, val_loss(box, obj, cls)\n",
        "    scheduler.last_epoch = start_epoch - 1  # do not move\n",
        "    scaler = amp.GradScaler(enabled=cuda)\n",
        "    compute_loss = ComputeLoss(model)  # init loss class\n",
        "    logger.info(f'Image sizes {imgsz} train, {imgsz_val} val\\n'\n",
        "                f'Using {dataloader.num_workers} dataloader workers\\n'\n",
        "                f'Logging results to {save_dir}\\n'\n",
        "                f'Starting training for {epochs} epochs...')\n",
        "    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\n",
        "        model.train()\n",
        "\n",
        "        # Update image weights (optional)\n",
        "        if opt.image_weights:\n",
        "            # Generate indices\n",
        "            if RANK in [-1, 0]:\n",
        "                cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  # class weights\n",
        "                iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  # image weights\n",
        "                dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  # rand weighted idx\n",
        "            # Broadcast if DDP\n",
        "            if RANK != -1:\n",
        "                indices = (torch.tensor(dataset.indices) if RANK == 0 else torch.zeros(dataset.n)).int()\n",
        "                dist.broadcast(indices, 0)\n",
        "                if RANK != 0:\n",
        "                    dataset.indices = indices.cpu().numpy()\n",
        "\n",
        "        # Update mosaic border\n",
        "        # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)\n",
        "        # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders\n",
        "\n",
        "        mloss = torch.zeros(4, device=device)  # mean losses\n",
        "        if RANK != -1:\n",
        "            dataloader.sampler.set_epoch(epoch)\n",
        "        pbar = enumerate(dataloader)\n",
        "        logger.info(('\\n' + '%10s' * 8) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'total', 'labels', 'img_size'))\n",
        "        if RANK in [-1, 0]:\n",
        "            pbar = tqdm(pbar, total=nb)  # progress bar\n",
        "        optimizer.zero_grad()\n",
        "        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n",
        "            ni = i + nb * epoch  # number integrated batches (since train start)\n",
        "            imgs = imgs.to(device, non_blocking=True).float() / 255.0  # uint8 to float32, 0-255 to 0.0-1.0\n",
        "\n",
        "            # Warmup\n",
        "            if ni <= nw:\n",
        "                xi = [0, nw]  # x interp\n",
        "                # model.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)\n",
        "                accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())\n",
        "                for j, x in enumerate(optimizer.param_groups):\n",
        "                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n",
        "                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n",
        "                    if 'momentum' in x:\n",
        "                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])\n",
        "\n",
        "            # Multi-scale\n",
        "            if opt.multi_scale:\n",
        "                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\n",
        "                sf = sz / max(imgs.shape[2:])  # scale factor\n",
        "                if sf != 1:\n",
        "                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)\n",
        "                    imgs = F.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\n",
        "\n",
        "            # Forward\n",
        "            with amp.autocast(enabled=cuda):\n",
        "                pred = model(imgs)  # forward\n",
        "                loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size\n",
        "                if RANK != -1:\n",
        "                    loss *= WORLD_SIZE  # gradient averaged between devices in DDP mode\n",
        "                if opt.quad:\n",
        "                    loss *= 4.\n",
        "\n",
        "            # Backward\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Optimize\n",
        "            if ni - last_opt_step >= accumulate:\n",
        "                scaler.step(optimizer)  # optimizer.step\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "                if ema:\n",
        "                    ema.update(model)\n",
        "                last_opt_step = ni\n",
        "\n",
        "            # Print\n",
        "            if RANK in [-1, 0]:\n",
        "                mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\n",
        "                mem = '%.3gG' % (torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0)  # (GB)\n",
        "                s = ('%10s' * 2 + '%10.4g' * 6) % (\n",
        "                    f'{epoch}/{epochs - 1}', mem, *mloss, targets.shape[0], imgs.shape[-1])\n",
        "                pbar.set_description(s)\n",
        "\n",
        "                # Plot\n",
        "                if plots and ni < 3:\n",
        "                    f = save_dir / f'train_batch{ni}.jpg'  # filename\n",
        "                    Thread(target=plot_images, args=(imgs, targets, paths, f), daemon=True).start()\n",
        "                    if loggers['tb'] and ni == 0:  # TensorBoard\n",
        "                        with warnings.catch_warnings():\n",
        "                            warnings.simplefilter('ignore')  # suppress jit trace warning\n",
        "                            loggers['tb'].add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), [])\n",
        "                elif plots and ni == 10 and loggers['wandb']:\n",
        "                    wandb_logger.log({'Mosaics': [loggers['wandb'].Image(str(x), caption=x.name) for x in\n",
        "                                                  save_dir.glob('train*.jpg') if x.exists()]})\n",
        "\n",
        "            # end batch ------------------------------------------------------------------------------------------------\n",
        "\n",
        "        # Scheduler\n",
        "        lr = [x['lr'] for x in optimizer.param_groups]  # for loggers\n",
        "        scheduler.step()\n",
        "\n",
        "        # DDP process 0 or single-GPU\n",
        "        if RANK in [-1, 0]:\n",
        "            # mAP\n",
        "            ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'gr', 'names', 'stride', 'class_weights'])\n",
        "            final_epoch = epoch + 1 == epochs\n",
        "            if not noval or final_epoch:  # Calculate mAP\n",
        "                wandb_logger.current_epoch = epoch + 1\n",
        "                results, maps, _ = val.run(data_dict,\n",
        "                                           batch_size=batch_size // WORLD_SIZE * 2,\n",
        "                                           imgsz=imgsz_val,\n",
        "                                           model=ema.ema,\n",
        "                                           single_cls=single_cls,\n",
        "                                           dataloader=valloader,\n",
        "                                           save_dir=save_dir,\n",
        "                                           save_json=is_coco and final_epoch,\n",
        "                                           verbose=nc < 50 and final_epoch,\n",
        "                                           plots=plots and final_epoch,\n",
        "                                           wandb_logger=wandb_logger,\n",
        "                                           compute_loss=compute_loss)\n",
        "\n",
        "            # Write\n",
        "            with open(results_file, 'a') as f:\n",
        "                f.write(s + '%10.4g' * 7 % results + '\\n')  # append metrics, val_loss\n",
        "\n",
        "            # Log\n",
        "            tags = ['train/box_loss', 'train/obj_loss', 'train/cls_loss',  # train loss\n",
        "                    'metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/mAP_0.5:0.95',\n",
        "                    'val/box_loss', 'val/obj_loss', 'val/cls_loss',  # val loss\n",
        "                    'x/lr0', 'x/lr1', 'x/lr2']  # params\n",
        "            for x, tag in zip(list(mloss[:-1]) + list(results) + lr, tags):\n",
        "                if loggers['tb']:\n",
        "                    loggers['tb'].add_scalar(tag, x, epoch)  # TensorBoard\n",
        "                if loggers['wandb']:\n",
        "                    wandb_logger.log({tag: x})  # W&B\n",
        "\n",
        "            # Update best mAP\n",
        "            fi = fitness(np.array(results).reshape(1, -1))  # weighted combination of [P, R, mAP@.5, mAP@.5-.95]\n",
        "            if fi > best_fitness:\n",
        "                best_fitness = fi\n",
        "            wandb_logger.end_epoch(best_result=best_fitness == fi)\n",
        "\n",
        "            # Save model\n",
        "            if (not nosave) or (final_epoch and not evolve):  # if save\n",
        "                ckpt = {'epoch': epoch,\n",
        "                        'best_fitness': best_fitness,\n",
        "                        'training_results': results_file.read_text(),\n",
        "                        'model': deepcopy(de_parallel(model)).half(),\n",
        "                        'ema': deepcopy(ema.ema).half(),\n",
        "                        'updates': ema.updates,\n",
        "                        'optimizer': optimizer.state_dict(),\n",
        "                        'wandb_id': wandb_logger.wandb_run.id if loggers['wandb'] else None}\n",
        "\n",
        "                # Save last, best and delete\n",
        "                torch.save(ckpt, '/content/gdrive/MyDrive/last.pt')\n",
        "                if best_fitness == fi:\n",
        "                    torch.save(ckpt, '/content/gdrive/MyDrive/best.pt')\n",
        "                if loggers['wandb']:\n",
        "                    if ((epoch + 1) % opt.save_period == 0 and not final_epoch) and opt.save_period != -1:\n",
        "                        wandb_logger.log_model(last.parent, opt, epoch, fi, best_model=best_fitness == fi)\n",
        "                del ckpt\n",
        "\n",
        "        # end epoch ----------------------------------------------------------------------------------------------------\n",
        "    # end training -----------------------------------------------------------------------------------------------------\n",
        "    if RANK in [-1, 0]:\n",
        "        logger.info(f'{epoch - start_epoch + 1} epochs completed in {(time.time() - t0) / 3600:.3f} hours.\\n')\n",
        "        if plots:\n",
        "            plot_results(save_dir=save_dir)  # save as results.png\n",
        "            if loggers['wandb']:\n",
        "                files = ['results.png', 'confusion_matrix.png', *[f'{x}_curve.png' for x in ('F1', 'PR', 'P', 'R')]]\n",
        "                wandb_logger.log({\"Results\": [loggers['wandb'].Image(str(save_dir / f), caption=f) for f in files\n",
        "                                              if (save_dir / f).exists()]})\n",
        "\n",
        "        if not evolve:\n",
        "            if is_coco:  # COCO dataset\n",
        "                for m in [last, best] if best.exists() else [last]:  # speed, mAP tests\n",
        "                    results, _, _ = val.run(data_dict,\n",
        "                                            batch_size=batch_size // WORLD_SIZE * 2,\n",
        "                                            imgsz=imgsz_val,\n",
        "                                            model=attempt_load(m, device).half(),\n",
        "                                            single_cls=single_cls,\n",
        "                                            dataloader=valloader,\n",
        "                                            save_dir=save_dir,\n",
        "                                            save_json=True,\n",
        "                                            plots=False)\n",
        "\n",
        "            # Strip optimizers\n",
        "            for f in last, best:\n",
        "                if f.exists():\n",
        "                    strip_optimizer(f)  # strip optimizers\n",
        "            if loggers['wandb']:  # Log the stripped model\n",
        "                loggers['wandb'].log_artifact(str(best if best.exists() else last), type='model',\n",
        "                                              name='run_' + wandb_logger.wandb_run.id + '_model',\n",
        "                                              aliases=['latest', 'best', 'stripped'])\n",
        "        wandb_logger.finish_run()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    return results\n",
        "\n",
        "\n",
        "def parse_opt(known=False):\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--weights', type=str, default='yolov5s.pt', help='initial weights path')\n",
        "    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')\n",
        "    parser.add_argument('--data', type=str, default='data/coco128.yaml', help='dataset.yaml path')\n",
        "    parser.add_argument('--hyp', type=str, default='data/hyps/hyp.scratch.yaml', help='hyperparameters path')\n",
        "    parser.add_argument('--epochs', type=int, default=300)\n",
        "    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs')\n",
        "    parser.add_argument('--img-size', nargs='+', type=int, default=[640, 640], help='[train, val] image sizes')\n",
        "    parser.add_argument('--rect', action='store_true', help='rectangular training')\n",
        "    parser.add_argument('--resume', nargs='?', const=True, default=False, help='resume most recent training')\n",
        "    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')\n",
        "    parser.add_argument('--noval', action='store_true', help='only validate final epoch')\n",
        "    parser.add_argument('--noautoanchor', action='store_true', help='disable autoanchor check')\n",
        "    parser.add_argument('--evolve', type=int, nargs='?', const=300, help='evolve hyperparameters for x generations')\n",
        "    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')\n",
        "    parser.add_argument('--cache-images', action='store_true', help='cache images for faster training')\n",
        "    parser.add_argument('--image-weights', action='store_true', help='use weighted image selection for training')\n",
        "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')\n",
        "    parser.add_argument('--single-cls', action='store_true', help='train multi-class data as single-class')\n",
        "    parser.add_argument('--adam', action='store_true', help='use torch.optim.Adam() optimizer')\n",
        "    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')\n",
        "    parser.add_argument('--workers', type=int, default=8, help='maximum number of dataloader workers')\n",
        "    parser.add_argument('--project', default='runs/train', help='save to project/name')\n",
        "    parser.add_argument('--entity', default=None, help='W&B entity')\n",
        "    parser.add_argument('--name', default='exp', help='save to project/name')\n",
        "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
        "    parser.add_argument('--quad', action='store_true', help='quad dataloader')\n",
        "    parser.add_argument('--linear-lr', action='store_true', help='linear LR')\n",
        "    parser.add_argument('--label-smoothing', type=float, default=0.0, help='Label smoothing epsilon')\n",
        "    parser.add_argument('--upload_dataset', action='store_true', help='Upload dataset as W&B artifact table')\n",
        "    parser.add_argument('--bbox_interval', type=int, default=-1, help='Set bounding-box image logging interval for W&B')\n",
        "    parser.add_argument('--save_period', type=int, default=-1, help='Log model after every \"save_period\" epoch')\n",
        "    parser.add_argument('--artifact_alias', type=str, default=\"latest\", help='version of dataset artifact to be used')\n",
        "    parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')\n",
        "    opt = parser.parse_known_args()[0] if known else parser.parse_args()\n",
        "    return opt\n",
        "\n",
        "\n",
        "def main(opt):\n",
        "    set_logging(RANK)\n",
        "    if RANK in [-1, 0]:\n",
        "        print(colorstr('train: ') + ', '.join(f'{k}={v}' for k, v in vars(opt).items()))\n",
        "        check_git_status()\n",
        "        check_requirements(exclude=['thop'])\n",
        "\n",
        "    # Resume\n",
        "    wandb_run = check_wandb_resume(opt)\n",
        "    if opt.resume and not wandb_run:  # resume an interrupted run\n",
        "        ckpt = opt.resume if isinstance(opt.resume, str) else get_latest_run()  # specified or most recent path\n",
        "        assert os.path.isfile(ckpt), 'ERROR: --resume checkpoint does not exist'\n",
        "        with open(Path(ckpt).parent.parent / 'opt.yaml') as f:\n",
        "            opt = argparse.Namespace(**yaml.safe_load(f))  # replace\n",
        "        opt.cfg, opt.weights, opt.resume = '', ckpt, True  # reinstate\n",
        "        logger.info('Resuming training from %s' % ckpt)\n",
        "    else:\n",
        "        # opt.hyp = opt.hyp or ('hyp.finetune.yaml' if opt.weights else 'hyp.scratch.yaml')\n",
        "        opt.data, opt.cfg, opt.hyp = check_file(opt.data), check_file(opt.cfg), check_file(opt.hyp)  # check files\n",
        "        assert len(opt.cfg) or len(opt.weights), 'either --cfg or --weights must be specified'\n",
        "        opt.img_size.extend([opt.img_size[-1]] * (2 - len(opt.img_size)))  # extend to 2 sizes (train, val)\n",
        "        opt.name = 'evolve' if opt.evolve else opt.name\n",
        "        opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok or opt.evolve))\n",
        "\n",
        "    # DDP mode\n",
        "    device = select_device(opt.device, batch_size=opt.batch_size)\n",
        "    if LOCAL_RANK != -1:\n",
        "        from datetime import timedelta\n",
        "        assert torch.cuda.device_count() > LOCAL_RANK, 'insufficient CUDA devices for DDP command'\n",
        "        torch.cuda.set_device(LOCAL_RANK)\n",
        "        device = torch.device('cuda', LOCAL_RANK)\n",
        "        dist.init_process_group(backend=\"nccl\" if dist.is_nccl_available() else \"gloo\", timeout=timedelta(seconds=60))\n",
        "        assert opt.batch_size % WORLD_SIZE == 0, '--batch-size must be multiple of CUDA device count'\n",
        "        assert not opt.image_weights, '--image-weights argument is not compatible with DDP training'\n",
        "\n",
        "    # Train\n",
        "    if not opt.evolve:\n",
        "        train(opt.hyp, opt, device)\n",
        "        if WORLD_SIZE > 1 and RANK == 0:\n",
        "            _ = [print('Destroying process group... ', end=''), dist.destroy_process_group(), print('Done.')]\n",
        "\n",
        "    # Evolve hyperparameters (optional)\n",
        "    else:\n",
        "        # Hyperparameter evolution metadata (mutation scale 0-1, lower_limit, upper_limit)\n",
        "        meta = {'lr0': (1, 1e-5, 1e-1),  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
        "                'lrf': (1, 0.01, 1.0),  # final OneCycleLR learning rate (lr0 * lrf)\n",
        "                'momentum': (0.3, 0.6, 0.98),  # SGD momentum/Adam beta1\n",
        "                'weight_decay': (1, 0.0, 0.001),  # optimizer weight decay\n",
        "                'warmup_epochs': (1, 0.0, 5.0),  # warmup epochs (fractions ok)\n",
        "                'warmup_momentum': (1, 0.0, 0.95),  # warmup initial momentum\n",
        "                'warmup_bias_lr': (1, 0.0, 0.2),  # warmup initial bias lr\n",
        "                'box': (1, 0.02, 0.2),  # box loss gain\n",
        "                'cls': (1, 0.2, 4.0),  # cls loss gain\n",
        "                'cls_pw': (1, 0.5, 2.0),  # cls BCELoss positive_weight\n",
        "                'obj': (1, 0.2, 4.0),  # obj loss gain (scale with pixels)\n",
        "                'obj_pw': (1, 0.5, 2.0),  # obj BCELoss positive_weight\n",
        "                'iou_t': (0, 0.1, 0.7),  # IoU training threshold\n",
        "                'anchor_t': (1, 2.0, 8.0),  # anchor-multiple threshold\n",
        "                'anchors': (2, 2.0, 10.0),  # anchors per output grid (0 to ignore)\n",
        "                'fl_gamma': (0, 0.0, 2.0),  # focal loss gamma (efficientDet default gamma=1.5)\n",
        "                'hsv_h': (1, 0.0, 0.1),  # image HSV-Hue augmentation (fraction)\n",
        "                'hsv_s': (1, 0.0, 0.9),  # image HSV-Saturation augmentation (fraction)\n",
        "                'hsv_v': (1, 0.0, 0.9),  # image HSV-Value augmentation (fraction)\n",
        "                'degrees': (1, 0.0, 45.0),  # image rotation (+/- deg)\n",
        "                'translate': (1, 0.0, 0.9),  # image translation (+/- fraction)\n",
        "                'scale': (1, 0.0, 0.9),  # image scale (+/- gain)\n",
        "                'shear': (1, 0.0, 10.0),  # image shear (+/- deg)\n",
        "                'perspective': (0, 0.0, 0.001),  # image perspective (+/- fraction), range 0-0.001\n",
        "                'flipud': (1, 0.0, 1.0),  # image flip up-down (probability)\n",
        "                'fliplr': (0, 0.0, 1.0),  # image flip left-right (probability)\n",
        "                'mosaic': (1, 0.0, 1.0),  # image mixup (probability)\n",
        "                'mixup': (1, 0.0, 1.0),  # image mixup (probability)\n",
        "                'copy_paste': (1, 0.0, 1.0)}  # segment copy-paste (probability)\n",
        "\n",
        "        with open(opt.hyp) as f:\n",
        "            hyp = yaml.safe_load(f)  # load hyps dict\n",
        "            if 'anchors' not in hyp:  # anchors commented in hyp.yaml\n",
        "                hyp['anchors'] = 3\n",
        "        assert LOCAL_RANK == -1, 'DDP mode not implemented for --evolve'\n",
        "        opt.noval, opt.nosave = True, True  # only val/save final epoch\n",
        "        # ei = [isinstance(x, (int, float)) for x in hyp.values()]  # evolvable indices\n",
        "        yaml_file = Path(opt.save_dir) / 'hyp_evolved.yaml'  # save best result here\n",
        "        if opt.bucket:\n",
        "            os.system('gsutil cp gs://%s/evolve.txt .' % opt.bucket)  # download evolve.txt if exists\n",
        "\n",
        "        for _ in range(opt.evolve):  # generations to evolve\n",
        "            if Path('evolve.txt').exists():  # if evolve.txt exists: select best hyps and mutate\n",
        "                # Select parent(s)\n",
        "                parent = 'single'  # parent selection method: 'single' or 'weighted'\n",
        "                x = np.loadtxt('evolve.txt', ndmin=2)\n",
        "                n = min(5, len(x))  # number of previous results to consider\n",
        "                x = x[np.argsort(-fitness(x))][:n]  # top n mutations\n",
        "                w = fitness(x) - fitness(x).min() + 1E-6  # weights (sum > 0)\n",
        "                if parent == 'single' or len(x) == 1:\n",
        "                    # x = x[random.randint(0, n - 1)]  # random selection\n",
        "                    x = x[random.choices(range(n), weights=w)[0]]  # weighted selection\n",
        "                elif parent == 'weighted':\n",
        "                    x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # weighted combination\n",
        "\n",
        "                # Mutate\n",
        "                mp, s = 0.8, 0.2  # mutation probability, sigma\n",
        "                npr = np.random\n",
        "                npr.seed(int(time.time()))\n",
        "                g = np.array([x[0] for x in meta.values()])  # gains 0-1\n",
        "                ng = len(meta)\n",
        "                v = np.ones(ng)\n",
        "                while all(v == 1):  # mutate until a change occurs (prevent duplicates)\n",
        "                    v = (g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0)\n",
        "                for i, k in enumerate(hyp.keys()):  # plt.hist(v.ravel(), 300)\n",
        "                    hyp[k] = float(x[i + 7] * v[i])  # mutate\n",
        "\n",
        "            # Constrain to limits\n",
        "            for k, v in meta.items():\n",
        "                hyp[k] = max(hyp[k], v[1])  # lower limit\n",
        "                hyp[k] = min(hyp[k], v[2])  # upper limit\n",
        "                hyp[k] = round(hyp[k], 5)  # significant digits\n",
        "\n",
        "            # Train mutation\n",
        "            results = train(hyp.copy(), opt, device)\n",
        "\n",
        "            # Write mutation results\n",
        "            print_mutation(hyp.copy(), results, yaml_file, opt.bucket)\n",
        "\n",
        "        # Plot results\n",
        "        plot_evolution(yaml_file)\n",
        "        print(f'Hyperparameter evolution complete. Best results saved as: {yaml_file}\\n'\n",
        "              f'Command to train a new model with these hyperparameters: $ python train.py --hyp {yaml_file}')\n",
        "\n",
        "\n",
        "def run(**kwargs):\n",
        "    # Usage: import train; train.run(imgsz=320, weights='yolov5m.pt')\n",
        "    opt = parse_opt(True)\n",
        "    for k, v in kwargs.items():\n",
        "        setattr(opt, k, v)\n",
        "    main(opt)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    opt = parse_opt()\n",
        "    main(opt)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-Nxeu5dNunp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05d25dfa-adf8-42a7-a865-e5eb14ab2f26"
      },
      "source": [
        "!python train.py --img 416 --batch 32 --epochs 1 --data data/hard_head.yaml --cfg models/yolov5s.yaml --weights yolov5s.pt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=models/yolov5s.yaml, data=data/hard_head.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=1, batch_size=32, img_size=[416], rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache_images=False, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, entity=None, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias=latest, local_rank=-1\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v5.0-290-g62409ee torch 1.9.0+cu102 CUDA:0 (Tesla T4, 15109.75MB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "2021-07-15 18:25:52.357516: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOv5 logging with 'pip install wandb' (recommended)\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 69.4MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
            "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "Model Summary: 283 layers, 7063542 parameters, 7063542 gradients, 16.4 GFLOPs\n",
            "\n",
            "Transferred 354/362 items from yolov5s.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "Optimizer groups: 62 .bias, 62 conv.weight, 59 other\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.2 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/Dataset/labels/train' images and labels...3800 found, 0 missing, 314 empty, 0 corrupted: 100% 3800/3800 [00:02<00:00, 1443.13it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/Dataset/labels/train.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/Dataset/labels/val' images and labels...475 found, 0 missing, 42 empty, 0 corrupted: 100% 475/475 [00:00<00:00, 702.29it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Dataset/labels/val.cache\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Plotting labels... \n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 5.46, Best Possible Recall (BPR) = 0.9997\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to runs/train/exp\n",
            "Starting training for 1 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "       0/0     2.87G   0.09805   0.03383         0    0.1319       185       416: 100% 119/119 [01:23<00:00,  1.42it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 8/8 [00:08<00:00,  1.10s/it]\n",
            "                 all        475       1742      0.476       0.39      0.376      0.105\n",
            "1 epochs completed in 0.027 hours.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzJcwVrcUCmO",
        "outputId": "b24b7589-f022-4200-bc7b-5fc38260aac1"
      },
      "source": [
        "try:\n",
        "    os.mkdir('test')\n",
        "except:\n",
        "    pass\n",
        "import glob\n",
        "img=glob.glob('/content/images/*')[:10]\n",
        "len(img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt68YPVBT_eK"
      },
      "source": [
        "import shutil as sh\n",
        "for i in img:\n",
        "    sh.copy(i,'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezxAEudqNTPZ"
      },
      "source": [
        "# YOLOv5 general utils\n",
        "\n",
        "import contextlib\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import platform\n",
        "import random\n",
        "import re\n",
        "import signal\n",
        "import time\n",
        "import urllib\n",
        "from itertools import repeat\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from pathlib import Path\n",
        "from subprocess import check_output\n",
        "\n",
        "import cv2\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pkg_resources as pkg\n",
        "import torch\n",
        "import torchvision\n",
        "import yaml\n",
        "\n",
        "from utils.google_utils import gsutil_getsize\n",
        "from utils.metrics import box_iou, fitness\n",
        "from utils.torch_utils import init_torch_seeds\n",
        "\n",
        "# Settings\n",
        "torch.set_printoptions(linewidth=320, precision=5, profile='long')\n",
        "np.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\n",
        "pd.options.display.max_columns = 10\n",
        "cv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\n",
        "os.environ['NUMEXPR_MAX_THREADS'] = str(min(os.cpu_count(), 8))  # NumExpr max threads\n",
        "\n",
        "\n",
        "class timeout(contextlib.ContextDecorator):\n",
        "    # Usage: @timeout(seconds) decorator or 'with timeout(seconds):' context manager\n",
        "    def __init__(self, seconds, *, timeout_msg='', suppress_timeout_errors=True):\n",
        "        self.seconds = int(seconds)\n",
        "        self.timeout_message = timeout_msg\n",
        "        self.suppress = bool(suppress_timeout_errors)\n",
        "\n",
        "    def _timeout_handler(self, signum, frame):\n",
        "        raise TimeoutError(self.timeout_message)\n",
        "\n",
        "    def __enter__(self):\n",
        "        signal.signal(signal.SIGALRM, self._timeout_handler)  # Set handler for SIGALRM\n",
        "        signal.alarm(self.seconds)  # start countdown for SIGALRM to be raised\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        signal.alarm(0)  # Cancel SIGALRM if it's scheduled\n",
        "        if self.suppress and exc_type is TimeoutError:  # Suppress TimeoutError\n",
        "            return True\n",
        "\n",
        "\n",
        "def set_logging(rank=-1, verbose=True):\n",
        "    logging.basicConfig(\n",
        "        format=\"%(message)s\",\n",
        "        level=logging.INFO if (verbose and rank in [-1, 0]) else logging.WARN)\n",
        "\n",
        "\n",
        "def init_seeds(seed=0):\n",
        "    # Initialize random number generator (RNG) seeds\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    init_torch_seeds(seed)\n",
        "\n",
        "\n",
        "def get_latest_run(search_dir='.'):\n",
        "    # Return path to most recent 'last.pt' in /runs (i.e. to --resume from)\n",
        "    last_list = glob.glob(f'{search_dir}/**/last*.pt', recursive=True)\n",
        "    return max(last_list, key=os.path.getctime) if last_list else ''\n",
        "\n",
        "\n",
        "def is_docker():\n",
        "    # Is environment a Docker container?\n",
        "    return Path('/workspace').exists()  # or Path('/.dockerenv').exists()\n",
        "\n",
        "\n",
        "def is_colab():\n",
        "    # Is environment a Google Colab instance?\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        return False\n",
        "\n",
        "\n",
        "def is_pip():\n",
        "    # Is file in a pip package?\n",
        "    return 'site-packages' in Path(__file__).absolute().parts\n",
        "\n",
        "\n",
        "def emojis(str=''):\n",
        "    # Return platform-dependent emoji-safe version of string\n",
        "    return str.encode().decode('ascii', 'ignore') if platform.system() == 'Windows' else str\n",
        "\n",
        "\n",
        "def file_size(file):\n",
        "    # Return file size in MB\n",
        "    return Path(file).stat().st_size / 1e6\n",
        "\n",
        "\n",
        "def check_online():\n",
        "    # Check internet connectivity\n",
        "    import socket\n",
        "    try:\n",
        "        socket.create_connection((\"1.1.1.1\", 443), 5)  # check host accessibility\n",
        "        return True\n",
        "    except OSError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def check_git_status(err_msg=', for updates see https://github.com/ultralytics/yolov5'):\n",
        "    # Recommend 'git pull' if code is out of date\n",
        "    print(colorstr('github: '), end='')\n",
        "    try:\n",
        "        assert Path('.git').exists(), 'skipping check (not a git repository)'\n",
        "        assert not is_docker(), 'skipping check (Docker image)'\n",
        "        assert check_online(), 'skipping check (offline)'\n",
        "\n",
        "        cmd = 'git fetch && git config --get remote.origin.url'\n",
        "        url = check_output(cmd, shell=True, timeout=5).decode().strip().rstrip('.git')  # git fetch\n",
        "        branch = check_output('git rev-parse --abbrev-ref HEAD', shell=True).decode().strip()  # checked out\n",
        "        n = int(check_output(f'git rev-list {branch}..origin/master --count', shell=True))  # commits behind\n",
        "        if n > 0:\n",
        "            s = f\"⚠️ WARNING: code is out of date by {n} commit{'s' * (n > 1)}. \" \\\n",
        "                f\"Use 'git pull' to update or 'git clone {url}' to download latest.\"\n",
        "        else:\n",
        "            s = f'up to date with {url} ✅'\n",
        "        print(emojis(s))  # emoji-safe\n",
        "    except Exception as e:\n",
        "        print(f'{e}{err_msg}')\n",
        "\n",
        "\n",
        "def check_python(minimum='3.6.2'):\n",
        "    # Check current python version vs. required python version\n",
        "    check_version(platform.python_version(), minimum, name='Python ')\n",
        "\n",
        "\n",
        "def check_version(current='0.0.0', minimum='0.0.0', name='version ', pinned=False):\n",
        "    # Check version vs. required version\n",
        "    current, minimum = (pkg.parse_version(x) for x in (current, minimum))\n",
        "    result = (current == minimum) if pinned else (current >= minimum)\n",
        "    assert result, f'{name}{minimum} required by YOLOv5, but {name}{current} is currently installed'\n",
        "\n",
        "\n",
        "def check_requirements(requirements='requirements.txt', exclude=()):\n",
        "    # Check installed dependencies meet requirements (pass *.txt file or list of packages)\n",
        "    prefix = colorstr('red', 'bold', 'requirements:')\n",
        "    check_python()  # check python version\n",
        "    if isinstance(requirements, (str, Path)):  # requirements.txt file\n",
        "        file = Path(requirements)\n",
        "        if not file.exists():\n",
        "            print(f\"{prefix} {file.resolve()} not found, check failed.\")\n",
        "            return\n",
        "        requirements = [f'{x.name}{x.specifier}' for x in pkg.parse_requirements(file.open()) if x.name not in exclude]\n",
        "    else:  # list or tuple of packages\n",
        "        requirements = [x for x in requirements if x not in exclude]\n",
        "\n",
        "    n = 0  # number of packages updates\n",
        "    for r in requirements:\n",
        "        try:\n",
        "            pkg.require(r)\n",
        "        except Exception as e:  # DistributionNotFound or VersionConflict if requirements not met\n",
        "            print(f\"{prefix} {r} not found and is required by YOLOv5, attempting auto-update...\")\n",
        "            try:\n",
        "                assert check_online(), f\"'pip install {r}' skipped (offline)\"\n",
        "                print(check_output(f\"pip install '{r}'\", shell=True).decode())\n",
        "                n += 1\n",
        "            except Exception as e:\n",
        "                print(f'{prefix} {e}')\n",
        "\n",
        "    if n:  # if packages updated\n",
        "        source = file.resolve() if 'file' in locals() else requirements\n",
        "        s = f\"{prefix} {n} package{'s' * (n > 1)} updated per {source}\\n\" \\\n",
        "            f\"{prefix} ⚠️ {colorstr('bold', 'Restart runtime or rerun command for updates to take effect')}\\n\"\n",
        "        print(emojis(s))  # emoji-safe\n",
        "\n",
        "\n",
        "def check_img_size(img_size, s=32):\n",
        "    # Verify img_size is a multiple of stride s\n",
        "    new_size = make_divisible(img_size, int(s))  # ceil gs-multiple\n",
        "    if new_size != img_size:\n",
        "        print('WARNING: --img-size %g must be multiple of max stride %g, updating to %g' % (img_size, s, new_size))\n",
        "    return new_size\n",
        "\n",
        "\n",
        "def check_imshow():\n",
        "    # Check if environment supports image displays\n",
        "    try:\n",
        "        assert not is_docker(), 'cv2.imshow() is disabled in Docker environments'\n",
        "        assert not is_colab(), 'cv2.imshow() is disabled in Google Colab environments'\n",
        "        cv2.imshow('test', np.zeros((1, 1, 3)))\n",
        "        cv2.waitKey(1)\n",
        "        cv2.destroyAllWindows()\n",
        "        cv2.waitKey(1)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f'WARNING: Environment does not support cv2.imshow() or PIL Image.show() image displays\\n{e}')\n",
        "        return False\n",
        "\n",
        "\n",
        "def check_file(file):\n",
        "    # Search/download file (if necessary) and return path\n",
        "    file = str(file)  # convert to str()\n",
        "    if Path(file).is_file() or file == '':  # exists\n",
        "        return file\n",
        "    elif file.startswith(('http:/', 'https:/')):  # download\n",
        "        url = str(Path(file)).replace(':/', '://')  # Pathlib turns :// -> :/\n",
        "        file = Path(urllib.parse.unquote(file)).name.split('?')[0]  # '%2F' to '/', split https://url.com/file.txt?auth\n",
        "        print(f'Downloading {url} to {file}...')\n",
        "        torch.hub.download_url_to_file(url, file)\n",
        "        assert Path(file).exists() and Path(file).stat().st_size > 0, f'File download failed: {url}'  # check\n",
        "        return file\n",
        "    else:  # search\n",
        "        files = glob.glob('./**/' + file, recursive=True)  # find file\n",
        "        assert len(files), f'File not found: {file}'  # assert file was found\n",
        "        assert len(files) == 1, f\"Multiple files match '{file}', specify exact path: {files}\"  # assert unique\n",
        "        return files[0]  # return file\n",
        "\n",
        "\n",
        "def check_dataset(data, autodownload=True):\n",
        "    # Download dataset if not found locally\n",
        "    path = Path(data.get('path', ''))  # optional 'path' field\n",
        "    if path:\n",
        "        for k in 'train', 'val', 'test':\n",
        "            if data.get(k):  # prepend path\n",
        "                data[k] = str(path / data[k]) if isinstance(data[k], str) else [str(path / x) for x in data[k]]\n",
        "\n",
        "    train, val, test, s = [data.get(x) for x in ('train', 'val', 'test', 'download')]\n",
        "    if val:\n",
        "        val = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])]  # val path\n",
        "        if not all(x.exists() for x in val):\n",
        "            print('\\nWARNING: Dataset not found, nonexistent paths: %s' % [str(x) for x in val if not x.exists()])\n",
        "            if s and autodownload:  # download script\n",
        "                if s.startswith('http') and s.endswith('.zip'):  # URL\n",
        "                    f = Path(s).name  # filename\n",
        "                    print(f'Downloading {s} ...')\n",
        "                    torch.hub.download_url_to_file(s, f)\n",
        "                    root = path.parent if 'path' in data else '..'  # unzip directory i.e. '../'\n",
        "                    Path(root).mkdir(parents=True, exist_ok=True)  # create root\n",
        "                    r = os.system(f'unzip -q {f} -d {root} && rm {f}')  # unzip\n",
        "                elif s.startswith('bash '):  # bash script\n",
        "                    print(f'Running {s} ...')\n",
        "                    r = os.system(s)\n",
        "                else:  # python script\n",
        "                    r = exec(s, {'yaml': data})  # return None\n",
        "                print('Dataset autodownload %s\\n' % ('success' if r in (0, None) else 'failure'))  # print result\n",
        "            else:\n",
        "                raise Exception('Dataset not found.')\n",
        "\n",
        "\n",
        "def download(url, dir='.', unzip=True, delete=True, curl=False, threads=1):\n",
        "    # Multi-threaded file download and unzip function\n",
        "    def download_one(url, dir):\n",
        "        # Download 1 file\n",
        "        f = dir / Path(url).name  # filename\n",
        "        if not f.exists():\n",
        "            print(f'Downloading {url} to {f}...')\n",
        "            if curl:\n",
        "                os.system(f\"curl -L '{url}' -o '{f}' --retry 9 -C -\")  # curl download, retry and resume on fail\n",
        "            else:\n",
        "                torch.hub.download_url_to_file(url, f, progress=True)  # torch download\n",
        "        if unzip and f.suffix in ('.zip', '.gz'):\n",
        "            print(f'Unzipping {f}...')\n",
        "            if f.suffix == '.zip':\n",
        "                s = f'unzip -qo {f} -d {dir}'  # unzip -quiet -overwrite\n",
        "            elif f.suffix == '.gz':\n",
        "                s = f'tar xfz {f} --directory {f.parent}'  # unzip\n",
        "            if delete:  # delete zip file after unzip\n",
        "                s += f' && rm {f}'\n",
        "            os.system(s)\n",
        "\n",
        "    dir = Path(dir)\n",
        "    dir.mkdir(parents=True, exist_ok=True)  # make directory\n",
        "    if threads > 1:\n",
        "        pool = ThreadPool(threads)\n",
        "        pool.imap(lambda x: download_one(*x), zip(url, repeat(dir)))  # multi-threaded\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "    else:\n",
        "        for u in tuple(url) if isinstance(url, str) else url:\n",
        "            download_one(u, dir)\n",
        "\n",
        "\n",
        "def make_divisible(x, divisor):\n",
        "    # Returns x evenly divisible by divisor\n",
        "    return math.ceil(x / divisor) * divisor\n",
        "\n",
        "\n",
        "def clean_str(s):\n",
        "    # Cleans a string by replacing special characters with underscore _\n",
        "    return re.sub(pattern=\"[|@#!¡·$€%&()=?¿^*;:,¨´><+]\", repl=\"_\", string=s)\n",
        "\n",
        "\n",
        "def one_cycle(y1=0.0, y2=1.0, steps=100):\n",
        "    # lambda function for sinusoidal ramp from y1 to y2\n",
        "    return lambda x: ((1 - math.cos(x * math.pi / steps)) / 2) * (y2 - y1) + y1\n",
        "\n",
        "\n",
        "def colorstr(*input):\n",
        "    # Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e.  colorstr('blue', 'hello world')\n",
        "    *args, string = input if len(input) > 1 else ('blue', 'bold', input[0])  # color arguments, string\n",
        "    colors = {'black': '\\033[30m',  # basic colors\n",
        "              'red': '\\033[31m',\n",
        "              'green': '\\033[32m',\n",
        "              'yellow': '\\033[33m',\n",
        "              'blue': '\\033[34m',\n",
        "              'magenta': '\\033[35m',\n",
        "              'cyan': '\\033[36m',\n",
        "              'white': '\\033[37m',\n",
        "              'bright_black': '\\033[90m',  # bright colors\n",
        "              'bright_red': '\\033[91m',\n",
        "              'bright_green': '\\033[92m',\n",
        "              'bright_yellow': '\\033[93m',\n",
        "              'bright_blue': '\\033[94m',\n",
        "              'bright_magenta': '\\033[95m',\n",
        "              'bright_cyan': '\\033[96m',\n",
        "              'bright_white': '\\033[97m',\n",
        "              'end': '\\033[0m',  # misc\n",
        "              'bold': '\\033[1m',\n",
        "              'underline': '\\033[4m'}\n",
        "    return ''.join(colors[x] for x in args) + f'{string}' + colors['end']\n",
        "\n",
        "\n",
        "def labels_to_class_weights(labels, nc=80):\n",
        "    # Get class weights (inverse frequency) from training labels\n",
        "    if labels[0] is None:  # no labels loaded\n",
        "        return torch.Tensor()\n",
        "\n",
        "    labels = np.concatenate(labels, 0)  # labels.shape = (866643, 5) for COCO\n",
        "    classes = labels[:, 0].astype(np.int)  # labels = [class xywh]\n",
        "    weights = np.bincount(classes, minlength=nc)  # occurrences per class\n",
        "\n",
        "    # Prepend gridpoint count (for uCE training)\n",
        "    # gpi = ((320 / 32 * np.array([1, 2, 4])) ** 2 * 3).sum()  # gridpoints per image\n",
        "    # weights = np.hstack([gpi * len(labels)  - weights.sum() * 9, weights * 9]) ** 0.5  # prepend gridpoints to start\n",
        "\n",
        "    weights[weights == 0] = 1  # replace empty bins with 1\n",
        "    weights = 1 / weights  # number of targets per class\n",
        "    weights /= weights.sum()  # normalize\n",
        "    return torch.from_numpy(weights)\n",
        "\n",
        "\n",
        "def labels_to_image_weights(labels, nc=80, class_weights=np.ones(80)):\n",
        "    # Produces image weights based on class_weights and image contents\n",
        "    class_counts = np.array([np.bincount(x[:, 0].astype(np.int), minlength=nc) for x in labels])\n",
        "    image_weights = (class_weights.reshape(1, nc) * class_counts).sum(1)\n",
        "    # index = random.choices(range(n), weights=image_weights, k=1)  # weight image sample\n",
        "    return image_weights\n",
        "\n",
        "\n",
        "def coco80_to_coco91_class():  # converts 80-index (val2014) to 91-index (paper)\n",
        "    # https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n",
        "    # a = np.loadtxt('data/coco.names', dtype='str', delimiter='\\n')\n",
        "    # b = np.loadtxt('data/coco_paper.names', dtype='str', delimiter='\\n')\n",
        "    # x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]  # darknet to coco\n",
        "    # x2 = [list(b[i] == a).index(True) if any(b[i] == a) else None for i in range(91)]  # coco to darknet\n",
        "    x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34,\n",
        "         35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n",
        "         64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]\n",
        "    return x\n",
        "\n",
        "\n",
        "def xyxy2xywh(x):\n",
        "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n",
        "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
        "    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\n",
        "    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\n",
        "    y[:, 2] = x[:, 2] - x[:, 0]  # width\n",
        "    y[:, 3] = x[:, 3] - x[:, 1]  # height\n",
        "    return y\n",
        "\n",
        "\n",
        "def xywh2xyxy(x):\n",
        "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
        "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
        "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
        "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
        "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
        "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
        "    return y\n",
        "\n",
        "\n",
        "def xywhn2xyxy(x, w=640, h=640, padw=0, padh=0):\n",
        "    # Convert nx4 boxes from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
        "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
        "    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + padw  # top left x\n",
        "    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + padh  # top left y\n",
        "    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + padw  # bottom right x\n",
        "    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + padh  # bottom right y\n",
        "    return y\n",
        "\n",
        "\n",
        "def xyxy2xywhn(x, w=640, h=640, clip=False, eps=0.0):\n",
        "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n",
        "    if clip:\n",
        "        clip_coords(x, (h - eps, w - eps))  # warning: inplace clip\n",
        "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
        "    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center\n",
        "    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center\n",
        "    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # width\n",
        "    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # height\n",
        "    return y\n",
        "\n",
        "\n",
        "def xyn2xy(x, w=640, h=640, padw=0, padh=0):\n",
        "    # Convert normalized segments into pixel segments, shape (n,2)\n",
        "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
        "    y[:, 0] = w * x[:, 0] + padw  # top left x\n",
        "    y[:, 1] = h * x[:, 1] + padh  # top left y\n",
        "    return y\n",
        "\n",
        "\n",
        "def segment2box(segment, width=640, height=640):\n",
        "    # Convert 1 segment label to 1 box label, applying inside-image constraint, i.e. (xy1, xy2, ...) to (xyxy)\n",
        "    x, y = segment.T  # segment xy\n",
        "    inside = (x >= 0) & (y >= 0) & (x <= width) & (y <= height)\n",
        "    x, y, = x[inside], y[inside]\n",
        "    return np.array([x.min(), y.min(), x.max(), y.max()]) if any(x) else np.zeros((1, 4))  # xyxy\n",
        "\n",
        "\n",
        "def segments2boxes(segments):\n",
        "    # Convert segment labels to box labels, i.e. (cls, xy1, xy2, ...) to (cls, xywh)\n",
        "    boxes = []\n",
        "    for s in segments:\n",
        "        x, y = s.T  # segment xy\n",
        "        boxes.append([x.min(), y.min(), x.max(), y.max()])  # cls, xyxy\n",
        "    return xyxy2xywh(np.array(boxes))  # cls, xywh\n",
        "\n",
        "\n",
        "def resample_segments(segments, n=1000):\n",
        "    # Up-sample an (n,2) segment\n",
        "    for i, s in enumerate(segments):\n",
        "        x = np.linspace(0, len(s) - 1, n)\n",
        "        xp = np.arange(len(s))\n",
        "        segments[i] = np.concatenate([np.interp(x, xp, s[:, i]) for i in range(2)]).reshape(2, -1).T  # segment xy\n",
        "    return segments\n",
        "\n",
        "\n",
        "def scale_coords(img1_shape, coords, img0_shape, ratio_pad=None):\n",
        "    # Rescale coords (xyxy) from img1_shape to img0_shape\n",
        "    if ratio_pad is None:  # calculate from img0_shape\n",
        "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n",
        "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n",
        "    else:\n",
        "        gain = ratio_pad[0][0]\n",
        "        pad = ratio_pad[1]\n",
        "\n",
        "    coords[:, [0, 2]] -= pad[0]  # x padding\n",
        "    coords[:, [1, 3]] -= pad[1]  # y padding\n",
        "    coords[:, :4] /= gain\n",
        "    clip_coords(coords, img0_shape)\n",
        "    return coords\n",
        "\n",
        "\n",
        "def clip_coords(boxes, shape):\n",
        "    # Clip bounding xyxy bounding boxes to image shape (height, width)\n",
        "    if isinstance(boxes, torch.Tensor):  # faster individually\n",
        "        boxes[:, 0].clamp_(0, shape[1])  # x1\n",
        "        boxes[:, 1].clamp_(0, shape[0])  # y1\n",
        "        boxes[:, 2].clamp_(0, shape[1])  # x2\n",
        "        boxes[:, 3].clamp_(0, shape[0])  # y2\n",
        "    else:  # np.array (faster grouped)\n",
        "        boxes[:, [0, 2]] = boxes[:, [0, 2]].clip(0, shape[1])  # x1, x2\n",
        "        boxes[:, [1, 3]] = boxes[:, [1, 3]].clip(0, shape[0])  # y1, y2\n",
        "\n",
        "\n",
        "def non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False,\n",
        "                        labels=(), max_det=300):\n",
        "    \"\"\"Runs Non-Maximum Suppression (NMS) on inference results\n",
        "\n",
        "    Returns:\n",
        "         list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
        "    \"\"\"\n",
        "\n",
        "    nc = prediction.shape[2] - 5  # number of classes\n",
        "    xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "    # Checks\n",
        "    assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "    assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "    # Settings\n",
        "    min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "    time_limit = 10.0  # seconds to quit after\n",
        "    redundant = True  # require redundant detections\n",
        "    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "    merge = False  # use merge-NMS\n",
        "\n",
        "    t = time.time()\n",
        "    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "    for xi, x in enumerate(prediction):  # image index, image inference\n",
        "        # Apply constraints\n",
        "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "        x = x[xc[xi]]  # confidence\n",
        "\n",
        "        # Cat apriori labels if autolabelling\n",
        "        if labels and len(labels[xi]):\n",
        "            l = labels[xi]\n",
        "            v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "            v[:, :4] = l[:, 1:5]  # box\n",
        "            v[:, 4] = 1.0  # conf\n",
        "            v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "            x = torch.cat((x, v), 0)\n",
        "\n",
        "        # If none remain process next image\n",
        "        if not x.shape[0]:\n",
        "            continue\n",
        "\n",
        "        # Compute conf\n",
        "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "\n",
        "        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "        box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "        # Detections matrix nx6 (xyxy, conf, cls)\n",
        "        if multi_label:\n",
        "            i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "        else:  # best class only\n",
        "            conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "\n",
        "        # Filter by class\n",
        "        if classes is not None:\n",
        "            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "        # Apply finite constraint\n",
        "        # if not torch.isfinite(x).all():\n",
        "        #     x = x[torch.isfinite(x).all(1)]\n",
        "\n",
        "        # Check shape\n",
        "        n = x.shape[0]  # number of boxes\n",
        "        if not n:  # no boxes\n",
        "            continue\n",
        "        elif n > max_nms:  # excess boxes\n",
        "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "        # Batched NMS\n",
        "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "        if i.shape[0] > max_det:  # limit detections\n",
        "            i = i[:max_det]\n",
        "        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "            weights = iou * scores[None]  # box weights\n",
        "            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "            if redundant:\n",
        "                i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "        output[xi] = x[i]\n",
        "        if (time.time() - t) > time_limit:\n",
        "            print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "            break  # time limit exceeded\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def strip_optimizer(f='best.pt', s=''):  # from utils.general import *; strip_optimizer()\n",
        "    # Strip optimizer from 'f' to finalize training, optionally save as 's'\n",
        "    x = torch.load(f, map_location=torch.device('cpu'))\n",
        "    if x.get('ema'):\n",
        "        x['model'] = x['ema']  # replace model with ema\n",
        "    for k in 'optimizer', 'training_results', 'wandb_id', 'ema', 'updates':  # keys\n",
        "        x[k] = None\n",
        "    x['epoch'] = -1\n",
        "    x['model'].half()  # to FP16\n",
        "    for p in x['model'].parameters():\n",
        "        p.requires_grad = False\n",
        "    torch.save(x, s or f)\n",
        "    mb = os.path.getsize(s or f) / 1E6  # filesize\n",
        "    print(f\"Optimizer stripped from {f},{(' saved as %s,' % s) if s else ''} {mb:.1f}MB\")\n",
        "\n",
        "\n",
        "def print_mutation(hyp, results, yaml_file='hyp_evolved.yaml', bucket=''):\n",
        "    # Print mutation results to evolve.txt (for use with train.py --evolve)\n",
        "    a = '%10s' * len(hyp) % tuple(hyp.keys())  # hyperparam keys\n",
        "    b = '%10.3g' * len(hyp) % tuple(hyp.values())  # hyperparam values\n",
        "    c = '%10.4g' * len(results) % results  # results (P, R, mAP@0.5, mAP@0.5:0.95, val_losses x 3)\n",
        "    print('\\n%s\\n%s\\nEvolved fitness: %s\\n' % (a, b, c))\n",
        "\n",
        "    if bucket:\n",
        "        url = 'gs://%s/evolve.txt' % bucket\n",
        "        if gsutil_getsize(url) > (os.path.getsize('evolve.txt') if os.path.exists('evolve.txt') else 0):\n",
        "            os.system('gsutil cp %s .' % url)  # download evolve.txt if larger than local\n",
        "\n",
        "    with open('evolve.txt', 'a') as f:  # append result\n",
        "        f.write(c + b + '\\n')\n",
        "    x = np.unique(np.loadtxt('evolve.txt', ndmin=2), axis=0)  # load unique rows\n",
        "    x = x[np.argsort(-fitness(x))]  # sort\n",
        "    np.savetxt('evolve.txt', x, '%10.3g')  # save sort by fitness\n",
        "\n",
        "    # Save yaml\n",
        "    for i, k in enumerate(hyp.keys()):\n",
        "        hyp[k] = float(x[0, i + 7])\n",
        "    with open(yaml_file, 'w') as f:\n",
        "        results = tuple(x[0, :7])\n",
        "        c = '%10.4g' * len(results) % results  # results (P, R, mAP@0.5, mAP@0.5:0.95, val_losses x 3)\n",
        "        f.write('# Hyperparameter Evolution Results\\n# Generations: %g\\n# Metrics: ' % len(x) + c + '\\n\\n')\n",
        "        yaml.safe_dump(hyp, f, sort_keys=False)\n",
        "\n",
        "    if bucket:\n",
        "        os.system('gsutil cp evolve.txt %s gs://%s' % (yaml_file, bucket))  # upload\n",
        "\n",
        "\n",
        "def apply_classifier(x, model, img, im0):\n",
        "    # Apply a second stage classifier to yolo outputs\n",
        "    im0 = [im0] if isinstance(im0, np.ndarray) else im0\n",
        "    for i, d in enumerate(x):  # per image\n",
        "        if d is not None and len(d):\n",
        "            d = d.clone()\n",
        "\n",
        "            # Reshape and pad cutouts\n",
        "            b = xyxy2xywh(d[:, :4])  # boxes\n",
        "            b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # rectangle to square\n",
        "            b[:, 2:] = b[:, 2:] * 1.3 + 30  # pad\n",
        "            d[:, :4] = xywh2xyxy(b).long()\n",
        "\n",
        "            # Rescale boxes from img_size to im0 size\n",
        "            scale_coords(img.shape[2:], d[:, :4], im0[i].shape)\n",
        "\n",
        "            # Classes\n",
        "            pred_cls1 = d[:, 5].long()\n",
        "            ims = []\n",
        "            for j, a in enumerate(d):  # per item\n",
        "                cutout = im0[i][int(a[1]):int(a[3]), int(a[0]):int(a[2])]\n",
        "                im = cv2.resize(cutout, (224, 224))  # BGR\n",
        "                # cv2.imwrite('example%i.jpg' % j, cutout)\n",
        "\n",
        "                im = im[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "                im = np.ascontiguousarray(im, dtype=np.float32)  # uint8 to float32\n",
        "                im /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "                ims.append(im)\n",
        "\n",
        "            pred_cls2 = model(torch.Tensor(ims).to(d.device)).argmax(1)  # classifier prediction\n",
        "            x[i] = x[i][pred_cls1 == pred_cls2]  # retain matching class detections\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def save_one_box(xyxy, im, file='image.jpg', gain=1.02, pad=10, square=False, BGR=False, save=True):\n",
        "    # Save image crop as {file} with crop size multiple {gain} and {pad} pixels. Save and/or return crop\n",
        "    xyxy = torch.tensor(xyxy).view(-1, 4)\n",
        "    b = xyxy2xywh(xyxy)  # boxes\n",
        "    if square:\n",
        "        b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # attempt rectangle to square\n",
        "    b[:, 2:] = b[:, 2:] * gain + pad  # box wh * gain + pad\n",
        "    xyxy = xywh2xyxy(b).long()\n",
        "    clip_coords(xyxy, im.shape)\n",
        "    crop = im[int(xyxy[0, 1]):int(xyxy[0, 3]), int(xyxy[0, 0]):int(xyxy[0, 2]), ::(1 if BGR else -1)]\n",
        "    path=str(increment_path(file, mkdir=True).with_suffix('.jpg'))\n",
        "    print(path+'\\n')\n",
        "    cv2.imwrite(path, crop)\n",
        "    return crop\n",
        "\n",
        "\n",
        "def increment_path(path, exist_ok=False, sep='', mkdir=False):\n",
        "    # Increment file or directory path, i.e. runs/exp --> runs/exp{sep}2, runs/exp{sep}3, ... etc.\n",
        "    path = Path(path)  # os-agnostic\n",
        "    if path.exists() and not exist_ok:\n",
        "        suffix = path.suffix\n",
        "        path = path.with_suffix('')\n",
        "        dirs = glob.glob(f\"{path}{sep}*\")  # similar paths\n",
        "        matches = [re.search(rf\"%s{sep}(\\d+)\" % path.stem, d) for d in dirs]\n",
        "        i = [int(m.groups()[0]) for m in matches if m]  # indices\n",
        "        n = max(i) + 1 if i else 2  # increment number\n",
        "        path = '/content/gdrive/MyDrive/predictions/'+str(n)  # update path\n",
        "    dir = path if path.suffix == '' else path.parent  # directory\n",
        "    if not dir.exists() and mkdir:\n",
        "        dir.mkdir(parents=True, exist_ok=True)  # make directory\n",
        "    return path\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK49pVfxbRu0",
        "outputId": "88ead92f-3076-47bb-b1a9-664f31fe9127"
      },
      "source": [
        "%%writefile detect.py\n",
        "import argparse\n",
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "FILE = Path(__file__).absolute()\n",
        "sys.path.append(FILE.parents[0].as_posix())  # add yolov5/ to path\n",
        "\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import check_img_size, check_requirements, check_imshow, colorstr, non_max_suppression, \\\n",
        "    apply_classifier, scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path, save_one_box\n",
        "from utils.plots import colors, plot_one_box\n",
        "from utils.torch_utils import select_device, load_classifier, time_synchronized\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def run(weights='yolov5s.pt',  # model.pt path(s)\n",
        "        source='data/images',  # file/dir/URL/glob, 0 for webcam\n",
        "        imgsz=640,  # inference size (pixels)\n",
        "        conf_thres=0.25,  # confidence threshold\n",
        "        iou_thres=0.45,  # NMS IOU threshold\n",
        "        max_det=1000,  # maximum detections per image\n",
        "        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
        "        view_img=False,  # show results\n",
        "        save_txt=False,  # save results to *.txt\n",
        "        save_conf=False,  # save confidences in --save-txt labels\n",
        "        save_crop=False,  # save cropped prediction boxes\n",
        "        nosave=False,  # do not save images/videos\n",
        "        classes=None,  # filter by class: --class 0, or --class 0 2 3\n",
        "        agnostic_nms=False,  # class-agnostic NMS\n",
        "        augment=False,  # augmented inference\n",
        "        visualize=False,  # visualize features\n",
        "        update=False,  # update all models\n",
        "        project='runs/detect',  # save results to project/name\n",
        "        name='exp',  # save results to project/name\n",
        "        exist_ok=False,  # existing project/name ok, do not increment\n",
        "        line_thickness=3,  # bounding box thickness (pixels)\n",
        "        hide_labels=False,  # hide labels\n",
        "        hide_conf=False,  # hide confidences\n",
        "        half=False,  # use FP16 half-precision inference\n",
        "        ):\n",
        "    save_img = not nosave and not source.endswith('.txt')  # save inference images\n",
        "    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n",
        "        ('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
        "\n",
        "    # Directories\n",
        "    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n",
        "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
        "\n",
        "    # Initialize\n",
        "    set_logging()\n",
        "    device = select_device(device)\n",
        "    half &= device.type != 'cpu'  # half precision only supported on CUDA\n",
        "\n",
        "    # Load model\n",
        "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "    stride = int(model.stride.max())  # model stride\n",
        "    imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
        "    names = model.module.names if hasattr(model, 'module') else model.names  # get class names\n",
        "    if half:\n",
        "        model.half()  # to FP16\n",
        "\n",
        "    # Second-stage classifier\n",
        "    classify = False\n",
        "    if classify:\n",
        "        modelc = load_classifier(name='resnet50', n=2)  # initialize\n",
        "        modelc.load_state_dict(torch.load('resnet50.pt', map_location=device)['model']).to(device).eval()\n",
        "\n",
        "    # Dataloader\n",
        "    if webcam:\n",
        "        view_img = check_imshow()\n",
        "        cudnn.benchmark = True  # set True to speed up constant image size inference\n",
        "        dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n",
        "        bs = len(dataset)  # batch_size\n",
        "    else:\n",
        "        dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
        "        bs = 1  # batch_size\n",
        "    vid_path, vid_writer = [None] * bs, [None] * bs\n",
        "\n",
        "    # Run inference\n",
        "    if device.type != 'cpu':\n",
        "        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
        "    t0 = time.time()\n",
        "    for path, img, im0s, vid_cap in dataset:\n",
        "        img = torch.from_numpy(img).to(device)\n",
        "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "        if img.ndimension() == 3:\n",
        "            img = img.unsqueeze(0)\n",
        "\n",
        "        # Inference\n",
        "        t1 = time_synchronized()\n",
        "        pred = model(img,\n",
        "                     augment=augment,\n",
        "                     visualize=increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False)[0]\n",
        "\n",
        "        # Apply NMS\n",
        "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "        t2 = time_synchronized()\n",
        "\n",
        "        # Apply Classifier\n",
        "        if classify:\n",
        "            pred = apply_classifier(pred, modelc, img, im0s)\n",
        "\n",
        "        # Process detections\n",
        "        for i, det in enumerate(pred):  # detections per image\n",
        "            if webcam:  # batch_size >= 1\n",
        "                p, s, im0, frame = path[i], f'{i}: ', im0s[i].copy(), dataset.count\n",
        "            else:\n",
        "                p, s, im0, frame = path, '', im0s.copy(), getattr(dataset, 'frame', 0)\n",
        "\n",
        "            p = Path(p)  # to Path\n",
        "            save_path = str(Path('/content/gdrive/MyDrive/predictions') / p.name)  # img.jpg\n",
        "            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt\n",
        "            s += '%gx%g ' % img.shape[2:]  # print string\n",
        "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "            imc = im0.copy() if save_crop else im0  # for save_crop\n",
        "            if len(det):\n",
        "                # Rescale boxes from img_size to im0 size\n",
        "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
        "\n",
        "                # Print results\n",
        "                for c in det[:, -1].unique():\n",
        "                    n = (det[:, -1] == c).sum()  # detections per class\n",
        "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "\n",
        "                # Write results\n",
        "                for *xyxy, conf, cls in reversed(det):\n",
        "                    if save_txt:  # Write to file\n",
        "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
        "                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
        "                        with open(txt_path + '.txt', 'a') as f:\n",
        "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
        "\n",
        "                    if save_img or save_crop or view_img:  # Add bbox to image\n",
        "                        c = int(cls)  # integer class\n",
        "                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
        "                        c1, c2 = (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3]))\n",
        "                        colors_tot=(np.mean(im0[c1[0]:c2[0],c1[1]:c2[1],0]),\n",
        "                                    np.mean(im0[c1[0]:c2[0],c1[1]:c2[1],1]),\n",
        "                                    np.mean(im0[c1[0]:c2[0],c1[1]:c2[1],2]))\n",
        "                        plot_one_box(xyxy, im0, label=label, color=colors_tot, line_thickness=line_thickness)\n",
        "                        if save_crop:\n",
        "                            save_one_box(xyxy, imc, file=Path('/content/gdrive/MyDrive/predictions') / f'{p.stem}.jpg', BGR=True)\n",
        "\n",
        "            # Print time (inference + NMS)\n",
        "\n",
        "            # Stream results\n",
        "            if view_img:\n",
        "                cv2.imshow(str(p), im0)\n",
        "                cv2.waitKey(1)  # 1 millisecond\n",
        "\n",
        "            # Save results (image with detections)\n",
        "            if save_img:\n",
        "                if dataset.mode == 'image':\n",
        "                    cv2.imwrite(save_path, im0)\n",
        "                else:  # 'video' or 'stream'\n",
        "                    if vid_path[i] != save_path:  # new video\n",
        "                        vid_path[i] = save_path\n",
        "                        if isinstance(vid_writer[i], cv2.VideoWriter):\n",
        "                            vid_writer[i].release()  # release previous video writer\n",
        "                        if vid_cap:  # video\n",
        "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
        "                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "                        else:  # stream\n",
        "                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
        "                            save_path += '.mp4'\n",
        "                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
        "                    vid_writer[i].write(im0)\n",
        "\n",
        "    if save_txt or save_img:\n",
        "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
        "        print(f\"Results saved to {save_dir}{s}\")\n",
        "\n",
        "    if update:\n",
        "        strip_optimizer(weights)  # update model (to fix SourceChangeWarning)\n",
        "\n",
        "\n",
        "\n",
        "def parse_opt():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--weights', nargs='+', type=str, default='yolov5s.pt', help='model.pt path(s)')\n",
        "    parser.add_argument('--source', type=str, default='data/images', help='file/dir/URL/glob, 0 for webcam')\n",
        "    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='inference size (pixels)')\n",
        "    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')\n",
        "    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')\n",
        "    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')\n",
        "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--view-img', action='store_true', help='show results')\n",
        "    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
        "    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
        "    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')\n",
        "    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')\n",
        "    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')\n",
        "    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
        "    parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
        "    parser.add_argument('--visualize', action='store_true', help='visualize features')\n",
        "    parser.add_argument('--update', action='store_true', help='update all models')\n",
        "    parser.add_argument('--project', default='runs/detect', help='save results to project/name')\n",
        "    parser.add_argument('--name', default='exp', help='save results to project/name')\n",
        "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
        "    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')\n",
        "    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')\n",
        "    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')\n",
        "    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n",
        "    opt = parser.parse_args()\n",
        "    return opt\n",
        "\n",
        "\n",
        "def main(opt):\n",
        "    check_requirements(exclude=('tensorboard', 'thop'))\n",
        "    run(**vars(opt))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    opt = parse_opt()\n",
        "    main(opt)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting detect.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpCGw50rOPL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26b0c345-239e-4a29-965b-356280e64e54"
      },
      "source": [
        "!python detect.py --source /content/gdrive/MyDrive/hardhat  --weights /content/gdrive/MyDrive/best.pt --conf 0.25"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "YOLOv5 🚀 v5.0-290-g62409ee torch 1.9.0+cu102 CUDA:0 (Tesla T4, 15109.75MB)\n",
            "\n",
            "Fusing layers... \n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "Model Summary: 224 layers, 7053910 parameters, 0 gradients, 16.3 GFLOPs\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "image 1/9035 /content/gdrive/MyDrive/hardhat/hardhat_0.jpg: image 2/9035 /content/gdrive/MyDrive/hardhat/hardhat_1.jpg: image 3/9035 /content/gdrive/MyDrive/hardhat/hardhat_10.jpg: image 4/9035 /content/gdrive/MyDrive/hardhat/hardhat_100.jpg: image 5/9035 /content/gdrive/MyDrive/hardhat/hardhat_1000.jpg: image 6/9035 /content/gdrive/MyDrive/hardhat/hardhat_1001.jpg: image 7/9035 /content/gdrive/MyDrive/hardhat/hardhat_1002.jpg: image 8/9035 /content/gdrive/MyDrive/hardhat/hardhat_1003.jpg: image 9/9035 /content/gdrive/MyDrive/hardhat/hardhat_1004.jpg: image 10/9035 /content/gdrive/MyDrive/hardhat/hardhat_1005.jpg: image 11/9035 /content/gdrive/MyDrive/hardhat/hardhat_1006.jpg: image 12/9035 /content/gdrive/MyDrive/hardhat/hardhat_1007.jpg: image 13/9035 /content/gdrive/MyDrive/hardhat/hardhat_1008.jpg: image 14/9035 /content/gdrive/MyDrive/hardhat/hardhat_1009.jpg: image 15/9035 /content/gdrive/MyDrive/hardhat/hardhat_101.jpg: image 16/9035 /content/gdrive/MyDrive/hardhat/hardhat_1010.jpg: image 17/9035 /content/gdrive/MyDrive/hardhat/hardhat_1011.jpg: image 18/9035 /content/gdrive/MyDrive/hardhat/hardhat_1012.jpg: image 19/9035 /content/gdrive/MyDrive/hardhat/hardhat_1013.jpg: image 20/9035 /content/gdrive/MyDrive/hardhat/hardhat_1014.jpg: image 21/9035 /content/gdrive/MyDrive/hardhat/hardhat_1015.jpg: image 22/9035 /content/gdrive/MyDrive/hardhat/hardhat_1016.jpg: image 23/9035 /content/gdrive/MyDrive/hardhat/hardhat_1017.jpg: image 24/9035 /content/gdrive/MyDrive/hardhat/hardhat_1018.jpg: image 25/9035 /content/gdrive/MyDrive/hardhat/hardhat_1019.jpg: image 26/9035 /content/gdrive/MyDrive/hardhat/hardhat_102.jpg: image 27/9035 /content/gdrive/MyDrive/hardhat/hardhat_1020.jpg: image 28/9035 /content/gdrive/MyDrive/hardhat/hardhat_1021.jpg: image 29/9035 /content/gdrive/MyDrive/hardhat/hardhat_1022.jpg: image 30/9035 /content/gdrive/MyDrive/hardhat/hardhat_1023.jpg: image 31/9035 /content/gdrive/MyDrive/hardhat/hardhat_1024.jpg: image 32/9035 /content/gdrive/MyDrive/hardhat/hardhat_1025.jpg: image 33/9035 /content/gdrive/MyDrive/hardhat/hardhat_1026.jpg: image 34/9035 /content/gdrive/MyDrive/hardhat/hardhat_1027.jpg: image 35/9035 /content/gdrive/MyDrive/hardhat/hardhat_1028.jpg: image 36/9035 /content/gdrive/MyDrive/hardhat/hardhat_1029.jpg: image 37/9035 /content/gdrive/MyDrive/hardhat/hardhat_103.jpg: image 38/9035 /content/gdrive/MyDrive/hardhat/hardhat_1030.jpg: image 39/9035 /content/gdrive/MyDrive/hardhat/hardhat_1031.jpg: image 40/9035 /content/gdrive/MyDrive/hardhat/hardhat_1032.jpg: image 41/9035 /content/gdrive/MyDrive/hardhat/hardhat_1033.jpg: image 42/9035 /content/gdrive/MyDrive/hardhat/hardhat_1034.jpg: image 43/9035 /content/gdrive/MyDrive/hardhat/hardhat_1035.jpg: image 44/9035 /content/gdrive/MyDrive/hardhat/hardhat_1036.jpg: image 45/9035 /content/gdrive/MyDrive/hardhat/hardhat_1037.jpg: image 46/9035 /content/gdrive/MyDrive/hardhat/hardhat_1038.jpg: image 47/9035 /content/gdrive/MyDrive/hardhat/hardhat_1039.jpg: image 48/9035 /content/gdrive/MyDrive/hardhat/hardhat_104.jpg: image 49/9035 /content/gdrive/MyDrive/hardhat/hardhat_1040.jpg: image 50/9035 /content/gdrive/MyDrive/hardhat/hardhat_1041.jpg: image 51/9035 /content/gdrive/MyDrive/hardhat/hardhat_1042.jpg: image 52/9035 /content/gdrive/MyDrive/hardhat/hardhat_1043.jpg: image 53/9035 /content/gdrive/MyDrive/hardhat/hardhat_1044.jpg: image 54/9035 /content/gdrive/MyDrive/hardhat/hardhat_1045.jpg: image 55/9035 /content/gdrive/MyDrive/hardhat/hardhat_1046.jpg: image 56/9035 /content/gdrive/MyDrive/hardhat/hardhat_1047.jpg: image 57/9035 /content/gdrive/MyDrive/hardhat/hardhat_1048.jpg: image 58/9035 /content/gdrive/MyDrive/hardhat/hardhat_1049.jpg: image 59/9035 /content/gdrive/MyDrive/hardhat/hardhat_105.jpg: image 60/9035 /content/gdrive/MyDrive/hardhat/hardhat_1050.jpg: image 61/9035 /content/gdrive/MyDrive/hardhat/hardhat_1051.jpg: image 62/9035 /content/gdrive/MyDrive/hardhat/hardhat_1052.jpg: image 63/9035 /content/gdrive/MyDrive/hardhat/hardhat_1053.jpg: image 64/9035 /content/gdrive/MyDrive/hardhat/hardhat_1054.jpg: image 65/9035 /content/gdrive/MyDrive/hardhat/hardhat_1055.jpg: image 66/9035 /content/gdrive/MyDrive/hardhat/hardhat_1056.jpg: image 67/9035 /content/gdrive/MyDrive/hardhat/hardhat_1057.jpg: image 68/9035 /content/gdrive/MyDrive/hardhat/hardhat_1058.jpg: image 69/9035 /content/gdrive/MyDrive/hardhat/hardhat_1059.jpg: image 70/9035 /content/gdrive/MyDrive/hardhat/hardhat_106.jpg: image 71/9035 /content/gdrive/MyDrive/hardhat/hardhat_1060.jpg: image 72/9035 /content/gdrive/MyDrive/hardhat/hardhat_1061.jpg: image 73/9035 /content/gdrive/MyDrive/hardhat/hardhat_1062.jpg: image 74/9035 /content/gdrive/MyDrive/hardhat/hardhat_1063.jpg: image 75/9035 /content/gdrive/MyDrive/hardhat/hardhat_1064.jpg: image 76/9035 /content/gdrive/MyDrive/hardhat/hardhat_1065.jpg: image 77/9035 /content/gdrive/MyDrive/hardhat/hardhat_1066.jpg: image 78/9035 /content/gdrive/MyDrive/hardhat/hardhat_1067.jpg: image 79/9035 /content/gdrive/MyDrive/hardhat/hardhat_1068.jpg: image 80/9035 /content/gdrive/MyDrive/hardhat/hardhat_1069.jpg: image 81/9035 /content/gdrive/MyDrive/hardhat/hardhat_107.jpg: image 82/9035 /content/gdrive/MyDrive/hardhat/hardhat_1070.jpg: image 83/9035 /content/gdrive/MyDrive/hardhat/hardhat_1071.jpg: image 84/9035 /content/gdrive/MyDrive/hardhat/hardhat_1072.jpg: image 85/9035 /content/gdrive/MyDrive/hardhat/hardhat_1073.jpg: image 86/9035 /content/gdrive/MyDrive/hardhat/hardhat_1074.jpg: image 87/9035 /content/gdrive/MyDrive/hardhat/hardhat_1075.jpg: image 88/9035 /content/gdrive/MyDrive/hardhat/hardhat_1076.jpg: image 89/9035 /content/gdrive/MyDrive/hardhat/hardhat_1077.jpg: image 90/9035 /content/gdrive/MyDrive/hardhat/hardhat_1078.jpg: image 91/9035 /content/gdrive/MyDrive/hardhat/hardhat_1079.jpg: image 92/9035 /content/gdrive/MyDrive/hardhat/hardhat_108.jpg: image 93/9035 /content/gdrive/MyDrive/hardhat/hardhat_1080.jpg: image 94/9035 /content/gdrive/MyDrive/hardhat/hardhat_1081.jpg: image 95/9035 /content/gdrive/MyDrive/hardhat/hardhat_1082.jpg: image 96/9035 /content/gdrive/MyDrive/hardhat/hardhat_1083.jpg: image 97/9035 /content/gdrive/MyDrive/hardhat/hardhat_1084.jpg: image 98/9035 /content/gdrive/MyDrive/hardhat/hardhat_1085.jpg: image 99/9035 /content/gdrive/MyDrive/hardhat/hardhat_1086.jpg: image 100/9035 /content/gdrive/MyDrive/hardhat/hardhat_1087.jpg: image 101/9035 /content/gdrive/MyDrive/hardhat/hardhat_1088.jpg: image 102/9035 /content/gdrive/MyDrive/hardhat/hardhat_1089.jpg: image 103/9035 /content/gdrive/MyDrive/hardhat/hardhat_109.jpg: image 104/9035 /content/gdrive/MyDrive/hardhat/hardhat_1090.jpg: image 105/9035 /content/gdrive/MyDrive/hardhat/hardhat_1091.jpg: image 106/9035 /content/gdrive/MyDrive/hardhat/hardhat_1092.jpg: image 107/9035 /content/gdrive/MyDrive/hardhat/hardhat_1093.jpg: image 108/9035 /content/gdrive/MyDrive/hardhat/hardhat_1094.jpg: image 109/9035 /content/gdrive/MyDrive/hardhat/hardhat_1095.jpg: image 110/9035 /content/gdrive/MyDrive/hardhat/hardhat_1096.jpg: image 111/9035 /content/gdrive/MyDrive/hardhat/hardhat_1097.jpg: image 112/9035 /content/gdrive/MyDrive/hardhat/hardhat_1098.jpg: image 113/9035 /content/gdrive/MyDrive/hardhat/hardhat_1099.jpg: image 114/9035 /content/gdrive/MyDrive/hardhat/hardhat_11.jpg: image 115/9035 /content/gdrive/MyDrive/hardhat/hardhat_110.jpg: image 116/9035 /content/gdrive/MyDrive/hardhat/hardhat_1100.jpg: image 117/9035 /content/gdrive/MyDrive/hardhat/hardhat_1101.jpg: image 118/9035 /content/gdrive/MyDrive/hardhat/hardhat_1102.jpg: image 119/9035 /content/gdrive/MyDrive/hardhat/hardhat_1103.jpg: image 120/9035 /content/gdrive/MyDrive/hardhat/hardhat_1104.jpg: image 121/9035 /content/gdrive/MyDrive/hardhat/hardhat_1105.jpg: image 122/9035 /content/gdrive/MyDrive/hardhat/hardhat_1106.jpg: image 123/9035 /content/gdrive/MyDrive/hardhat/hardhat_1107.jpg: image 124/9035 /content/gdrive/MyDrive/hardhat/hardhat_1108.jpg: image 125/9035 /content/gdrive/MyDrive/hardhat/hardhat_1109.jpg: image 126/9035 /content/gdrive/MyDrive/hardhat/hardhat_111.jpg: image 127/9035 /content/gdrive/MyDrive/hardhat/hardhat_1110.jpg: image 128/9035 /content/gdrive/MyDrive/hardhat/hardhat_1111.jpg: image 129/9035 /content/gdrive/MyDrive/hardhat/hardhat_1112.jpg: image 130/9035 /content/gdrive/MyDrive/hardhat/hardhat_1113.jpg: image 131/9035 /content/gdrive/MyDrive/hardhat/hardhat_1114.jpg: image 132/9035 /content/gdrive/MyDrive/hardhat/hardhat_1115.jpg: image 133/9035 /content/gdrive/MyDrive/hardhat/hardhat_1116.jpg: image 134/9035 /content/gdrive/MyDrive/hardhat/hardhat_1117.jpg: image 135/9035 /content/gdrive/MyDrive/hardhat/hardhat_1118.jpg: image 136/9035 /content/gdrive/MyDrive/hardhat/hardhat_1119.jpg: image 137/9035 /content/gdrive/MyDrive/hardhat/hardhat_112.jpg: image 138/9035 /content/gdrive/MyDrive/hardhat/hardhat_1120.jpg: image 139/9035 /content/gdrive/MyDrive/hardhat/hardhat_1121.jpg: image 140/9035 /content/gdrive/MyDrive/hardhat/hardhat_1122.jpg: image 141/9035 /content/gdrive/MyDrive/hardhat/hardhat_1123.jpg: image 142/9035 /content/gdrive/MyDrive/hardhat/hardhat_1124.jpg: image 143/9035 /content/gdrive/MyDrive/hardhat/hardhat_1125.jpg: image 144/9035 /content/gdrive/MyDrive/hardhat/hardhat_1126.jpg: image 145/9035 /content/gdrive/MyDrive/hardhat/hardhat_1127.jpg: image 146/9035 /content/gdrive/MyDrive/hardhat/hardhat_1128.jpg: image 147/9035 /content/gdrive/MyDrive/hardhat/hardhat_1129.jpg: image 148/9035 /content/gdrive/MyDrive/hardhat/hardhat_113.jpg: image 149/9035 /content/gdrive/MyDrive/hardhat/hardhat_1130.jpg: image 150/9035 /content/gdrive/MyDrive/hardhat/hardhat_1131.jpg: image 151/9035 /content/gdrive/MyDrive/hardhat/hardhat_1132.jpg: image 152/9035 /content/gdrive/MyDrive/hardhat/hardhat_1133.jpg: image 153/9035 /content/gdrive/MyDrive/hardhat/hardhat_1134.jpg: image 154/9035 /content/gdrive/MyDrive/hardhat/hardhat_1135.jpg: image 155/9035 /content/gdrive/MyDrive/hardhat/hardhat_1136.jpg: image 156/9035 /content/gdrive/MyDrive/hardhat/hardhat_1137.jpg: image 157/9035 /content/gdrive/MyDrive/hardhat/hardhat_1138.jpg: image 158/9035 /content/gdrive/MyDrive/hardhat/hardhat_1139.jpg: image 159/9035 /content/gdrive/MyDrive/hardhat/hardhat_114.jpg: image 160/9035 /content/gdrive/MyDrive/hardhat/hardhat_1140.jpg: image 161/9035 /content/gdrive/MyDrive/hardhat/hardhat_1141.jpg: image 162/9035 /content/gdrive/MyDrive/hardhat/hardhat_1142.jpg: image 163/9035 /content/gdrive/MyDrive/hardhat/hardhat_1143.jpg: image 164/9035 /content/gdrive/MyDrive/hardhat/hardhat_1144.jpg: image 165/9035 /content/gdrive/MyDrive/hardhat/hardhat_1145.jpg: image 166/9035 /content/gdrive/MyDrive/hardhat/hardhat_1146.jpg: image 167/9035 /content/gdrive/MyDrive/hardhat/hardhat_1147.jpg: image 168/9035 /content/gdrive/MyDrive/hardhat/hardhat_1148.jpg: image 169/9035 /content/gdrive/MyDrive/hardhat/hardhat_1149.jpg: image 170/9035 /content/gdrive/MyDrive/hardhat/hardhat_115.jpg: image 171/9035 /content/gdrive/MyDrive/hardhat/hardhat_1150.jpg: image 172/9035 /content/gdrive/MyDrive/hardhat/hardhat_1151.jpg: image 173/9035 /content/gdrive/MyDrive/hardhat/hardhat_1152.jpg: image 174/9035 /content/gdrive/MyDrive/hardhat/hardhat_1153.jpg: image 175/9035 /content/gdrive/MyDrive/hardhat/hardhat_1154.jpg: image 176/9035 /content/gdrive/MyDrive/hardhat/hardhat_1155.jpg: image 177/9035 /content/gdrive/MyDrive/hardhat/hardhat_1156.jpg: image 178/9035 /content/gdrive/MyDrive/hardhat/hardhat_1157.jpg: image 179/9035 /content/gdrive/MyDrive/hardhat/hardhat_1158.jpg: image 180/9035 /content/gdrive/MyDrive/hardhat/hardhat_1159.jpg: image 181/9035 /content/gdrive/MyDrive/hardhat/hardhat_116.jpg: image 182/9035 /content/gdrive/MyDrive/hardhat/hardhat_1160.jpg: image 183/9035 /content/gdrive/MyDrive/hardhat/hardhat_1161.jpg: image 184/9035 /content/gdrive/MyDrive/hardhat/hardhat_1162.jpg: image 185/9035 /content/gdrive/MyDrive/hardhat/hardhat_1163.jpg: image 186/9035 /content/gdrive/MyDrive/hardhat/hardhat_1164.jpg: image 187/9035 /content/gdrive/MyDrive/hardhat/hardhat_1165.jpg: image 188/9035 /content/gdrive/MyDrive/hardhat/hardhat_1166.jpg: image 189/9035 /content/gdrive/MyDrive/hardhat/hardhat_1167.jpg: image 190/9035 /content/gdrive/MyDrive/hardhat/hardhat_1168.jpg: image 191/9035 /content/gdrive/MyDrive/hardhat/hardhat_1169.jpg: image 192/9035 /content/gdrive/MyDrive/hardhat/hardhat_117.jpg: image 193/9035 /content/gdrive/MyDrive/hardhat/hardhat_1170.jpg: image 194/9035 /content/gdrive/MyDrive/hardhat/hardhat_1171.jpg: image 195/9035 /content/gdrive/MyDrive/hardhat/hardhat_1172.jpg: image 196/9035 /content/gdrive/MyDrive/hardhat/hardhat_1173.jpg: image 197/9035 /content/gdrive/MyDrive/hardhat/hardhat_1174.jpg: image 198/9035 /content/gdrive/MyDrive/hardhat/hardhat_1175.jpg: image 199/9035 /content/gdrive/MyDrive/hardhat/hardhat_1176.jpg: image 200/9035 /content/gdrive/MyDrive/hardhat/hardhat_1177.jpg: image 201/9035 /content/gdrive/MyDrive/hardhat/hardhat_1178.jpg: image 202/9035 /content/gdrive/MyDrive/hardhat/hardhat_1179.jpg: image 203/9035 /content/gdrive/MyDrive/hardhat/hardhat_118.jpg: image 204/9035 /content/gdrive/MyDrive/hardhat/hardhat_1180.jpg: image 205/9035 /content/gdrive/MyDrive/hardhat/hardhat_1181.jpg: image 206/9035 /content/gdrive/MyDrive/hardhat/hardhat_1182.jpg: image 207/9035 /content/gdrive/MyDrive/hardhat/hardhat_1183.jpg: image 208/9035 /content/gdrive/MyDrive/hardhat/hardhat_1184.jpg: image 209/9035 /content/gdrive/MyDrive/hardhat/hardhat_1185.jpg: image 210/9035 /content/gdrive/MyDrive/hardhat/hardhat_1186.jpg: image 211/9035 /content/gdrive/MyDrive/hardhat/hardhat_1187.jpg: image 212/9035 /content/gdrive/MyDrive/hardhat/hardhat_1188.jpg: image 213/9035 /content/gdrive/MyDrive/hardhat/hardhat_1189.jpg: image 214/9035 /content/gdrive/MyDrive/hardhat/hardhat_119.jpg: image 215/9035 /content/gdrive/MyDrive/hardhat/hardhat_1190.jpg: image 216/9035 /content/gdrive/MyDrive/hardhat/hardhat_1191.jpg: image 217/9035 /content/gdrive/MyDrive/hardhat/hardhat_1192.jpg: image 218/9035 /content/gdrive/MyDrive/hardhat/hardhat_1193.jpg: image 219/9035 /content/gdrive/MyDrive/hardhat/hardhat_1194.jpg: image 220/9035 /content/gdrive/MyDrive/hardhat/hardhat_1195.jpg: image 221/9035 /content/gdrive/MyDrive/hardhat/hardhat_1196.jpg: image 222/9035 /content/gdrive/MyDrive/hardhat/hardhat_1197.jpg: image 223/9035 /content/gdrive/MyDrive/hardhat/hardhat_1198.jpg: image 224/9035 /content/gdrive/MyDrive/hardhat/hardhat_1199.jpg: image 225/9035 /content/gdrive/MyDrive/hardhat/hardhat_12.jpg: image 226/9035 /content/gdrive/MyDrive/hardhat/hardhat_120.jpg: image 227/9035 /content/gdrive/MyDrive/hardhat/hardhat_1200.jpg: image 228/9035 /content/gdrive/MyDrive/hardhat/hardhat_1201.jpg: image 229/9035 /content/gdrive/MyDrive/hardhat/hardhat_1202.jpg: image 230/9035 /content/gdrive/MyDrive/hardhat/hardhat_1203.jpg: image 231/9035 /content/gdrive/MyDrive/hardhat/hardhat_1204.jpg: image 232/9035 /content/gdrive/MyDrive/hardhat/hardhat_1205.jpg: image 233/9035 /content/gdrive/MyDrive/hardhat/hardhat_1206.jpg: image 234/9035 /content/gdrive/MyDrive/hardhat/hardhat_1207.jpg: image 235/9035 /content/gdrive/MyDrive/hardhat/hardhat_1208.jpg: image 236/9035 /content/gdrive/MyDrive/hardhat/hardhat_1209.jpg: image 237/9035 /content/gdrive/MyDrive/hardhat/hardhat_121.jpg: image 238/9035 /content/gdrive/MyDrive/hardhat/hardhat_1210.jpg: image 239/9035 /content/gdrive/MyDrive/hardhat/hardhat_1211.jpg: image 240/9035 /content/gdrive/MyDrive/hardhat/hardhat_1212.jpg: image 241/9035 /content/gdrive/MyDrive/hardhat/hardhat_1213.jpg: image 242/9035 /content/gdrive/MyDrive/hardhat/hardhat_1214.jpg: image 243/9035 /content/gdrive/MyDrive/hardhat/hardhat_1215.jpg: image 244/9035 /content/gdrive/MyDrive/hardhat/hardhat_1216.jpg: image 245/9035 /content/gdrive/MyDrive/hardhat/hardhat_1217.jpg: image 246/9035 /content/gdrive/MyDrive/hardhat/hardhat_1218.jpg: image 247/9035 /content/gdrive/MyDrive/hardhat/hardhat_1219.jpg: image 248/9035 /content/gdrive/MyDrive/hardhat/hardhat_122.jpg: image 249/9035 /content/gdrive/MyDrive/hardhat/hardhat_1220.jpg: image 250/9035 /content/gdrive/MyDrive/hardhat/hardhat_1221.jpg: image 251/9035 /content/gdrive/MyDrive/hardhat/hardhat_1222.jpg: image 252/9035 /content/gdrive/MyDrive/hardhat/hardhat_1223.jpg: image 253/9035 /content/gdrive/MyDrive/hardhat/hardhat_1224.jpg: image 254/9035 /content/gdrive/MyDrive/hardhat/hardhat_1225.jpg: image 255/9035 /content/gdrive/MyDrive/hardhat/hardhat_1226.jpg: image 256/9035 /content/gdrive/MyDrive/hardhat/hardhat_1227.jpg: image 257/9035 /content/gdrive/MyDrive/hardhat/hardhat_1228.jpg: image 258/9035 /content/gdrive/MyDrive/hardhat/hardhat_1229.jpg: image 259/9035 /content/gdrive/MyDrive/hardhat/hardhat_123.jpg: image 260/9035 /content/gdrive/MyDrive/hardhat/hardhat_1230.jpg: image 261/9035 /content/gdrive/MyDrive/hardhat/hardhat_1231.jpg: image 262/9035 /content/gdrive/MyDrive/hardhat/hardhat_1232.jpg: image 263/9035 /content/gdrive/MyDrive/hardhat/hardhat_1233.jpg: image 264/9035 /content/gdrive/MyDrive/hardhat/hardhat_1234.jpg: image 265/9035 /content/gdrive/MyDrive/hardhat/hardhat_1235.jpg: image 266/9035 /content/gdrive/MyDrive/hardhat/hardhat_1236.jpg: image 267/9035 /content/gdrive/MyDrive/hardhat/hardhat_1237.jpg: image 268/9035 /content/gdrive/MyDrive/hardhat/hardhat_1238.jpg: image 269/9035 /content/gdrive/MyDrive/hardhat/hardhat_1239.jpg: image 270/9035 /content/gdrive/MyDrive/hardhat/hardhat_124.jpg: image 271/9035 /content/gdrive/MyDrive/hardhat/hardhat_1240.jpg: image 272/9035 /content/gdrive/MyDrive/hardhat/hardhat_1241.jpg: image 273/9035 /content/gdrive/MyDrive/hardhat/hardhat_1242.jpg: image 274/9035 /content/gdrive/MyDrive/hardhat/hardhat_1243.jpg: image 275/9035 /content/gdrive/MyDrive/hardhat/hardhat_1244.jpg: image 276/9035 /content/gdrive/MyDrive/hardhat/hardhat_1245.jpg: image 277/9035 /content/gdrive/MyDrive/hardhat/hardhat_1246.jpg: image 278/9035 /content/gdrive/MyDrive/hardhat/hardhat_1247.jpg: image 279/9035 /content/gdrive/MyDrive/hardhat/hardhat_1248.jpg: image 280/9035 /content/gdrive/MyDrive/hardhat/hardhat_1249.jpg: image 281/9035 /content/gdrive/MyDrive/hardhat/hardhat_125.jpg: image 282/9035 /content/gdrive/MyDrive/hardhat/hardhat_1250.jpg: image 283/9035 /content/gdrive/MyDrive/hardhat/hardhat_1251.jpg: image 284/9035 /content/gdrive/MyDrive/hardhat/hardhat_1252.jpg: image 285/9035 /content/gdrive/MyDrive/hardhat/hardhat_1253.jpg: image 286/9035 /content/gdrive/MyDrive/hardhat/hardhat_1254.jpg: image 287/9035 /content/gdrive/MyDrive/hardhat/hardhat_1255.jpg: image 288/9035 /content/gdrive/MyDrive/hardhat/hardhat_1256.jpg: image 289/9035 /content/gdrive/MyDrive/hardhat/hardhat_1257.jpg: image 290/9035 /content/gdrive/MyDrive/hardhat/hardhat_1258.jpg: image 291/9035 /content/gdrive/MyDrive/hardhat/hardhat_1259.jpg: image 292/9035 /content/gdrive/MyDrive/hardhat/hardhat_126.jpg: image 293/9035 /content/gdrive/MyDrive/hardhat/hardhat_1260.jpg: image 294/9035 /content/gdrive/MyDrive/hardhat/hardhat_1261.jpg: image 295/9035 /content/gdrive/MyDrive/hardhat/hardhat_1262.jpg: image 296/9035 /content/gdrive/MyDrive/hardhat/hardhat_1263.jpg: image 297/9035 /content/gdrive/MyDrive/hardhat/hardhat_1264.jpg: image 298/9035 /content/gdrive/MyDrive/hardhat/hardhat_1265.jpg: image 299/9035 /content/gdrive/MyDrive/hardhat/hardhat_1266.jpg: image 300/9035 /content/gdrive/MyDrive/hardhat/hardhat_1267.jpg: image 301/9035 /content/gdrive/MyDrive/hardhat/hardhat_1268.jpg: image 302/9035 /content/gdrive/MyDrive/hardhat/hardhat_1269.jpg: image 303/9035 /content/gdrive/MyDrive/hardhat/hardhat_127.jpg: image 304/9035 /content/gdrive/MyDrive/hardhat/hardhat_1270.jpg: image 305/9035 /content/gdrive/MyDrive/hardhat/hardhat_1271.jpg: image 306/9035 /content/gdrive/MyDrive/hardhat/hardhat_1272.jpg: image 307/9035 /content/gdrive/MyDrive/hardhat/hardhat_1273.jpg: image 308/9035 /content/gdrive/MyDrive/hardhat/hardhat_1274.jpg: image 309/9035 /content/gdrive/MyDrive/hardhat/hardhat_1275.jpg: image 310/9035 /content/gdrive/MyDrive/hardhat/hardhat_1276.jpg: image 311/9035 /content/gdrive/MyDrive/hardhat/hardhat_1277.jpg: image 312/9035 /content/gdrive/MyDrive/hardhat/hardhat_1278.jpg: image 313/9035 /content/gdrive/MyDrive/hardhat/hardhat_1279.jpg: image 314/9035 /content/gdrive/MyDrive/hardhat/hardhat_128.jpg: image 315/9035 /content/gdrive/MyDrive/hardhat/hardhat_1280.jpg: image 316/9035 /content/gdrive/MyDrive/hardhat/hardhat_1281.jpg: image 317/9035 /content/gdrive/MyDrive/hardhat/hardhat_1282.jpg: image 318/9035 /content/gdrive/MyDrive/hardhat/hardhat_1283.jpg: image 319/9035 /content/gdrive/MyDrive/hardhat/hardhat_1284.jpg: image 320/9035 /content/gdrive/MyDrive/hardhat/hardhat_1285.jpg: image 321/9035 /content/gdrive/MyDrive/hardhat/hardhat_1286.jpg: image 322/9035 /content/gdrive/MyDrive/hardhat/hardhat_1287.jpg: image 323/9035 /content/gdrive/MyDrive/hardhat/hardhat_1288.jpg: image 324/9035 /content/gdrive/MyDrive/hardhat/hardhat_1289.jpg: image 325/9035 /content/gdrive/MyDrive/hardhat/hardhat_129.jpg: image 326/9035 /content/gdrive/MyDrive/hardhat/hardhat_1290.jpg: image 327/9035 /content/gdrive/MyDrive/hardhat/hardhat_1291.jpg: image 328/9035 /content/gdrive/MyDrive/hardhat/hardhat_1292.jpg: image 329/9035 /content/gdrive/MyDrive/hardhat/hardhat_1293.jpg: image 330/9035 /content/gdrive/MyDrive/hardhat/hardhat_1294.jpg: image 331/9035 /content/gdrive/MyDrive/hardhat/hardhat_1295.jpg: image 332/9035 /content/gdrive/MyDrive/hardhat/hardhat_1296.jpg: image 333/9035 /content/gdrive/MyDrive/hardhat/hardhat_1297.jpg: image 334/9035 /content/gdrive/MyDrive/hardhat/hardhat_1298.jpg: image 335/9035 /content/gdrive/MyDrive/hardhat/hardhat_1299.jpg: image 336/9035 /content/gdrive/MyDrive/hardhat/hardhat_13.jpg: image 337/9035 /content/gdrive/MyDrive/hardhat/hardhat_130.jpg: image 338/9035 /content/gdrive/MyDrive/hardhat/hardhat_1300.jpg: image 339/9035 /content/gdrive/MyDrive/hardhat/hardhat_1301.jpg: image 340/9035 /content/gdrive/MyDrive/hardhat/hardhat_1302.jpg: image 341/9035 /content/gdrive/MyDrive/hardhat/hardhat_1303.jpg: image 342/9035 /content/gdrive/MyDrive/hardhat/hardhat_1304.jpg: image 343/9035 /content/gdrive/MyDrive/hardhat/hardhat_1305.jpg: image 344/9035 /content/gdrive/MyDrive/hardhat/hardhat_1306.jpg: image 345/9035 /content/gdrive/MyDrive/hardhat/hardhat_1307.jpg: image 346/9035 /content/gdrive/MyDrive/hardhat/hardhat_1308.jpg: image 347/9035 /content/gdrive/MyDrive/hardhat/hardhat_1309.jpg: image 348/9035 /content/gdrive/MyDrive/hardhat/hardhat_131.jpg: image 349/9035 /content/gdrive/MyDrive/hardhat/hardhat_1310.jpg: image 350/9035 /content/gdrive/MyDrive/hardhat/hardhat_1311.jpg: image 351/9035 /content/gdrive/MyDrive/hardhat/hardhat_1312.jpg: image 352/9035 /content/gdrive/MyDrive/hardhat/hardhat_1313.jpg: image 353/9035 /content/gdrive/MyDrive/hardhat/hardhat_1314.jpg: image 354/9035 /content/gdrive/MyDrive/hardhat/hardhat_1315.jpg: image 355/9035 /content/gdrive/MyDrive/hardhat/hardhat_1316.jpg: image 356/9035 /content/gdrive/MyDrive/hardhat/hardhat_1317.jpg: image 357/9035 /content/gdrive/MyDrive/hardhat/hardhat_1318.jpg: image 358/9035 /content/gdrive/MyDrive/hardhat/hardhat_1319.jpg: image 359/9035 /content/gdrive/MyDrive/hardhat/hardhat_132.jpg: image 360/9035 /content/gdrive/MyDrive/hardhat/hardhat_1320.jpg: image 361/9035 /content/gdrive/MyDrive/hardhat/hardhat_1321.jpg: image 362/9035 /content/gdrive/MyDrive/hardhat/hardhat_1322.jpg: image 363/9035 /content/gdrive/MyDrive/hardhat/hardhat_1323.jpg: image 364/9035 /content/gdrive/MyDrive/hardhat/hardhat_1324.jpg: image 365/9035 /content/gdrive/MyDrive/hardhat/hardhat_1325.jpg: image 366/9035 /content/gdrive/MyDrive/hardhat/hardhat_1326.jpg: image 367/9035 /content/gdrive/MyDrive/hardhat/hardhat_1327.jpg: image 368/9035 /content/gdrive/MyDrive/hardhat/hardhat_1328.jpg: image 369/9035 /content/gdrive/MyDrive/hardhat/hardhat_1329.jpg: image 370/9035 /content/gdrive/MyDrive/hardhat/hardhat_133.jpg: image 371/9035 /content/gdrive/MyDrive/hardhat/hardhat_1330.jpg: image 372/9035 /content/gdrive/MyDrive/hardhat/hardhat_1331.jpg: image 373/9035 /content/gdrive/MyDrive/hardhat/hardhat_1332.jpg: image 374/9035 /content/gdrive/MyDrive/hardhat/hardhat_1333.jpg: image 375/9035 /content/gdrive/MyDrive/hardhat/hardhat_1334.jpg: image 376/9035 /content/gdrive/MyDrive/hardhat/hardhat_1335.jpg: image 377/9035 /content/gdrive/MyDrive/hardhat/hardhat_1336.jpg: image 378/9035 /content/gdrive/MyDrive/hardhat/hardhat_1337.jpg: image 379/9035 /content/gdrive/MyDrive/hardhat/hardhat_1338.jpg: image 380/9035 /content/gdrive/MyDrive/hardhat/hardhat_1339.jpg: image 381/9035 /content/gdrive/MyDrive/hardhat/hardhat_134.jpg: image 382/9035 /content/gdrive/MyDrive/hardhat/hardhat_1340.jpg: image 383/9035 /content/gdrive/MyDrive/hardhat/hardhat_1341.jpg: image 384/9035 /content/gdrive/MyDrive/hardhat/hardhat_1342.jpg: image 385/9035 /content/gdrive/MyDrive/hardhat/hardhat_1343.jpg: image 386/9035 /content/gdrive/MyDrive/hardhat/hardhat_1344.jpg: image 387/9035 /content/gdrive/MyDrive/hardhat/hardhat_1345.jpg: image 388/9035 /content/gdrive/MyDrive/hardhat/hardhat_1346.jpg: image 389/9035 /content/gdrive/MyDrive/hardhat/hardhat_1347.jpg: image 390/9035 /content/gdrive/MyDrive/hardhat/hardhat_1348.jpg: image 391/9035 /content/gdrive/MyDrive/hardhat/hardhat_1349.jpg: image 392/9035 /content/gdrive/MyDrive/hardhat/hardhat_135.jpg: image 393/9035 /content/gdrive/MyDrive/hardhat/hardhat_1350.jpg: image 394/9035 /content/gdrive/MyDrive/hardhat/hardhat_1351.jpg: image 395/9035 /content/gdrive/MyDrive/hardhat/hardhat_1352.jpg: image 396/9035 /content/gdrive/MyDrive/hardhat/hardhat_1353.jpg: image 397/9035 /content/gdrive/MyDrive/hardhat/hardhat_1354.jpg: image 398/9035 /content/gdrive/MyDrive/hardhat/hardhat_1355.jpg: image 399/9035 /content/gdrive/MyDrive/hardhat/hardhat_1356.jpg: image 400/9035 /content/gdrive/MyDrive/hardhat/hardhat_1357.jpg: image 401/9035 /content/gdrive/MyDrive/hardhat/hardhat_1358.jpg: image 402/9035 /content/gdrive/MyDrive/hardhat/hardhat_1359.jpg: image 403/9035 /content/gdrive/MyDrive/hardhat/hardhat_136.jpg: image 404/9035 /content/gdrive/MyDrive/hardhat/hardhat_1360.jpg: image 405/9035 /content/gdrive/MyDrive/hardhat/hardhat_1361.jpg: image 406/9035 /content/gdrive/MyDrive/hardhat/hardhat_1362.jpg: image 407/9035 /content/gdrive/MyDrive/hardhat/hardhat_1363.jpg: image 408/9035 /content/gdrive/MyDrive/hardhat/hardhat_1364.jpg: image 409/9035 /content/gdrive/MyDrive/hardhat/hardhat_1365.jpg: image 410/9035 /content/gdrive/MyDrive/hardhat/hardhat_1366.jpg: image 411/9035 /content/gdrive/MyDrive/hardhat/hardhat_1367.jpg: image 412/9035 /content/gdrive/MyDrive/hardhat/hardhat_1368.jpg: image 413/9035 /content/gdrive/MyDrive/hardhat/hardhat_1369.jpg: image 414/9035 /content/gdrive/MyDrive/hardhat/hardhat_137.jpg: image 415/9035 /content/gdrive/MyDrive/hardhat/hardhat_1370.jpg: image 416/9035 /content/gdrive/MyDrive/hardhat/hardhat_1371.jpg: image 417/9035 /content/gdrive/MyDrive/hardhat/hardhat_1372.jpg: image 418/9035 /content/gdrive/MyDrive/hardhat/hardhat_1373.jpg: image 419/9035 /content/gdrive/MyDrive/hardhat/hardhat_1374.jpg: image 420/9035 /content/gdrive/MyDrive/hardhat/hardhat_1375.jpg: image 421/9035 /content/gdrive/MyDrive/hardhat/hardhat_1376.jpg: image 422/9035 /content/gdrive/MyDrive/hardhat/hardhat_1377.jpg: image 423/9035 /content/gdrive/MyDrive/hardhat/hardhat_1378.jpg: image 424/9035 /content/gdrive/MyDrive/hardhat/hardhat_1379.jpg: image 425/9035 /content/gdrive/MyDrive/hardhat/hardhat_138.jpg: image 426/9035 /content/gdrive/MyDrive/hardhat/hardhat_1380.jpg: image 427/9035 /content/gdrive/MyDrive/hardhat/hardhat_1381.jpg: image 428/9035 /content/gdrive/MyDrive/hardhat/hardhat_1382.jpg: image 429/9035 /content/gdrive/MyDrive/hardhat/hardhat_1383.jpg: image 430/9035 /content/gdrive/MyDrive/hardhat/hardhat_1384.jpg: image 431/9035 /content/gdrive/MyDrive/hardhat/hardhat_1385.jpg: image 432/9035 /content/gdrive/MyDrive/hardhat/hardhat_1386.jpg: image 433/9035 /content/gdrive/MyDrive/hardhat/hardhat_1387.jpg: image 434/9035 /content/gdrive/MyDrive/hardhat/hardhat_1388.jpg: image 435/9035 /content/gdrive/MyDrive/hardhat/hardhat_1389.jpg: image 436/9035 /content/gdrive/MyDrive/hardhat/hardhat_139.jpg: image 437/9035 /content/gdrive/MyDrive/hardhat/hardhat_1390.jpg: image 438/9035 /content/gdrive/MyDrive/hardhat/hardhat_1391.jpg: image 439/9035 /content/gdrive/MyDrive/hardhat/hardhat_1392.jpg: image 440/9035 /content/gdrive/MyDrive/hardhat/hardhat_1393.jpg: image 441/9035 /content/gdrive/MyDrive/hardhat/hardhat_1394.jpg: image 442/9035 /content/gdrive/MyDrive/hardhat/hardhat_1395.jpg: image 443/9035 /content/gdrive/MyDrive/hardhat/hardhat_1396.jpg: image 444/9035 /content/gdrive/MyDrive/hardhat/hardhat_1397.jpg: image 445/9035 /content/gdrive/MyDrive/hardhat/hardhat_1398.jpg: image 446/9035 /content/gdrive/MyDrive/hardhat/hardhat_1399.jpg: image 447/9035 /content/gdrive/MyDrive/hardhat/hardhat_14.jpg: image 448/9035 /content/gdrive/MyDrive/hardhat/hardhat_140.jpg: image 449/9035 /content/gdrive/MyDrive/hardhat/hardhat_1400.jpg: image 450/9035 /content/gdrive/MyDrive/hardhat/hardhat_1401.jpg: image 451/9035 /content/gdrive/MyDrive/hardhat/hardhat_1402.jpg: image 452/9035 /content/gdrive/MyDrive/hardhat/hardhat_1403.jpg: image 453/9035 /content/gdrive/MyDrive/hardhat/hardhat_1404.jpg: image 454/9035 /content/gdrive/MyDrive/hardhat/hardhat_1405.jpg: image 455/9035 /content/gdrive/MyDrive/hardhat/hardhat_1406.jpg: image 456/9035 /content/gdrive/MyDrive/hardhat/hardhat_1407.jpg: image 457/9035 /content/gdrive/MyDrive/hardhat/hardhat_1408.jpg: image 458/9035 /content/gdrive/MyDrive/hardhat/hardhat_1409.jpg: image 459/9035 /content/gdrive/MyDrive/hardhat/hardhat_141.jpg: image 460/9035 /content/gdrive/MyDrive/hardhat/hardhat_1410.jpg: image 461/9035 /content/gdrive/MyDrive/hardhat/hardhat_1411.jpg: image 462/9035 /content/gdrive/MyDrive/hardhat/hardhat_1412.jpg: image 463/9035 /content/gdrive/MyDrive/hardhat/hardhat_1413.jpg: image 464/9035 /content/gdrive/MyDrive/hardhat/hardhat_1414.jpg: image 465/9035 /content/gdrive/MyDrive/hardhat/hardhat_1415.jpg: image 466/9035 /content/gdrive/MyDrive/hardhat/hardhat_1416.jpg: image 467/9035 /content/gdrive/MyDrive/hardhat/hardhat_1417.jpg: image 468/9035 /content/gdrive/MyDrive/hardhat/hardhat_1418.jpg: image 469/9035 /content/gdrive/MyDrive/hardhat/hardhat_1419.jpg: image 470/9035 /content/gdrive/MyDrive/hardhat/hardhat_142.jpg: image 471/9035 /content/gdrive/MyDrive/hardhat/hardhat_1420.jpg: image 472/9035 /content/gdrive/MyDrive/hardhat/hardhat_1421.jpg: image 473/9035 /content/gdrive/MyDrive/hardhat/hardhat_1422.jpg: image 474/9035 /content/gdrive/MyDrive/hardhat/hardhat_1423.jpg: image 475/9035 /content/gdrive/MyDrive/hardhat/hardhat_1424.jpg: image 476/9035 /content/gdrive/MyDrive/hardhat/hardhat_1425.jpg: image 477/9035 /content/gdrive/MyDrive/hardhat/hardhat_1426.jpg: image 478/9035 /content/gdrive/MyDrive/hardhat/hardhat_1427.jpg: image 479/9035 /content/gdrive/MyDrive/hardhat/hardhat_1428.jpg: image 480/9035 /content/gdrive/MyDrive/hardhat/hardhat_1429.jpg: image 481/9035 /content/gdrive/MyDrive/hardhat/hardhat_143.jpg: image 482/9035 /content/gdrive/MyDrive/hardhat/hardhat_1430.jpg: image 483/9035 /content/gdrive/MyDrive/hardhat/hardhat_1431.jpg: image 484/9035 /content/gdrive/MyDrive/hardhat/hardhat_1432.jpg: image 485/9035 /content/gdrive/MyDrive/hardhat/hardhat_1433.jpg: image 486/9035 /content/gdrive/MyDrive/hardhat/hardhat_1434.jpg: image 487/9035 /content/gdrive/MyDrive/hardhat/hardhat_1435.jpg: image 488/9035 /content/gdrive/MyDrive/hardhat/hardhat_1436.jpg: image 489/9035 /content/gdrive/MyDrive/hardhat/hardhat_1437.jpg: image 490/9035 /content/gdrive/MyDrive/hardhat/hardhat_1438.jpg: image 491/9035 /content/gdrive/MyDrive/hardhat/hardhat_1439.jpg: image 492/9035 /content/gdrive/MyDrive/hardhat/hardhat_144.jpg: image 493/9035 /content/gdrive/MyDrive/hardhat/hardhat_1440.jpg: image 494/9035 /content/gdrive/MyDrive/hardhat/hardhat_1441.jpg: image 495/9035 /content/gdrive/MyDrive/hardhat/hardhat_1442.jpg: image 496/9035 /content/gdrive/MyDrive/hardhat/hardhat_1443.jpg: image 497/9035 /content/gdrive/MyDrive/hardhat/hardhat_1444.jpg: image 498/9035 /content/gdrive/MyDrive/hardhat/hardhat_1445.jpg: image 499/9035 /content/gdrive/MyDrive/hardhat/hardhat_1446.jpg: image 500/9035 /content/gdrive/MyDrive/hardhat/hardhat_1447.jpg: image 501/9035 /content/gdrive/MyDrive/hardhat/hardhat_1448.jpg: image 502/9035 /content/gdrive/MyDrive/hardhat/hardhat_1449.jpg: image 503/9035 /content/gdrive/MyDrive/hardhat/hardhat_145.jpg: image 504/9035 /content/gdrive/MyDrive/hardhat/hardhat_1450.jpg: image 505/9035 /content/gdrive/MyDrive/hardhat/hardhat_1451.jpg: image 506/9035 /content/gdrive/MyDrive/hardhat/hardhat_1452.jpg: image 507/9035 /content/gdrive/MyDrive/hardhat/hardhat_1453.jpg: image 508/9035 /content/gdrive/MyDrive/hardhat/hardhat_1454.jpg: image 509/9035 /content/gdrive/MyDrive/hardhat/hardhat_1455.jpg: image 510/9035 /content/gdrive/MyDrive/hardhat/hardhat_1456.jpg: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdwB3kxLUhXw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}